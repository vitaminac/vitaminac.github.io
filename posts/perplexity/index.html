<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="content-language" content="en" />
    <link rel="icon" href="/favicon.ico" />
    <title>Perplexity</title>
    <meta name="og:title" content="Perplexity" />
    <meta name="author" content="Gao" />
    <meta
      name="keywords"
      content="Perplexity,NLP,information theory,cross-entropy,LLM,transformer"
    />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="next-head-count" content="10" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css"
      integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"
      integrity="sha512-c42qTSw/wPZ3/5LBzD+Bw5f7bSF2oxou6wEb+I/lqeaKV5FDIfMvvRp772y4jcJLKuGUOpbJMdg/BTl50fJYAw=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css"
      integrity="sha384-ZPe7yZ91iWxYumsBEOn7ieg8q/o+qh/hQpSaPow8T6BwALcXSCS6C6fSRPIAnTQs"
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css"
      integrity="sha512-hasIneQUHlh06VNBe7f6ZcHmeRTLIaQWFd43YriJ0UND19bvYRauxthDg8E4eVNPm9bRUhr5JGeqH7FRFXQu5g=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    />
    <link
      rel="preload"
      href="/_next/static/css/02f4e97d1c9ded28.css"
      as="style"
    />
    <link
      rel="stylesheet"
      href="/_next/static/css/02f4e97d1c9ded28.css"
      data-n-g=""
    />
    <noscript data-n-css=""></noscript>
    <script
      defer=""
      nomodule=""
      src="/_next/static/chunks/polyfills-42372ed130431b0a.js"
    ></script>
    <script
      src="/_next/static/chunks/webpack-b8f8d6679aaa5f42.js"
      defer=""
    ></script>
    <script
      src="/_next/static/chunks/framework-49c6cecf1f6d5795.js"
      defer=""
    ></script>
    <script
      src="/_next/static/chunks/main-78a206356629e049.js"
      defer=""
    ></script>
    <script
      src="/_next/static/chunks/pages/_app-18f3937ef5119228.js"
      defer=""
    ></script>
    <script
      src="/_next/static/chunks/470-88ca63604181d961.js"
      defer=""
    ></script>
    <script
      src="/_next/static/chunks/287-15331c4732f63f37.js"
      defer=""
    ></script>
    <script
      src="/_next/static/chunks/pages/posts/%5Bslug%5D-bde1d94dd5e8ff43.js"
      defer=""
    ></script>
    <script src="/_next/static/fixed/_buildManifest.js" defer=""></script>
    <script src="/_next/static/fixed/_ssgManifest.js" defer=""></script>
  </head>
  <body>
    <div id="__next">
      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
        integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH"
        crossorigin="anonymous"
      />
      <nav
        class="navbar navbar-expand-md navbar-light sticky-top NavigationBar_navBar__fSiIq tw-bg-white/95 tw-shadow tw-shadow-black/25"
      >
        <div class="container-fluid px-md-5">
          <div class="navbar-brand animate__animated animate__pulse">
            <span
              class="tw-rounded tw-bg-black tw-p-1 tw-text-xl tw-font-medium tw-text-white"
              >Gao&#x27;s blog</span
            >
          </div>
          <button
            class="navbar-toggler"
            type="button"
            data-bs-toggle="collapse"
            data-bs-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent"
            aria-expanded="false"
            aria-label="Toggle navigation"
          >
            <span class="navbar-toggler-icon"></span>
          </button>
          <div class="navbar-collapse collapse" id="navbarSupportedContent">
            <ul class="navbar-nav mb-2 mb-lg-0 ms-auto">
              <li class="nav-item">
                <a
                  class="nav-link active"
                  aria-current="page"
                  href="/posts/langs/en/"
                  >Home</a
                >
              </li>
              <li class="nav-item">
                <a
                  class="nav-link"
                  aria-current="page"
                  href="/posts/langs/en/tags/"
                  >Tags</a
                >
              </li>
              <li class="nav-item dropdown">
                <a
                  class="nav-link dropdown-toggle"
                  href="#"
                  role="button"
                  data-bs-toggle="dropdown"
                  aria-expanded="false"
                  >Language</a
                >
                <ul class="dropdown-menu dropdown-menu-end">
                  <li>
                    <a class="dropdown-item" href="/posts/langs/en/">English</a>
                  </li>
                  <li>
                    <a class="dropdown-item" href="/posts/langs/es/">Spanish</a>
                  </li>
                  <li>
                    <a class="dropdown-item" href="/posts/langs/zh/">Chinese</a>
                  </li>
                </ul>
              </li>
            </ul>
          </div>
        </div>
      </nav>
      <header
        class="tw-flex tw-h-96 tw-w-screen tw-flex-col tw-content-center tw-justify-center tw-bg-cover tw-bg-center tw-bg-no-repeat tw-text-center tw-text-white"
        style="
          background-image: url(/_next/static/media/default-post-preview.c96b099a.jpeg);
        "
      >
        <h1>Perplexity</h1>
        <p>Author: Gao</p>
        <span
          ><i class="fa-regular fa-calendar"></i
          ><time class="tw-pl-1.5" datetime="2024-10-14 23:41:47"
            >October 14, 2024</time
          ></span
        >
        <section class="tw-mt-3 tw-text-center">
          <p>Tags<!-- -->:</p>
          <ul class="tw-p-0">
            <li class="tw-inline tw-px-1">
              <a
                class="tw-m-0.5 tw-inline-block tw-rounded tw-border tw-border-solid tw-border-white tw-bg-neutral-300 tw-px-2 tw-text-sm tw-leading-6 tw-text-white tw-no-underline hover:tw-bg-sky-300"
                href="/posts/langs/en/tags/Perplexity/"
                >Perplexity</a
              >
            </li>
            <li class="tw-inline tw-px-1">
              <a
                class="tw-m-0.5 tw-inline-block tw-rounded tw-border tw-border-solid tw-border-white tw-bg-neutral-300 tw-px-2 tw-text-sm tw-leading-6 tw-text-white tw-no-underline hover:tw-bg-sky-300"
                href="/posts/langs/en/tags/NLP/"
                >NLP</a
              >
            </li>
            <li class="tw-inline tw-px-1">
              <a
                class="tw-m-0.5 tw-inline-block tw-rounded tw-border tw-border-solid tw-border-white tw-bg-neutral-300 tw-px-2 tw-text-sm tw-leading-6 tw-text-white tw-no-underline hover:tw-bg-sky-300"
                href="/posts/langs/en/tags/information%20theory/"
                >information theory</a
              >
            </li>
            <li class="tw-inline tw-px-1">
              <a
                class="tw-m-0.5 tw-inline-block tw-rounded tw-border tw-border-solid tw-border-white tw-bg-neutral-300 tw-px-2 tw-text-sm tw-leading-6 tw-text-white tw-no-underline hover:tw-bg-sky-300"
                href="/posts/langs/en/tags/cross-entropy/"
                >cross-entropy</a
              >
            </li>
            <li class="tw-inline tw-px-1">
              <a
                class="tw-m-0.5 tw-inline-block tw-rounded tw-border tw-border-solid tw-border-white tw-bg-neutral-300 tw-px-2 tw-text-sm tw-leading-6 tw-text-white tw-no-underline hover:tw-bg-sky-300"
                href="/posts/langs/en/tags/LLM/"
                >LLM</a
              >
            </li>
            <li class="tw-inline tw-px-1">
              <a
                class="tw-m-0.5 tw-inline-block tw-rounded tw-border tw-border-solid tw-border-white tw-bg-neutral-300 tw-px-2 tw-text-sm tw-leading-6 tw-text-white tw-no-underline hover:tw-bg-sky-300"
                href="/posts/langs/en/tags/transformer/"
                >transformer</a
              >
            </li>
          </ul>
        </section>
      </header>
      <div class="tw-mx-auto tw-w-11/12">
        <main>
          <article>
            <iframe
              title="perplexity"
              src="/perplexity/index.html"
              width="100%"
              height="0px"
            ></iframe>
          </article>
        </main>
        <aside></aside>
      </div>
      <footer class="tw-text-center tw-text-gray-400">
        <div class="row justify-content-center">
          <ul class="list-inline text-center tw-text-stone-700">
            <li class="list-inline-item">
              <a
                class="tw-text-stone-700"
                target="_blank"
                href="https://www.linkedin.com/in/shuxig"
                ><span class="fa-stack fa-lg"
                  ><i class="fa fa-circle fa-stack-2x"></i
                  ><i
                    class="fab fa-linkedin-in fa-stack-1x fa-inverse"
                  ></i></span
              ></a>
            </li>
            <li class="list-inline-item">
              <a
                class="tw-text-stone-700"
                target="_blank"
                href="mailto:dalao1002@gmail.com?subject=From my blog&amp;body=Hi,I found this website very useful"
                ><span class="fa-stack fa-lg"
                  ><i class="fa fa-circle fa-stack-2x"></i
                  ><i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span
              ></a>
            </li>
            <li class="list-inline-item">
              <a
                class="tw-text-stone-700"
                target="_blank"
                href="https://github.com/vitaminac"
                ><span class="fa-stack fa-lg"
                  ><i class="fa fa-circle fa-stack-2x"></i
                  ><i class="fab fa-github fa-stack-1x fa-inverse"></i></span
              ></a>
            </li>
            <li class="list-inline-item">
              <a
                class="tw-text-stone-700"
                target="_blank"
                href="https://gist.github.com/vitaminac"
                ><span class="fa-stack fa-lg"
                  ><i class="fa fa-circle fa-stack-2x"></i
                  ><i
                    class="fab fa-github-square fa-stack-1x fa-inverse"
                  ></i></span
              ></a>
            </li>
            <li class="list-inline-item">
              <a
                class="tw-text-stone-700"
                target="_blank"
                href="https://gitlab.com/vitaminac"
                ><span class="fa-stack fa-lg"
                  ><i class="fa fa-circle fa-stack-2x"></i
                  ><i class="fab fa-gitlab fa-stack-1x fa-inverse"></i></span
              ></a>
            </li>
            <li class="list-inline-item">
              <a
                class="tw-text-stone-700"
                target="_blank"
                href="https://www.kaggle.com/dalao1002"
                ><span class="fa-stack fa-lg"
                  ><i class="fa fa-circle fa-stack-2x"></i
                  ><i class="fab fa-kaggle fa-stack-1x fa-inverse"></i></span
              ></a>
            </li>
            <li class="list-inline-item">
              <a
                class="tw-text-stone-700"
                target="_blank"
                href="https://leetcode.com/dalao1002"
                ><span class="fa-stack fa-lg"
                  ><i class="fa fa-circle fa-stack-2x"></i
                  ><i class="fas fa-code fa-stack-1x fa-inverse"></i></span
              ></a>
            </li>
            <li class="list-inline-item">
              <a
                class="tw-text-stone-700"
                target="_blank"
                href="https://stackoverflow.com/users/9980245"
                ><span class="fa-stack fa-lg"
                  ><i class="fa fa-circle fa-stack-2x"></i
                  ><i
                    class="fab fa-stack-overflow fa-stack-1x fa-inverse"
                  ></i></span
              ></a>
            </li>
          </ul>
        </div>
        <img
          alt="QR"
          loading="lazy"
          width="1710"
          height="624"
          decoding="async"
          data-nimg="1"
          class="tw-mx-auto tw-max-w-full tw-object-contain tw-object-center lg:tw-max-w-xl"
          style="
            color: transparent;
            background-size: cover;
            background-position: 50% 50%;
            background-repeat: no-repeat;
            background-image: url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 320 120&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADCAIAAAAhqtkfAAAAVklEQVR42gFLALT/AKrStMHRxZXRpW3NilHIekDGcw/CYyjCZwC/z8LTwcan0LKu37rI48+228C838WZ16oApdSxvdPCi86doNmvx+LNu9vDx+LOn9muCIkyTVum2sgAAAAASUVORK5CYII=&#x27;/%3E%3C/svg%3E&quot;);
          "
          src="/_next/static/media/QR.b48259ea.png"
        />
        <p>Copyright © Gao&#x27;s blog 2024</p>
      </footer>
    </div>
    <script id="__NEXT_DATA__" type="application/json">
      {
        "props": {
          "pageProps": {
            "postData": {
              "title": "Perplexity",
              "markdownContentSource": "\n## Introduction\n\nThe perplexity is a widely used evaluation metric in natural language processing (NLP) that measures how well a auto-regressive/causal language model predicts a sample text. It is not well defined for masked language models.\n\n## Defition of perplexity of discrete probability distribution\n\nIn information theory, perplexity is a measure of uncertainty in the value of a sample from a discrete probability distribution. The larger the perplexity, the less likely it is that an observer can guess the value which will be drawn from the distribution. The perplexity $PP$ of a discrete probability distribution $p$ is defined as\n\n$$\nPP(p) := b^{H(p)} = b^{-\\sum_{x}{p(x)}log_{b}p(x)} = \\prod_{x}p(x)^{-p(x)}\n$$\n\nwhere $H(p)$ is the entropy in bits of the distribution, and $x$ ranges over the events. The perplexity is independent of the base $b$, the $b$ is customarily 2 but doesn't need not be 2, while the entropy and the exponentiation use the same base should be enough.\n\n## Evaluating probability model using perplexity\n\nMany time probability distribution $p$ is unknown, and one may propose an probability model $q$ based on samples that were drawn from $p$, we may evaluate $q$ by asking how well it predicts a separate test samples $x_1$, $x_2$, ..., $x_n$ also drawn from $p$. The perplexity of the model $q$ is defined as\n\n$$\nPP(p) := b^{-\\frac{1}{N}\\sum_{i=1}^{N}{log_{b}q(x_i)}}\n$$\n\n## Calcuting of perplexity of a tokenized sequence\n\nGiven a tokenized sequence $X=(x_1,\\dots,x_t)$ and an casual language model $Q_\\theta$, the probability $P$ of the sequence can be defined as\n\n$$\n\\begin{split}\nP(X;Q_\\theta) \u0026:= Q_{\\theta}(x_1) Q_{\\theta}(x_2 \\mid x_1) \\dots Q_{\\theta}(x_t \\mid x_{\u003c t})\\\\\n\u0026= \\prod_{i=1}^{t} Q_{\\theta}(x_i \\mid x_{\u003c i})\n\\end{split}\n$$\n\nwhere $Q_\\theta(x_i \\mid x_{\u003c i})$ is the probability assigns to the i-th token conditioned on the preceding tokens $x_{\u003c i}$ according to our model\n\nIt would be nice to compare the probabilities assigned to different sentences to see which sentences are better predicted by the language model. However, since the probability of a sentence is obtained from a product of probabilities, the longer is the sentence the lower will be its probability (since it’s a product of factors with values smaller thanone). We should find a way of measuring these sentence probabilities, without the influence of the sentence length. This can be done by normalizing the sentence probability by the number of words in the sentence. Since the probability of a sentence is obtained by multiplying many factors, we can average them using the geometric mean. So the normalized probability of the tokenized sequence is then\n\n$$\nP_{\\text{nomalized}}(X;Q_\\theta) := \\sqrt[t]{\\prod_{i=1}^{t} Q_{\\theta}(x_i \\mid x_{\u003c i})}\n$$\n\nThe perplexity is just the reciprocal of this expression\n\n$$\n\\begin{split}\nPP(X;Q_\\theta) \u0026:= \\frac{1}{P_{\\text{nomalized}}(X;Q_\\theta)}\\\\\n\u0026= \\frac{1}{\\sqrt[t]{\\prod_{i=1}^{t} Q_{\\theta}(x_i \\mid x_{\u003c i})}}\\\\\n\u0026= (\\prod_{i=1}^{t} Q_{\\theta}(x_i \\mid x_{\u003c i}))^{-\\frac{1}{t}}\\\\\n\u0026= b^{-\\frac{1}{t} \\sum_{i=1}^{t}{\\log_{b}{Q_\\theta(x_i \\mid x_{\u003c i})}}}\n\\end{split}\n$$\n\nWe have the same equation as perplexity for probability model, a language model is nothing more than a probability model trained over tokenized sequences. Since the tokenization procedure has a direct impact on a model’s perplexity which should always be taken into consideration when comparing different models.\n\nThe perplexity is convenient for casual language model with decoder-only transformer architure evaluation because usually cross entropy loss function is used in which the negative log-likelihood is already computed during the inference process. The model inference will calculate the negative log-likelihood conditional to context of all target token you pass in a single forward calculation. Using $e$ as $b$, the calculation can be simplied to\n\n$$\nPP(X;Q_\\theta) := e^{\\frac{1}{t}\\boldsymbol{1_{n}} \\cdot \\mathcal{L(X;\\theta)}}\n$$\n\nwhere $\\boldsymbol{1_{n}}$ is $\\begin{bmatrix} 1,\\dots,1 \\end{bmatrix} \\in \\mathcal{R^t}$ and $\\mathcal{L(X;Q_\\theta)}$ is the output of LLM model in which $\\mathcal{L(X;Q_\\theta)} = \\begin{bmatrix} -\\log{Q_\\theta(x_1)}\\\\ \\vdots\\\\ -\\log{Q_\\theta(x_t \\mid x_{\u003c t})}\\\\ \\end{bmatrix}$\n\nWe typically have a constraint on the number of tokens the model can process. The largest version of GPT-2, for example, has a fixed length of 1024 tokens, so we cannot calculate $Q_\\theta(x_t \\mid x{\u003c t})$ directly when $t$ is greater than 1024.\n\nOne approach to solve this problem is break the sequence into disjoint subsequences equal to the model’s maximum input size and add up the decomposed perplexity of each subsequence independently.\n\n![disjoint approach](ppl_chunked.gif)\n\nThis is quick to compute since the perplexity of each segment can be computed in one forward pass, but serves as a poor approximation of the fully-factorized perplexity and will typically yield a higher (worse) perplexity because the model will have less context at most of the prediction steps.\n\nInstead, the perplexity of fixed-length models should be evaluated with a sliding-window strategy. This involves repeatedly sliding the context window so that the model has more context when making each prediction.\n\n![sliding approach](ppl_sliding.gif)\n\nThis is a closer approximation to the true decomposition of the sequence probability and will typically yield a more favorable score. The downside is that it requires a separate forward pass for each token in the corpus. A good practical compromise is to employ a strided sliding window, moving the context by larger strides rather than sliding by 1 token a time. This allows computation to proceed much faster while still giving the model a large context to make predictions at each step.\n\n## Reference\n\n* [Wikipedia - Perplexity](https://en.wikipedia.org/wiki/Perplexity)\n* [Article One: Two minutes NLP — Perplexity explained with simple probabilities](https://www.cs.bu.edu/fac/snyder/cs505/PerplexityPosts.pdf)\n* [Perplexity of fixed-length models](https://huggingface.co/docs/transformers/perplexity)\n* [HuggingFace Evaluation Metric: perplexity](https://huggingface.co/spaces/evaluate-metric/perplexity)\n* [Evaluation Metrics for Language Modeling](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)",
              "slug": "perplexity",
              "date": "2024-10-14 23:41:47",
              "lang": "en",
              "tags": [
                "Perplexity",
                "NLP",
                "information theory",
                "cross-entropy",
                "LLM",
                "transformer"
              ],
              "path": "/perplexity/index.html"
            },
            "lang": "en",
            "_nextI18Next": {
              "initialI18nStore": {
                "en": {
                  "navbar": {
                    "Home": "Home",
                    "Language": "Language",
                    "Tags": "Tags",
                    "en": "English",
                    "es": "Spanish",
                    "zh": "Chinese"
                  },
                  "post-layout": {
                    "Author": "Author: {{author}}",
                    "Tags": "Tags"
                  }
                }
              },
              "initialLocale": "en",
              "ns": ["navbar", "post-layout"],
              "userConfig": {
                "i18n": {
                  "defaultLocale": "en",
                  "locales": ["en", "es", "zh"]
                },
                "default": {
                  "i18n": {
                    "defaultLocale": "en",
                    "locales": ["en", "es", "zh"]
                  }
                }
              }
            }
          },
          "__N_SSG": true
        },
        "page": "/posts/[slug]",
        "query": { "slug": "perplexity" },
        "buildId": "fixed",
        "isFallback": false,
        "gsp": true,
        "scriptLoader": []
      }
    </script>
  </body>
</html>
