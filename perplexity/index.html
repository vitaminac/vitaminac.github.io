<!doctype html>
<html lang="en">
  <!-- Head tag -->
  <!-- SEO -->

  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <title>Perplexity｜Gao&#39;s Blog</title>
    <meta property="og:title" content="Perplexity｜Gao&#39;s Blog" />

    <meta name="description" content="Perplexity｜Gao&#39;s Blog" />
    <meta property="og:description" content="Perplexity｜Gao&#39;s Blog" />
    <meta name="author" content="Gao" />
    <meta name="robots" content="index, follow" />
    <meta
      property="og:image"
      content="https://vitaminac.github.io/images/favicon.jpg"
    />
    <link rel="shortcut icon" href="/images/favicon.jpg" />

    <meta
      name="keywords"
      content="Perplexity,NLP,information theory,cross-entropy,LLM,transformer"
    />

    <meta name="theme-color" content="#600090" />
    <meta name="msapplication-navbutton-color" content="#600090" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="#600090" />
    <link
      rel="alternate"
      type="application/atom+xml"
      title="Gao&#39;s Blog"
      href="/atom.xml"
    />

    <link
      rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/animate.css/3.5.2/animate.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css"
      cross-origin="anonymous"
      referrer-policy="no-referrer"
    />
    <link rel="canonical" href="https://vitaminac.github.io/perplexity/" />

    <!-- jQuery -->
    <script
      type="text/javascript"
      src="https://code.jquery.com/jquery-3.6.0.min.js"
    ></script>
    <!-- Bootstrap -->
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css"
      integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC"
      crossorigin="anonymous"
    />
    <script
      src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.2/dist/umd/popper.min.js"
      integrity="sha384-IQsoLXl5PILFhosVNubq5LC7Qb9DXgDA9i+tQ8Zj3iwWAwPtgFTxbJ8NT4GN1R8p"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.min.js"
      integrity="sha384-cVKIPhGWiC2Al4u+LWgxfKTRIcfu0JTxR+EQDz/bgldoEyl4H0zUF0QKbrJ0EcQF"
      crossorigin="anonymous"
    ></script>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/blog-style.css" />
    <link rel="stylesheet" href="/css/syntax.css" />

    <!-- Google Tag Manager -->

    <script>
      (function (w, d, s, l, i) {
        w[l] = w[l] || [];
        w[l].push({ "gtm.start": new Date().getTime(), event: "gtm.js" });
        var f = d.getElementsByTagName(s)[0],
          j = d.createElement(s),
          dl = l != "dataLayer" ? "&l=" + l : "";
        j.async = true;
        j.src = "https://www.googletagmanager.com/gtm.js?id=" + i + dl;
        f.parentNode.insertBefore(j, f);
      })(window, document, "script", "dataLayer", "GTM-WDC9JKC");
    </script>

    <!-- End Google Tag Manager -->

    <!-- Global site tag (gtag.js) - Google Analytics -->

    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-151409235-1"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "UA-151409235-1");
    </script>

    <!-- Google AdSense -->
    <script
      data-ad-client="ca-pub-8356359077918854"
      async
      src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"
    ></script>
    <meta name="generator" content="Hexo 6.3.0" />
  </head>
  <style>
    header.intro-header {
      background-image: url("/images/header.gif");
    }
  </style>
  <!-- hack iOS CSS :active style -->

  <body ontouchstart="" class="animated fadeIn">
    <!-- Main Content -->

    <!--only post-->

    <img
      class="wechat-title-img"
      src="/_next/static/media/default-post-preview.c96b099a.jpeg"
    />

    <!-- Post Content -->
    <article class="container" style="width: 100%; max-width: 100%">
      <!-- Post Container -->
      <div class="post-container">
        <h2 id="Introduction">
          <a href="#Introduction" class="headerlink" title="Introduction"></a
          >Introduction
        </h2>
        <p>
          The perplexity is a widely used evaluation metric in natural language
          processing (NLP) that measures how well a auto-regressive&#x2F;causal
          language model predicts a sample text. It is not well defined for
          masked language models.
        </p>
        <h2 id="Defition-of-perplexity-of-discrete-probability-distribution">
          <a
            href="#Defition-of-perplexity-of-discrete-probability-distribution"
            class="headerlink"
            title="Defition of perplexity of discrete probability distribution"
          ></a
          >Defition of perplexity of discrete probability distribution
        </h2>
        <p>
          In information theory, perplexity is a measure of uncertainty in the
          value of a sample from a discrete probability distribution. The larger
          the perplexity, the less likely it is that an observer can guess the
          value which will be drawn from the distribution. The perplexity $PP$
          of a discrete probability distribution $p$ is defined as
        </p>
        <p>
          $$<br />PP(p) :&#x3D; b^{H(p)} &#x3D; b^{-\sum_{x}{p(x)}log_{b}p(x)}
          &#x3D; \prod_{x}p(x)^{-p(x)}<br />$$
        </p>
        <p>
          where $H(p)$ is the entropy in bits of the distribution, and $x$
          ranges over the events. The perplexity is independent of the base $b$,
          the $b$ is customarily 2 but doesn’t need not be 2, while the entropy
          and the exponentiation use the same base should be enough.
        </p>
        <h2 id="Evaluating-probability-model-using-perplexity">
          <a
            href="#Evaluating-probability-model-using-perplexity"
            class="headerlink"
            title="Evaluating probability model using perplexity"
          ></a
          >Evaluating probability model using perplexity
        </h2>
        <p>
          Many time probability distribution $p$ is unknown, and one may propose
          an probability model $q$ based on samples that were drawn from $p$, we
          may evaluate $q$ by asking how well it predicts a separate test
          samples $x_1$, $x_2$, …, $x_n$ also drawn from $p$. The perplexity of
          the model $q$ is defined as
        </p>
        <p>
          $$<br />PP(p) :&#x3D;
          b^{-\frac{1}{N}\sum_{i&#x3D;1}^{N}{log_{b}q(x_i)}}<br />$$
        </p>
        <h2 id="Calcuting-of-perplexity-of-a-tokenized-sequence">
          <a
            href="#Calcuting-of-perplexity-of-a-tokenized-sequence"
            class="headerlink"
            title="Calcuting of perplexity of a tokenized sequence"
          ></a
          >Calcuting of perplexity of a tokenized sequence
        </h2>
        <p>
          Given a tokenized sequence $X&#x3D;(x_1,\dots,x_t)$ and an casual
          language model $Q_\theta$, the probability $P$ of the sequence can be
          defined as
        </p>
        <p>
          $$<br />\begin{split}<br />P(X;Q_\theta) &amp;:&#x3D; Q_{\theta}(x_1)
          Q_{\theta}(x_2 \mid x_1) \dots Q_{\theta}(x_t \mid x_{&lt; t})\\\\<br />&amp;&#x3D;
          \prod_{i&#x3D;1}^{t} Q_{\theta}(x_i \mid x_{&lt; i})<br />\end{split}<br />$$
        </p>
        <p>
          where $Q_\theta(x_i \mid x_{&lt; i})$ is the probability assigns to
          the i-th token conditioned on the preceding tokens $x_{&lt; i}$
          according to our model
        </p>
        <p>
          It would be nice to compare the probabilities assigned to different
          sentences to see which sentences are better predicted by the language
          model. However, since the probability of a sentence is obtained from a
          product of probabilities, the longer is the sentence the lower will be
          its probability (since it’s a product of factors with values smaller
          thanone). We should find a way of measuring these sentence
          probabilities, without the influence of the sentence length. This can
          be done by normalizing the sentence probability by the number of words
          in the sentence. Since the probability of a sentence is obtained by
          multiplying many factors, we can average them using the geometric
          mean. So the normalized probability of the tokenized sequence is then
        </p>
        <p>
          $$<br />P_{\text{nomalized}}(X;Q_\theta) :&#x3D;
          \sqrt[t]{\prod_{i&#x3D;1}^{t} Q_{\theta}(x_i \mid x_{&lt; i})}<br />$$
        </p>
        <p>The perplexity is just the reciprocal of this expression</p>
        <p>
          $$<br />\begin{split}<br />PP(X;Q_\theta) &amp;:&#x3D;
          \frac{1}{P_{\text{nomalized}}(X;Q_\theta)}\\\\<br />&amp;&#x3D;
          \frac{1}{\sqrt[t]{\prod_{i&#x3D;1}^{t} Q_{\theta}(x_i \mid x_{&lt;
          i})}}\\\\<br />&amp;&#x3D; (\prod_{i&#x3D;1}^{t} Q_{\theta}(x_i \mid
          x_{&lt; i}))^{-\frac{1}{t}}\\\\<br />&amp;&#x3D; b^{-\frac{1}{t}
          \sum_{i&#x3D;1}^{t}{\log_{b}{Q_\theta(x_i \mid x_{&lt; i})}}}<br />\end{split}<br />$$
        </p>
        <p>
          We have the same equation as perplexity for probability model, a
          language model is nothing more than a probability model trained over
          tokenized sequences. Since the tokenization procedure has a direct
          impact on a model’s perplexity which should always be taken into
          consideration when comparing different models.
        </p>
        <p>
          The perplexity is convenient for casual language model with
          decoder-only transformer architure evaluation because usually cross
          entropy loss function is used in which the negative log-likelihood is
          already computed during the inference process. The model inference
          will calculate the negative log-likelihood conditional to context of
          all target token you pass in a single forward calculation. Using $e$
          as $b$, the calculation can be simplied to
        </p>
        <p>
          $$<br />PP(X;Q_\theta) :&#x3D; e^{\frac{1}{t}\boldsymbol{1_{n}} \cdot
          \mathcal{L(X;\theta)}}<br />$$
        </p>
        <p>
          where $\boldsymbol{1_{n}}$ is $\begin{bmatrix} 1,\dots,1 \end{bmatrix}
          \in \mathcal{R^t}$ and $\mathcal{L(X;Q_\theta)}$ is the output of LLM
          model in which $\mathcal{L(X;Q_\theta)} &#x3D; \begin{bmatrix}
          -\log{Q_\theta(x_1)}\\\\ \vdots\\\\ -\log{Q_\theta(x_t \mid x_{&lt;
          t})}\\\\ \end{bmatrix}$
        </p>
        <p>
          We typically have a constraint on the number of tokens the model can
          process. The largest version of GPT-2, for example, has a fixed length
          of 1024 tokens, so we cannot calculate $Q_\theta(x_t \mid x{&lt; t})$
          directly when $t$ is greater than 1024.
        </p>
        <p>
          One approach to solve this problem is break the sequence into disjoint
          subsequences equal to the model’s maximum input size and add up the
          decomposed perplexity of each subsequence independently.
        </p>
        <p><img src="ppl_chunked.gif" alt="disjoint approach" /></p>
        <p>
          This is quick to compute since the perplexity of each segment can be
          computed in one forward pass, but serves as a poor approximation of
          the fully-factorized perplexity and will typically yield a higher
          (worse) perplexity because the model will have less context at most of
          the prediction steps.
        </p>
        <p>
          Instead, the perplexity of fixed-length models should be evaluated
          with a sliding-window strategy. This involves repeatedly sliding the
          context window so that the model has more context when making each
          prediction.
        </p>
        <p><img src="ppl_sliding.gif" alt="sliding approach" /></p>
        <p>
          This is a closer approximation to the true decomposition of the
          sequence probability and will typically yield a more favorable score.
          The downside is that it requires a separate forward pass for each
          token in the corpus. A good practical compromise is to employ a
          strided sliding window, moving the context by larger strides rather
          than sliding by 1 token a time. This allows computation to proceed
          much faster while still giving the model a large context to make
          predictions at each step.
        </p>
        <h2 id="Reference">
          <a href="#Reference" class="headerlink" title="Reference"></a
          >Reference
        </h2>
        <ul>
          <li>
            <a
              target="_blank"
              rel="noopener"
              href="https://en.wikipedia.org/wiki/Perplexity"
              >Wikipedia - Perplexity</a
            >
          </li>
          <li>
            <a
              target="_blank"
              rel="noopener"
              href="https://www.cs.bu.edu/fac/snyder/cs505/PerplexityPosts.pdf"
              >Article One: Two minutes NLP — Perplexity explained with simple
              probabilities</a
            >
          </li>
          <li>
            <a
              target="_blank"
              rel="noopener"
              href="https://huggingface.co/docs/transformers/perplexity"
              >Perplexity of fixed-length models</a
            >
          </li>
          <li>
            <a
              target="_blank"
              rel="noopener"
              href="https://huggingface.co/spaces/evaluate-metric/perplexity"
              >HuggingFace Evaluation Metric: perplexity</a
            >
          </li>
          <li>
            <a
              target="_blank"
              rel="noopener"
              href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/"
              >Evaluation Metrics for Language Modeling</a
            >
          </li>
        </ul>

        <hr />

        <ul class="list-group"></ul>
      </div>
    </article>
    <!-- Custom Theme JavaScript -->
    <script type="text/javascript" src="/js/blog.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.min.js"></script>
    <script>
      $(document).ready(function () {
        anchors.add("p");
      });
    </script>

    <!-- mathjax -->
    <script type="text/x-mathjax-config">
          MathJax.Hub.Config({
          jax: ["input/TeX", "output/SVG"],
          extensions: ["tex2jax.js"],
          showMathMenu: false,
          SVG: { useGlobalCache: false },
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"] ],
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
              processEscapes: true
          },
          TeX: {
              equationNumbers: { autoNumber: "AMS" },
              noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
              Macros: { href: "{}" }
          }
      });
    </script>
    <script
      type="text/javascript"
      async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML-full"
    ></script>

    <!-- jquery.tagcloud.js -->
    <script>
      // only load tagcloud.js in tag.html
      if ($("#tag_cloud").length !== 0) {
        async("https://vitaminac.github.io/js/jquery.tagcloud.js", function () {
          $.fn.tagcloud.defaults = {
            //size: {start: 1, end: 1, unit: 'em'},
            color: {
              start: "#bbbbee",
              end: "#0085a1",
            },
          };
          $("#tag_cloud a").tagcloud();
        });
      }
    </script>

    <script type="text/javascript" src="/js/common.js"></script>

    <!-- render ipynb file -->

    <!-- render ipynb file -->
  </body>
</html>
