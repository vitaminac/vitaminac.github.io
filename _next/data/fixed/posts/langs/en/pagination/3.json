{
  "pageProps": {
    "lang": "en",
    "pagination": 3,
    "allPostsData": [
      {
        "title": "Perplexity",
        "markdownContentSource": "\n## Introduction\n\nThe perplexity is a widely used evaluation metric in natural language processing (NLP) that measures how well a auto-regressive/causal language model predicts a sample text. It is not well defined for masked language models.\n\n## Defition of perplexity of discrete probability distribution\n\nIn information theory, perplexity is a measure of uncertainty in the value of a sample from a discrete probability distribution. The larger the perplexity, the less likely it is that an observer can guess the value which will be drawn from the distribution. The perplexity $PP$ of a discrete probability distribution $p$ is defined as\n\n$$\nPP(p) := b^{H(p)} = b^{-\\sum_{x}{p(x)}log_{b}p(x)} = \\prod_{x}p(x)^{-p(x)}\n$$\n\nwhere $H(p)$ is the entropy in bits of the distribution, and $x$ ranges over the events. The perplexity is independent of the base $b$, the $b$ is customarily 2 but doesn't need not be 2, while the entropy and the exponentiation use the same base should be enough.\n\n## Evaluating probability model using perplexity\n\nMany time probability distribution $p$ is unknown, and one may propose an probability model $q$ based on samples that were drawn from $p$, we may evaluate $q$ by asking how well it predicts a separate test samples $x_1$, $x_2$, ..., $x_n$ also drawn from $p$. The perplexity of the model $q$ is defined as\n\n$$\nPP(p) := b^{-\\frac{1}{N}\\sum_{i=1}^{N}{log_{b}q(x_i)}}\n$$\n\n## Calcuting of perplexity of a tokenized sequence\n\nGiven a tokenized sequence $X=(x_1,\\dots,x_t)$ and an casual language model $Q_\\theta$, the probability $P$ of the sequence can be defined as\n\n$$\n\\begin{split}\nP(X;Q_\\theta) &:= Q_{\\theta}(x_1) Q_{\\theta}(x_2 \\mid x_1) \\dots Q_{\\theta}(x_t \\mid x_{< t})\\\\\n&= \\prod_{i=1}^{t} Q_{\\theta}(x_i \\mid x_{< i})\n\\end{split}\n$$\n\nwhere $Q_\\theta(x_i \\mid x_{< i})$ is the probability assigns to the i-th token conditioned on the preceding tokens $x_{< i}$ according to our model\n\nIt would be nice to compare the probabilities assigned to different sentences to see which sentences are better predicted by the language model. However, since the probability of a sentence is obtained from a product of probabilities, the longer is the sentence the lower will be its probability (since it’s a product of factors with values smaller thanone). We should find a way of measuring these sentence probabilities, without the influence of the sentence length. This can be done by normalizing the sentence probability by the number of words in the sentence. Since the probability of a sentence is obtained by multiplying many factors, we can average them using the geometric mean. So the normalized probability of the tokenized sequence is then\n\n$$\nP_{\\text{nomalized}}(X;Q_\\theta) := \\sqrt[t]{\\prod_{i=1}^{t} Q_{\\theta}(x_i \\mid x_{< i})}\n$$\n\nThe perplexity is just the reciprocal of this expression\n\n$$\n\\begin{split}\nPP(X;Q_\\theta) &:= \\frac{1}{P_{\\text{nomalized}}(X;Q_\\theta)}\\\\\n&= \\frac{1}{\\sqrt[t]{\\prod_{i=1}^{t} Q_{\\theta}(x_i \\mid x_{< i})}}\\\\\n&= (\\prod_{i=1}^{t} Q_{\\theta}(x_i \\mid x_{< i}))^{-\\frac{1}{t}}\\\\\n&= b^{-\\frac{1}{t} \\sum_{i=1}^{t}{\\log_{b}{Q_\\theta(x_i \\mid x_{< i})}}}\n\\end{split}\n$$\n\nWe have the same equation as perplexity for probability model, a language model is nothing more than a probability model trained over tokenized sequences. Since the tokenization procedure has a direct impact on a model’s perplexity which should always be taken into consideration when comparing different models.\n\nThe perplexity is convenient for casual language model with decoder-only transformer architure evaluation because usually cross entropy loss function is used in which the negative log-likelihood is already computed during the inference process. The model inference will calculate the negative log-likelihood conditional to context of all target token you pass in a single forward calculation. Using $e$ as $b$, the calculation can be simplied to\n\n$$\nPP(X;Q_\\theta) := e^{\\frac{1}{t}\\boldsymbol{1_{n}} \\cdot \\mathcal{L(X;\\theta)}}\n$$\n\nwhere $\\boldsymbol{1_{n}}$ is $\\begin{bmatrix} 1,\\dots,1 \\end{bmatrix} \\in \\mathcal{R^t}$ and $\\mathcal{L(X;Q_\\theta)}$ is the output of LLM model in which $\\mathcal{L(X;Q_\\theta)} = \\begin{bmatrix} -\\log{Q_\\theta(x_1)}\\\\ \\vdots\\\\ -\\log{Q_\\theta(x_t \\mid x_{< t})}\\\\ \\end{bmatrix}$\n\nWe typically have a constraint on the number of tokens the model can process. The largest version of GPT-2, for example, has a fixed length of 1024 tokens, so we cannot calculate $Q_\\theta(x_t \\mid x{< t})$ directly when $t$ is greater than 1024.\n\nOne approach to solve this problem is break the sequence into disjoint subsequences equal to the model’s maximum input size and add up the decomposed perplexity of each subsequence independently.\n\n![disjoint approach](ppl_chunked.gif)\n\nThis is quick to compute since the perplexity of each segment can be computed in one forward pass, but serves as a poor approximation of the fully-factorized perplexity and will typically yield a higher (worse) perplexity because the model will have less context at most of the prediction steps.\n\nInstead, the perplexity of fixed-length models should be evaluated with a sliding-window strategy. This involves repeatedly sliding the context window so that the model has more context when making each prediction.\n\n![sliding approach](ppl_sliding.gif)\n\nThis is a closer approximation to the true decomposition of the sequence probability and will typically yield a more favorable score. The downside is that it requires a separate forward pass for each token in the corpus. A good practical compromise is to employ a strided sliding window, moving the context by larger strides rather than sliding by 1 token a time. This allows computation to proceed much faster while still giving the model a large context to make predictions at each step.\n\n## Reference\n\n* [Wikipedia - Perplexity](https://en.wikipedia.org/wiki/Perplexity)\n* [Article One: Two minutes NLP — Perplexity explained with simple probabilities](https://www.cs.bu.edu/fac/snyder/cs505/PerplexityPosts.pdf)\n* [Perplexity of fixed-length models](https://huggingface.co/docs/transformers/perplexity)\n* [HuggingFace Evaluation Metric: perplexity](https://huggingface.co/spaces/evaluate-metric/perplexity)\n* [Evaluation Metrics for Language Modeling](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)",
        "slug": "perplexity",
        "date": "2024-10-14 23:41:47",
        "lang": "en",
        "tags": [
          "Perplexity",
          "NLP",
          "information theory",
          "cross-entropy",
          "LLM",
          "transformer"
        ],
        "path": "/perplexity/index.html"
      },
      {
        "title": "Vector Database",
        "markdownContentSource": "\nA vector database indexes and stores vector embeddings for fast retrieval and similarity search, with capabilities like CRUD operations. Vector embeddings are generated by large language models.\n\n![vector database](vector-database.webp)\n\n1. Use the embedding model to create vector embeddings for the content we want to index.\n1. The vector embedding is inserted into the vector database, with some reference to the original content the embedding was created from.\n1. When the application issues a query, use the same embedding model to create embeddings for the query and use those embeddings to query the database for similar vector embeddings.\n1. Tracking back the original content associated with most similar vector embeddings.\n\n![pipeline](pipeline.webp)\n\n1. Indexing: The vector database create indexes for vector embedding using an algorithm such as Random Projection, Product Quantization, Locality-sensitive hashing, or Hierarchical Navigable Small World (more on these below). This step maps the vector embedding to a data structure that will enable faster searching.\n1. Querying: The vector database compute the index of query vector to find the approximated nearest neighbors vector embedding applying a similarity metric.\n1. Post Processing: The vector database retrieves the final nearest neighbors from the dataset and compute the similarity and re-ranking the nearest neighbors vector embedding.\n\nIn vector databases, a similarity metric is applied to rank a vector that is the most similar to query vector, there is different similarity measure. The basic rule of thumb in selecting the best similarity metric is to match it to the one used to train the embedding model, which is usually the dot product.\n\n## Reference\n\n* https://www.pinecone.io/learn/vector-database/\n* https://www.pinecone.io/learn/vector-similarity/\n* https://www.pinecone.io/learn/series/faiss/",
        "slug": "vector-database",
        "date": "2024-09-29 19:19:40",
        "lang": "en",
        "tags": ["vector database"],
        "path": "/vector-database/index.html"
      },
      {
        "title": "Roadmap Toward AI",
        "markdownContentSource": "\n* [CMU B.S. AI Curriculum](https://www.cs.cmu.edu/bs-in-artificial-intelligence/curriculum)\n\n# Mathematics\n\n## Courses\n\n- [x] [Mathematics for Machine Learning Specialization](https://www.coursera.org/specializations/mathematics-machine-learning)\n\n## Books\n\n- [ ] [Mathematics for Machine Learning](https://mml-book.github.io/)\n- [ ] [Algebra, Topology, Differential Calculus, and Optimization Theory for Computer Science and Machine Learning](https://www.cis.upenn.edu/~jean/gbooks/geomath.html)\n- [ ] [Linear Algebra and Optimization with Applications to Machine Learning](https://www.cis.upenn.edu/~jean/gbooks/linalg.html)\n\n# Numerical Optimization\n\n## Books\n\n- [ ] [Numerical Recipes](http://numerical.recipes/)\n- [ ] [Numerical Optimization](https://link.springer.com/book/10.1007/978-0-387-40065-5)\n- [ ] [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/)\n- [ ] Numerical Analysis\n\n# Statistical Learning\n\n## Books\n\n- [ ] [An Introduction to Statistical Learning](https://www.statlearning.com/)\n- [ ] [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)\n- [ ] [统计学习方法](https://book.douban.com/subject/33437381/)\n- [ ] [The Nature of Statistical Learning Theory](https://www.springer.com/gp/book/9780387987804)\n- [ ] [Statistical Learning Theory](https://www.wiley.com/en-us/Statistical+Learning+Theory-p-9780471030034)\n- [ ] [Python for Data Analysis](https://wesmckinney.com/book/)\n\n# Artificial Intelligence\n\n## Tutorials\n\n- [ ] [Microsoft: Artificial Intelligence for Beginners - A Curriculum](https://microsoft.github.io/AI-For-Beginners/)\n\n## Courses\n\n- [x] [Coursera: AI for Everyone](https://www.coursera.org/learn/ai-for-everyone)\n- [ ] [UC Berkeley CS188: Introduction to Artificial Intelligence](https://inst.eecs.berkeley.edu/~cs188)\n- [ ] [Stanford CS221: Artificial Intelligence: Principles and Techniques](https://stanford-cs221.github.io/)\n\n## Books\n\n- [ ] [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/)\n\n# Machine Learning\n\n## Tutorials\n\n- [x] [Kaggle: Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)\n- [x] [Kaggle: Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)\n- [x] [Kaggle: Feature Engineering](https://www.kaggle.com/learn/feature-engineering)\n- [ ] [Stanford CS 229 Cheatsheets](https://github.com/afshinea/stanford-cs-229-machine-learning)\n\n## Courses\n\n- [ ] [Coursera: Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)\n- [ ] [Coursera: Machine Learning in Production](https://www.coursera.org/learn/introduction-to-machine-learning-in-production)\n- [ ] [Stanford CS229: Machine Learning](https://cs229.stanford.edu/syllabus-fall2022.html)\n- [ ] [Stanford CS228: Probabilistic Graphical Models](https://cs.stanford.edu/~ermon/cs228)\n- [ ] [NTU by Hung-Yi Lee: Machine Learning](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php)\n\n## Books\n\n- [ ] [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/)\n- [ ] [Machine Learning Yearning](https://www.deeplearning.ai/machine-learning-yearning/)\n- [ ] [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/prml-errata-3rd-20110921.pdf)\n- [ ] [Probabilistic Machine Learning Book Series](https://probml.github.io/pml-book/)\n- [ ] [机器学习](https://book.douban.com/subject/26708119/)\n- [ ] [南瓜书PumpkinBook](https://datawhalechina.github.io/pumpkin-book)\n- [ ] [Probabilistic Graphical Models: Principles and Techniques](https://mitpress.mit.edu/books/probabilistic-graphical-models)\n- [ ] [Foundations of Machine Learning](https://mitpress.mit.edu/9780262039406/foundations-of-machine-learning/)\n- [ ] Probably Approximately Correct\n\n## Methods\n\n* Supervised Learning\n  * K-Nearest Neighbors\n* Unsupervised Learning\n  * Clustering\n    * K-Means\n    * Elbow Method\n* Online Leaning\n* [Active Learning](https://paperswithcode.com/task/active-learning)\n* [Meta-Learning](https://paperswithcode.com/task/meta-learning)\n* Bayesian Network\n* Boltzmann Machine\n\n# Deep Learning\n\n## Tutorials\n\n- [ ] [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)\n- [ ] [UFLDL Tutorial](http://deeplearning.stanford.edu/tutorial/)\n- [ ] [Stanford CS 230 Cheatsheets](https://github.com/afshinea/stanford-cs-230-deep-learning)\n\n## Courses\n\n- [x] [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)\n- [ ] [Stanford CS230: Deep Learning](https://cs230.stanford.edu/lecture/)\n- [ ] [MIT 6.S191 Introduction to Deep Learning](http://introtodeeplearning.com/)\n- [ ] [fast.ai: Practical Deep Learning for Coders](https://course.fast.ai/)\n- [ ] [CMU 10-414/714: Deep Learning Systems: Algorithms and Implementation](https://dlsyscourse.org/)\n- [ ] [DeepMind x UCL: The Deep Learning Lecture](https://deepmind.com/learning-resources/deep-learning-lecture-series-2020)\n\n## Books\n\n- [ ] [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python-second-edition)\n- [ ] [Deep Learning](https://www.deeplearningbook.org/)\n- [ ] [Dive into Deep Learning](https://d2l.ai/)\n- [ ] [Understanding Deep Learning](https://udlbook.github.io/udlbook/)\n- [ ] [Deep Learning: Foundations and Concepts](https://www.bishopbook.com/)\n\n## Tools\n\n* [PlotNeuralNet](https://github.com/HarisIqbal88/PlotNeuralNet)\n* [A Neural Network Playgroud](https://playground.tensorflow.org/)\n\n## Libraries\n\n* [TensorBoard](https://github.com/tensorflow/tensorboard)\n\n# Computer Vision\n\n## Courses\n\n- [ ] [Stanford CS231n: Deep Learning for Computer Vision](https://cs231n.github.io/)\n\n## Books\n\n* [Computer Vision: Algorithms and Applications](http://szeliski.org/Book/)\n* [Computer Vision:  Models, Learning, and Inference](http://www.computervisionmodels.com/)\n\n## Tools\n\n* [OpenCV](https://opencv.org/)\n* [Tesseract OCR](https://github.com/tesseract-ocr/tesseract)\n* [EasyOCR](https://github.com/JaidedAI/EasyOCR)\n* [Teachable Machine](https://teachablemachine.withgoogle.com/)\n\n## Libraries\n\n* [TorchVision](https://github.com/pytorch/vision)\n* [scikit-image](https://scikit-image.org/)\n* [ImageAI](https://github.com/OlafenwaMoses/ImageAI)\n* [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose)\n\n## Projects\n\n* [video2x](https://github.com/k4yt3x/video2x)\n* [PaintsChainer](https://github.com/pfnet/PaintsChainer)\n\n# Natual Language Processing\n\n## Tutorials\n\n- [ ] [Scikit-learn: Text Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n- [ ] [NLP Course | For You](https://lena-voita.github.io/nlp_course.html)\n\n## Courses\n\n- [ ] [Stanford CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)\n- [ ] [Stanford CS224d: Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/)\n\n## Books\n\n* [Foundations of Statistical Natural Language Processing](https://nlp.stanford.edu/fsnlp/)\n* [Speech and Language Processing](https://home.cs.colorado.edu/~martin/slp.html)\n* [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/)\n\n## Tools\n\n* [DeepSpeech](https://github.com/mozilla/DeepSpeech)\n\n## Projects\n\n* [Natural Language Processing Tutorial for Deep Learning Researchers](https://github.com/graykode/nlp-tutorial)\n\n# Reinforcement Learning\n\n## Tutorials\n\n- [ ] [Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)\n\n## Courses\n\n- [ ] [UC Berkeley CS285/294 Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse/)\n- [ ] [DeepMind x UCL: Introduction to Reinforcement Learning with David Silver](https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver)\n- [ ] [Stanford CS234: Reinforcement Learning](http://web.stanford.edu/class/cs234/index.html)\n\n## Books\n\n* [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html)\n* [蘑菇书EasyRL](https://datawhalechina.github.io/easy-rl)\n\n## Methods\n\n* Evolutionary Algorithm\n  * Genetic Algorithm\n  * Neuroevolution\n  * Ant Colony Optimization\n\n# Large Language Models\n\n## Tutorials\n\n- [ ] [Hugging Face: NLP Course](https://huggingface.co/learn/nlp-course/chapter1/1)\n- [ ] [Hugging Face: Conceptual Guides of Transformer](https://huggingface.co/docs/transformers/philosophy)\n- [ ] [Microsoft: Generative AI for Beginners - A Course](https://microsoft.github.io/generative-ai-for-beginners)\n- [ ] [Cohere: LLM University - Large Language Models](https://cohere.com/llmu)\n- [ ] [LLM101n](https://github.com/karpathy/LLM101n)\n\n## Courses\n\n- [ ] [Coursera: Generative AI for Everyone](https://www.coursera.org/learn/generative-ai-for-everyone)\n- [x] [DeepLearning.AI: ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\n- [ ] [DeepLearning.AI: Building Systems with the ChatGPT API](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/)\n- [ ] [Coursera: Generative AI with Large Language Models](https://www.coursera.org/learn/generative-ai-with-llms)\n- [ ] [Stanford CS25: Transformers](https://web.stanford.edu/class/cs25/)\n- [ ] [Stanford CS324: Large Language Models](https://stanford-cs324.github.io/winter2023/)\n- [ ] [DeepLearning.AI: LangChain for LLM Application Development](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)\n\n## Books\n\n- [ ] [Build a Large Language Model (From Scratch)](https://github.com/rasbt/LLMs-from-scratch)\n\n## Libraries\n\n* [TRL - Transformer Reinforcement Learning](https://github.com/lvwerra/trl)\n* [Trax](https://github.com/google/trax)\n* [AlpacaFarm](https://github.com/tatsu-lab/alpaca_farm)\n\n## Projects\n\n* [nanoGPT](https://github.com/karpathy/nanoGPT)\n* [minGPT](https://github.com/karpathy/minGPT)\n\n# Papers\n\n* [深度学习论文精读](https://github.com/mli/paper-reading)\n* [StoryGAN](https://paperswithcode.com/paper/storygan-a-sequential-conditional-gan-for)\n\n# Datasets\n\n* [Kaggle: Datasets](https://www.kaggle.com/datasets)\n* [Papers With Code: Datasets](https://paperswithcode.com/datasets)\n* [Google Dataset Search](https://datasetsearch.research.google.com/)\n* [Hugging Face: Datasets](https://huggingface.co/datasets)\n\n# Open Source Models\n\n* [Kaggle: Models](https://www.kaggle.com/models)\n* [Hugging Face: Models](https://huggingface.co/models)\n* [Pytorch Hub](https://pytorch.org/hub/)\n* [Tensorflow Hub](https://tfhub.dev/)\n* [TensorFlow Model Garden](https://github.com/tensorflow/models)\n",
        "slug": "roadmap-ai",
        "date": "2024-09-15 00:20:21",
        "lang": "en",
        "tags": ["roadmap", "ai", "deep learning"],
        "path": "/roadmap-ai/index.html"
      },
      {
        "title": "DLL Hijacking via zlib DLL Proxy",
        "markdownContentSource": "\nDLL hijacking involves manipulating a program to load a DLL that contains the desired code. We will use a simple technique here for ilustration: DLL replacement. We will build a DLL that perserve the same functionlity and interface of original DLL. So we can swap the original DLL with a the one built by us that contains additional code.\n\nAssuming the vulnerable program has dependency on `zlib1.dll`, which is the open source library. We can first check the version from file properties and download the corresponding version of source code for example [zlib v1.2.1](https://github.com/madler/zlib/tree/v1.2.1).\n\nExecute `cd` into source source folder and then execute `nmake -f win32/Makefile.msc`, we will obtain [zlib.lib](/downloads/code/dll-hijacking/zlib.lib), [zlib1.res](/downloads/code/dll-hijacking/zlib1.res). We can now build our `zlib1.dll`. Put our source code [zlib1.c](/downloads/code/dll-hijacking/zlib1.c) with additional instantiation logic and copy `zlib.lib`, `zlib1.res`, [zlib.def](https://github.com/madler/zlib/blob/v1.2.1/win32/zlib.def) into the same folder and then execute following commands\n\n    cl /c /nologo /O2 /MD /utf-8 zlib1.c\n    link /NOLOGO /RELEASE /DEF:zlib.def /DLL /IMPLIB:zdll.lib /OUT:zlib1.dll zlib1.obj zlib.lib zlib1.res User32.lib Gdi32.lib\n\nIn case we don't have module-definition or `.def` file then we will need to modify the source code of our proxy DLL `zlib1.c`. We need to first execute `dumpbin /EXPORTS zlib1.dll` so we will obtain the export table of original `zlib1.dll`. The output should be something similar to following:\n\n          1    0 00001000 adler32\n          2    1 000011E0 compress\n          3    2 00001130 compress2\n          4    3 00001200 compressBound\n          5    4 00001510 crc32\n          6    5 00001760 deflate\n          7    6 00001680 deflateBound\n          8    7 00001C60 deflateCopy\n          9    8 00001BA0 deflateEnd\n         10    9 00003050 deflateInit2_\n         11    A 00003280 deflateInit_\n         12    B 00002CC0 deflateParams\n         13    C 00001640 deflatePrime\n         14    D 00002C20 deflateReset\n         15    E 00001530 deflateSetDictionary\n         16    F 00001220 get_crc_table\n         17   10 00003B20 gzclearerr\n         18   11 000039D0 gzclose\n         19   12 00003DC0 gzdopen\n         20   13 00003930 gzeof\n         21   14 00003A20 gzerror\n         22   15 00003880 gzflush\n         23   16 000040B0 gzgetc\n         24   17 000040E0 gzgets\n         25   18 00003DA0 gzopen\n         26   19 000036C0 gzprintf\n         27   1A 00003740 gzputc\n         28   1B 00003770 gzputs\n         29   1C 00003E00 gzread\n         30   1D 000038C0 gzrewind\n         31   1E 00004140 gzseek\n         32   1F 000032B0 gzsetparams\n         33   20 000042E0 gztell\n         34   21 000035A0 gzungetc\n         35   22 000035F0 gzwrite\n         36   23 00005950 inflate\n         37   24 000043C0 inflateBack\n         38   25 00005240 inflateBackEnd\n         39   26 00004300 inflateBackInit_\n         40   27 00007060 inflateCopy\n         41   28 00006DB0 inflateEnd\n         42   29 00005740 inflateInit2_\n         43   2A 00005810 inflateInit_\n         44   2B 000056F0 inflateReset\n         45   2C 00006E00 inflateSetDictionary\n         46   2D 00006F30 inflateSync\n         47   2E 00007030 inflateSyncPoint\n         48   2F 00009050 uncompress\n         49   30 00009120 zError\n         50   31 00009110 zlibCompileFlags\n         51   32 00009100 zlibVersion\n\nfor every export attribute we will need a add a line to our `zlib1.c`\n\n    #pragma comment(linker, \"/export:_<export attribute>\")\n\nfor example\n\n    #pragma comment(linker, \"/export:_zlibVersion\")\n\nWe place generated `zlib1.dll` into program directory and replace the original one, our logic will be executed when program try to load `zlib1.dll`.\n\nAlternatively for any other DLL even it is not open source, we can build a proxy DLL that redirect all the calls to original DLL file. There are automatic tool for generating the proxy DLL.\n\n### Reference\n\n* [API Interception via DLL Redirection](https://www.exploit-db.com/docs/english/13140-api-interception-via-dll-redirection.pdf)\n* [HackTricks: Dll Hijacking](https://book.hacktricks.xyz/windows-hardening/windows-local-privilege-escalation/dll-hijacking#dll-proxifying)\n* [Can we export a function made available through a static library](https://stackoverflow.com/a/45744068/9980245)\n* [creating dll with cl and linker](https://social.msdn.microsoft.com/Forums/vstudio/en-US/840b6359-c3aa-44b8-b3ef-50e3556e6430/creating-dll-with-cl-and-linker?forum=vclanguage)\n* [/MD, /MT, /LD (Use Run-Time Library)](https://docs.microsoft.com/en-us/cpp/build/reference/md-mt-ld-use-run-time-library?view=msvc-170)\n* [Compiler options listed alphabetically](https://docs.microsoft.com/en-us/cpp/build/reference/compiler-options-listed-alphabetically?view=msvc-170)\n* [Exporting from a DLL Using DEF Files](https://learn.microsoft.com/en-us/cpp/build/exporting-from-a-dll-using-def-files)\n* [/DEF (Specify Module-Definition File)](https://docs.microsoft.com/en-us/cpp/build/reference/def-specify-module-definition-file?view=msvc-170)\n* [c++ creating a window from a dll](https://sim0n.wordpress.com/2009/03/29/c-creating-a-window-from-a-dll/)\n* [Create your Proxy DLLs automatically](https://www.codeproject.com/Articles/16541/Create-your-Proxy-DLLs-automatically)\n* [ProxiFy - Automatic Proxy DLL Generation](https://www.codeproject.com/Articles/1179147/ProxiFy-Automatic-Proxy-DLL-Generation)\n* [SharpDllProxy](https://github.com/Flangvik/SharpDllProxy)\n* [DLL代理转发](https://www.cnblogs.com/hetianlab/p/14031412.html)",
        "slug": "dll-hijacking-via-zlib-dll-proxy",
        "date": "2024-07-21 11:59:24",
        "lang": "en",
        "tags": ["dll injection", "dll hijacking", "zlib", "外挂"],
        "path": "/dll-hijacking-via-zlib-dll-proxy/index.html"
      },
      {
        "title": "Flash crDroid ROM for Redmi Note 7",
        "markdownContentSource": "\n\n1. You need to have a Xiaomi account that has been given permission for unlocking device.\n1. Download MiFlash from https://www.miui.com/unlock/index.html\n1. Download the crDroid ROM for Redmi Note 7 https://crdroid.net/lavender/9\n1. Download firmware update from [1]\n1. Download NikGapps from https://sourceforge.net/projects/nikgapps/files/Releases/NikGapps-T/. Core variant is enough.\n1. Download OrangeFox recovery for Redmi Note 7 https://orangefox.download/en/device/lavender\n   * crDroid version 9.8 introduce dynamic partition. We need to use OrangeFox because is the one that support dynamic partition since https://orangefox.download/release/64e67545531cf4e269b8aaa3.\n1. Download Magisk from https://github.com/topjohnwu/Magisk/releases\n1. Connect the phone to your laptop and open MiFlash to unlock your device following the instruction.\n1. Reboot into bootloader mode\n1. Execute the command to detect your device `fastboot.exe devices`\n1. Flash OrangeFox recovery from bootloader with `fastboot.exe flash recovery recovery.img`\n1. Reboot into recovery mode `fastboot.exe reboot`\n1. Wipe Dalvik, cache, system, vendor from recovery\n1. Copy the downloaded files to your phone in recovery mode\n1. Flash firmware update from recovery\n1. Flash `crDroid` from recovery\n1. Reboot into system once and reboot again into recovery\n1. Flash `NikGapps` from recovery\n1. Boot into system\n1. Install `Magisk-XX.apk`\n1. Follow this instruction to complete `Magisk` installation https://topjohnwu.github.io/Magisk/install.html\n1. Install `F-Droid`\n1. Install `Aurora Store` from `F-Droid`\n1. Install `Gboard`\n1. Install `Chrome` from `Aurora Store`\n1. Install 腾讯应用宝 from `Chrome`\n\n## Reference\n\n1. https://xdaforums.com/t/rom-13-0-official-lavender-crdroid-v9-10-28-10-2023.4638733/\n2. https://xdaforums.com/t/closed-rom-13-0-official-lavender-crdroid-v9-8-14-08-2023.4523341/\n3. https://xdaforums.com/t/closed-rom-13-0-official-lavender-crdroid-v9-8-14-08-2023.4523341/page-4#post-88874307",
        "slug": "flash-crDroid-redmi-note-7",
        "date": "2024-05-25 18:11:53",
        "lang": "en",
        "tags": ["lavender", "crDroid"],
        "path": "/flash-crDroid-redmi-note-7/index.html"
      },
      {
        "title": "RaspberryPi Odoo",
        "markdownContentSource": "\n## Configure Prestashop\n\n### [Install Docker Engine](https://docs.docker.com/engine/install/debian/)\n\n### [Install Docker Compose](https://docs.docker.com/compose/install/)\n\nSince Docker Compose [stable release](https://github.com/docker/compose/releases/tag/1.29.2) doesn't provide binary for `armv7`, you need to try installing newer version [Docker Compose V2](https://docs.docker.com/compose/cli-command/#install-on-linux). To correctly install Docker Compose V2 we need to install it with `su` user.\n\n### Run Prestashop from Docker\n\n[MySQL docker image](https://hub.docker.com/_/mysql) and [Prestashop docker image](https://hub.docker.com/r/prestashop/prestashop/) don't provide support for `armv7l`. [MariaDB](https://hub.docker.com/_/mariadb) provides support for `arm64`. However, [Raspberry Pi OS is 32 bits](https://raspberrypi.stackexchange.com/a/101216), you can verify with `uname -m` that kernel is built for `armv7l`. Although [Raspberry Pi is working on 64 bits OS beta version](https://forums.raspberrypi.com/viewtopic.php?t=275370), the better alternative now is to use [alpine-prestashop](https://hub.docker.com/r/yobasystems/alpine-prestashop/) and [alpine-mariadb](https://hub.docker.com/r/yobasystems/alpine-mariadb/).\n\nExecute following commands\n\n    cd ~\n    [ -d prestashop ] || mkdir prestashop\n    cd prestashop\n    curl https://gist.githubusercontent.com/vitaminac/851e2caa9337b31568844cfe53ef6806/raw/fffde01a2e826630ed1c8f95b32c4586359a7053/raspberry-pi-prestashop-docker-compose.yml -o docker-compose.yml\n    sudo docker compose up\n\n### Complete Installation\n\nOpen http://localhost:8080 you will see the installation page of Prestashop, follow the instruction and complete the installation. In the **System configuration** you need to adjust the setting according to the environment variables of docker compose file.\n\n* Database server address: **mysql**\n* Database name: **prestadb**\n* Database login: **prestauser**\n* Database password: **prestapass**\n* Tables prefix: leave it as default\n* Drop existing tables: leave it as default\n\nAfter the installation is completed you need to delete the `install` folder with following command\n\n    sudo rm -rdf $(sudo docker volume inspect $(sudo docker volume ls | grep prestashop-php | awk '{print $2}') | grep Mountpoint | awk '{print substr($2, 2, length($2) - 3)}')/install\n\nNow you can access your front office of your Prestashop with http://localhost:8080 and back office with http://localhost:8080/admin4365218asd897fasd54f78asdf57/index.php.\n\n## Configure Odoo\n\n    sudo docker run -p 8069:8069 mahood73/rpi-odoo\n\n## Reference\n\n* https://gist.github.com/vitaminac/851e2caa9337b31568844cfe53ef6806\n* https://gist.github.com/vitaminac/a69d692f774bc0f37075371c1ace77c3",
        "slug": "RaspberryPi-Odoo",
        "date": "2024-01-02 21:12:35",
        "lang": "en",
        "tags": ["Linux", "Raspberry", "Prestashop", "Odoo", "Docker"],
        "path": "/RaspberryPi-Odoo/index.html"
      },
      {
        "title": "OpenWrt Installation and Configuration on D-LINK DIR-882-A1",
        "markdownContentSource": "\n## Download OpenWrt Firmware\n\nOpen https://openwrt.org/toh/d-link/dir-882_a1#installation and select *Firmware Install*. At this step we need to download **factory** version.\n\n## Flash OpenWrt Firmware\n\n1. Connect to D-Link router through Ethernet cable.\n2. Power off the router\n3. Hold on the reset key and restart the router, release the reset key until you see the LED starts blinking.\n4. On the computer, manually assign a static IP address on the 192.168.0.xxx subnet, other than 192.168.0.1 (e.g. 192.168.0.2)\n5. Open a web browser and navigate to [http://192.168.0.1](http://192.168.0.1)\n6. Choose the firmware file you have downloaded previously and click on **Upload**.\n7. After the file has been uploaded, you should see a *Device is upgrading the firmware* message on the web browse\n8. The device will reboot automatically after the flashing process finishes\n\n## Configuration through OpenWrt LuCI web interface\n\n* Change to DHCP mode after the router is restarted\n* The gateway by default is http://192.168.1.1 without password\n* System > Administration\n    * change router admin password\n    * SSH Access\n      * modify interface to LAN\n      * change SSH port to other\n    * SSH Keys\n      * past the content of your public key file. It will be a long string starting with `ssh-rsa …`\n* Network > Wireless > 2.4G AP & 5G AP\n  * Device Configuration\n    * Advance Settings: modify country code\n  * Interface Configuration\n    * General Setup: modify ESSID\n    * Wireless Security\n      * Encryption: WPA2-PSK\n      * Cipher: Force CCMP\n      * Key: Your Wi-Fi Password\n    * Advanced Settings\n      * uncheck *Disassociate On Low Acknowledgement*\n      * check *Disable Inactivity Polling*\n      * Time interval for rekeying GTK: 3600\n* Network > Wireless > 5G AP > Device Configuration\n  * Operating frequency\n    * Mode: **AC**\n    * Channel: 36\n    * Width **80mhz**\n* Network > Wireless: Save & Apply\n* Network > Interface\n  1. LAN > General Settings: modify IPv4 address to 192.168.0.1 if 192.168.1.1 as gateway has conflict with upstream\n  1. WAN > Advanced Settings: unchecked **Use DNS servers advertised by peer**\n  1. WAN > Advanced Settings: add and click **+** button for following DNS **1.1.1.1**, **8.8.8.8**, **8.8.4.4** in **Use custom DNS servers**\n  1. Save & Apply\n* Network > Firewall > General Settings\n    1. Routing/NAT Offloading\n        * check **Software flow offloading**\n        * check **Hardware flow offloading**\n    1. Save & Apply\n* Network > DHCP and DNS > Hostnames: add custom binding of domain name to static IP for example blocking a website. You might need to add two versions of mapping, one for IPv4 and one for IPv6.\n  * for blocking the domain it is recommended to use adblock\n\n## Configuration through OpenWrt SSH\n\n### Disabling password authentication\n\n    uci set dropbear.@dropbear[0].PasswordAuth=\"0\"\n    uci set dropbear.@dropbear[0].RootPasswordAuth=\"0\"\n    uci commit dropbear\n    /etc/init.d/dropbear restart\n\n## Obtain PPPoE Username & Password\n\nThe steps to obtain PPPoE will vary per country and operator. I've followed [this tutorial](https://www.frikidelto.com/tutorial/como-conseguir-el-usuario-y-contrasena-pppoe-para-instalar-un-router-neutro/).\n\n## Configure PPPoE\n\n1. Network> Interfaces > Devices: Add device configuration\n    1. Select VLAN (802.1q) as device type\n    2. Select WAN as base device\n    3. Introduce 20 in VLAN ID 20 (This configuration depends on the operator)\n    4. uncheck \"Enable IPv6\"\n    5. Save\n    6. Save & apply\n2. Network> Interfaces > Interface: click edit on WAN configuration\n    1. Change protocol to PPPoE\n    2. Select Software VLAN wan.20 as device\n    3. Introduce PPPoE username and password\n    4. Save\n    5. Save & apply\n\n## Adblock\n\n* System > Software: Update lists\n* System > Software: Search **curl** and install.\n* System > Software: Search `adblock`, `luci-app-adblock` and install.\n* Services > Adblock > Additional Settings > Download Utility: Select **curl**\n* Services > Edit Blacklist: add domain you want to block\n* Services > Adblock: Save & Apply\n\n## Other Utilities\n\n* System > Software: Search *htop* and install.\n\nYou can also access router OS through SSH with `ssh root@192.168.0.1` and install the packages with `opkg` command. It is recommended to not upgrade `luci-*` packages, this package is web interface of router, and it often generates conflict between the new version and old version.\n\n## Upgrade Firmware\n\nGo to https://firmware-selector.openwrt.org/?version=22.03.2&target=ramips%2Fmt7621&id=dlink_dir-882-a1 and add `luci`, `luci-ssl` and any of your packages, in my case are `luci`, `luci-ssl`, `curl`, `adblock`, `luci-app-adblock`, `htop`, click **Request Build**, and download the **sysupgrade** OpenWrt firmware and then enter System > Backup/Flash Firmware to upload the bin file, it is recommended to backup and export the configuration.\n\n## Benchmark\n\n### Wireless Speed Test\n\n#### TX: Device - Wireless - Router\n\nInstall [`iperf3`](https://openwrt.org/packages/pkgdata/iperf3) on OpenWrt and after than run `iperf3 -s`. Then from a client device connected to the network through wireless execute the command `iperf3 -c <IP Address of Your OpenWrt Router> -p <iperf3 Server Port> -f m -t 30 -w 256k -P 4`.\n\n#### TX: Device - Wireless - Router - Ethernet - Device\n\nInstall `iperf3` on another device connected to the network through Ethernet interface and then repeat the same testing again this device. The reason we need to repeat the testing with this configuration is because [your router might not be able to process the packets as consumer efficiently but do switch packet quickly](https://forum.openwrt.org/t/iperf-measure-network-speed/9203/2).\n\n#### RX\n\nRepeat the experiments but reverse the relationship of server and client. You should now run `iperf3 -s` in your device to verify the download speed.\n\n## Map with a Free Domain\n\n### Verify NAT\n\nFirst you need to confirm with your internet provider that your router doesn't live behind a [NAT](https://en.wikipedia.org/wiki/Network_address_translation) or [CGNAT](https://en.wikipedia.org/wiki/Carrier-grade_NAT). You can also manually verify this by checking your IP address when you search in Google and the IP address from OpenWrt GUI -> Network -> Interfaces -> WAN -> IPv4.\n\n### Create Domain\n\nYou can request a free domain with [Duck DNS](https://www.duckdns.org/).\n\n### [Keep the IP Address Up-to-Date](https://www.duckdns.org/install.jsp)\n\n## Expose Internal Service to Internet\n\nFrom OpenWrt Luci GUI -> Network -> DHCP and DNS -> Static Leases, set your internal host with a static IP address so port forwarding can work.\n\nGo to OpenWrt Luci GUI -> Network -> Firewall -> Port Forwards, then add rules to forward traffic from Internet to your internal host which lives inside of private network e.g. port forwarding to SSH server.\n\n## Reference\n\n* [Image formats](https://openwrt.org/docs/techref/image.format)\n* [D-Link Recovery GUI](https://openwrt.org/docs/guide-user/installation/recovery_methods/d-link_recovery_gui)\n* [Canales wifi en la banda 5 GHz en España: Lo que nunca te han contado](https://bandaancha.eu/articulos/canales-wifi-banda-5-ghz-espana-mejor-9826)\n* [Todos los canales y bandas de frecuencias wifi legales en España en 2,4; 5 y 6 GHz](https://bandaancha.eu/articulos/todas-canales-bandas-wifi-2-4-5-6ghz-10117)\n* [¿Es mejor un router wifi si tiene muchas antenas MIMO?](https://bandaancha.eu/articulos/mejor-router-wifi-tiene-muchas-antenas-9810)\n* [When to Use 20mhz vs 40mhz vs 80mhz](https://www.cbtnuggets.com/blog/certifications/cisco/when-to-use-20mhz-vs-40mhz-vs-80mhz)\n* [Wi-Fi Ping Spikes: Causes and Fixes](https://www.smallnetbuilder.com/wireless/wireless-features/33228-wi-fi-ping-spikes-causes-and-fixes)\n* [Different Wi-Fi Protocols and Data Rates](https://www.intel.com/content/www/us/en/support/articles/000005725/wireless/legacy-intel-wireless-products.html)\n* [Edit /etc/hosts via WebUI](https://forum.archive.openwrt.org/viewtopic.php?id=54943)",
        "slug": "D-LINK-DIR-882-A1-OpenWrt",
        "date": "2022-10-30 17:03:37",
        "lang": "en",
        "tags": ["openwrt", "network", "router", "hard router"],
        "path": "/D-LINK-DIR-882-A1-OpenWrt/index.html"
      },
      {
        "title": "Remote Access Via TightVNC To Windows Machine",
        "markdownContentSource": "\n## Install VNC Server on Windows Machine\n\nDownload [TightVNC](https://www.tightvnc.com/download.php) which is free VNC server that support Windows 11 and then install TightVNC on Windows machine.\n\n## TightVNC Configuration on Windows Machine\n\n1. Click on **Register TightVNC Service**\n2. Click on **TightVNC Service - Offline Configuration**\n    * From tab **Server**\n        * Check **Require VNC Authentication** and set password\n        * Uncheck **Enable file transfers**\n        * Uncheck **Hide desktop wallpaper**\n        * Uncheck **Serve Java Viewer to Web clients**\n    * From tab **Access Control**\n        * Check **Allow loopback connections**\n        * Check **Allow only loopback connections**\n    * From tab **Administration**\n        * Check **Protect control operations with a password**\n    * Save the confguration\n3. Click on **Start TightVNC Service**\n\n## Configure OpenSSH Server\n\nInstall OpenSSH server following these [instructions](https://github.com/PowerShell/Win32-OpenSSH/wiki/Install-Win32-OpenSSH) and configure sshd with these [instructions](https://github.com/PowerShell/Win32-OpenSSH/wiki/sshd_config) e.g. set port to **2222**.\n\n## [Key Based Authentication](https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_keymanagement)\n\nExecute following command in remote machine and copy the content of `id_ed25519.pub` to the `~/.ssh/authorized_keys` of Windows machine. If you face permission issue try this [workaround](https://stackoverflow.com/a/50502015).\n\n    ssh-keygen -t ed25519\n\n## Port Forward for SSH Server\n\nFollow [these instructions](/D-LINK-DIR-882-A1-OpenWrt/#Expose-Internal-Service-to-Internet) to expose port **2222** to the Internet.\n\n## Connect from Remote Machine\n\nDownload [TightVNC Java Viewer](https://www.tightvnc.com/download.php), unzip and open jar. Execute SSH [port forwarding to TightVNC](https://www.tightvnc.com/faq.php#portfwd) `ssh -N -L 5900:localhost:5900 <your Windows user name>@<your public domain name> -p <SSH port> -vv`. Introduce `localhost` on **Remote Host** text box, `5900` on **Port** text box of **TightVNC Java Viewer** and click Connect, introduce access password, got it!\n\n## Reference\n\n* [How secure is TightVNC?](https://www.tightvnc.com/faq.php#howsecure)\n* [How would I connect from the Internet to a machine in the internal network which is behind a router?](https://www.tightvnc.com/faq.php#portfwd)",
        "slug": "Remote-Access-Via-TightVNC-To-Windows-Machine",
        "date": "2022-08-13 11:35:55",
        "lang": "en",
        "tags": ["VNC", "TightVNC"],
        "path": "/Remote-Access-Via-TightVNC-To-Windows-Machine/index.html"
      },
      {
        "title": "Raspberry Pi 4B Setup",
        "markdownContentSource": "\n## Buy a Raspberry Pi 4B\n\nYou can buy the Raspberry Pi 4B board on Amazon, the recommended version is 4 GB. You may also need to buy a SD card, and optional case with fan.\n\n### Choose SD Card\n\nRaspberry Pi has no internal storage, relying instead on a removable micro SD card as his primary boot drives. [Raspberry Pi 3 doesn't support UHS mode because it requires 1.8v signaling while Raspberry Pi 3 is fixed at 3.3v](https://www.reddit.com/r/raspberry_pi/comments/4aoc3r/how_to_overclock_the_microsd_card_reader_on_a/d125ced/). This is technically the upper bound of write speed (25MB/sec) for [non-UHS SD cards](https://www.sdcard.org/developers/sd-standard-overview/speed-class/). Now Raspberry Pi 4B provides a [SD card socket runs on DDR50 bus speed mode](https://datasheets.raspberrypi.org/rpi4/raspberry-pi-4-datasheet.pdf) and a legacy [SDIO](https://www.sdcard.org/developers/sd-standard-overview/sdio-isdio/) interface is available on the GPIO pins.\n\n[![Raspberry Pi 4B](raspberry-pi-4B-block-diagram.png)](https://www.raspberrypi.org/forums/viewtopic.php?t=271632)\n\nThere is a micro-SD performance improvements with [Raspberry Pi 4](https://www.jeffgeerling.com/blog/2019/raspberry-pi-microsd-card-performance-comparison-2019). Raspberry Pi 4 still doesn't provide good support for [A2](https://www.sdcard.org/developers/sd-standard-overview/application-performance-class/) SD card, because A2 cards rely on specialized hardware in the host device to work well. The [benchmark](https://www.pidramble.com/wiki/benchmarks/microsd-cards#4-model-b) showed that A1 is the best card to use with Rapsberry Pi 4B and got better result than A2.\n\n### [Overclock SD Card](https://www.jeffgeerling.com/blog/2016/how-overclock-microsd-card-reader-on-raspberry-pi-3)\n\nDDR50 requires 1.8V signaling, supports frequency up to 50 MHz, sampled on both clock edges and bus speed up to 50 MB/sec per [Physical Layer Simplified Specification](https://www.sdcard.org/downloads/pls/). However, [UHS-I](https://www.sdcard.org/developers/sd-standard-overview/bus-speed-default-speed-high-speed-uhs-sd-express/) SD card [supports a clock frequency of 50 MHz (DDR50), 100 MHz (SDR50) and 208MHz (SDR104)](https://en.wikipedia.org/wiki/SD_card#UHS-I) per [Physical Layer Simplified Specification](https://www.sdcard.org/downloads/pls/). So in theory we can overclock SD card frequency to a higher value, e.g. 100 MHz. However, overclocking [only works for Raspberry Pi 3](https://www.raspberrypi.org/forums/viewtopic.php?t=266682#p1620747).\n\n### [SD Card Speed Test](https://www.raspberrypi.org/blog/sd-card-speed-test/)\n\nInstall the tool to test your SD card speed\n\n    sudo apt update\n    sudo apt install agnostics\n\nmy SanDisk Ultra A1 128 GB SD card has the following result\n\n    Sequential write speed 25460 KB/sec\n    Random write speed 1077 IOPS\n    Random read speed 2642 IOPS\n\n## Download Raspberry Pi OS image\n\nDownload **Raspberry Pi OS Lite** or **Raspberry Pi OS with desktop** from [Raspberry Pi official operating system images](https://www.raspberrypi.com/software/operating-systems/#raspberry-pi-os-64-bit).\n\n## Burn the image to SD card\n\nDownload [Rufus](https://rufus.ie/) and unzip the image file you downloaded previously, you will see a single **.img** file. Select SD card as device and select **.img** file as **Disk or ISO image** and then click **Start**. You may need **SD Card Reader USB adapter** if your computer doesn't have SD card slot.\n\n## CLI Enviroment Configuration\n\n### Boot Raspberry Pi OS Lite\n\nPlug the SD card to your Raspberry Pi and power on the device, follow the instructions to complete the basic setting. You will see the TTY when the boot process finish.\n\n### Switch to TTY\n\nYou can press Ctrl + Alt + F1/F2/F3/F4/F5/F6/F8/F10/F11/F12 to switch to different TTY console and press Ctrl + Alt + F7 you can switch to desktop environment if you have it installed.\n\n### [Set Language](https://www.raspberrypi.com/documentation/computers/configuration.html#localising-your-raspberry-pi)\n\nExecute `sudo raspi-config` and then select Localization Options > Locale. This option will execute following [commands](https://github.com/RPi-Distro/raspi-config/blob/2be7313596b228c349c829ca7258cbdbd4b5deb1/raspi-config#L532).\n\n### [Set Keyboard Layout](https://wiki.debian.org/Keyboard)\n\nExecute `sudo raspi-config` and then select Localization Options > Keyboard. This option will execute following [commands](https://github.com/RPi-Distro/raspi-config/blob/2be7313596b228c349c829ca7258cbdbd4b5deb1/raspi-config#L518).\n\nFor standard US layout you can select `Generic 105-key PC (intl.)` and then select `English US` and `default layout`. We need to `sudo reboot` to take effect.\n\n### [Rotate the TTY Screen](https://github.com/pimoroni/hyperpixel4/issues/39#issuecomment-717248457)\n\nIn order to rotate the screen in TTY mode you can use following command\n\n    echo <rotation> | sudo tee -a /sys/class/graphics/fbcon/rotate > /dev/null\n\nTo always rotate the screen at startup you can add following parameter to `/boot/cmdline.txt`\n\n    fbcon=rotate:<rotation>\n\nThe `<rotation>` can be 0, 1, 2, 3 that correspond to 0-degree, 90-degree, 180-degree, 270-degree clockwise rotation.\n\n### [Connect to Wi-Fi](https://www.raspberrypi.com/documentation/computers/configuration.html#wireless-networking-command-line)\n\n#### Configure Wi-Fi country code\n\nIn the Raspberry Pi OS, [5 GHz wireless networking is disabled until a Wi-Fi country code has been configured by the user](https://www.raspberrypi.org/documentation/computers/configuration.html#ensure-wireless-operation).\n\nExecute `sudo raspi-config` and then select Localization Options > WLAN Country. This option will execute following [commands](https://github.com/RPi-Distro/raspi-config/blob/2be7313596b228c349c829ca7258cbdbd4b5deb1/raspi-config#L615-L630).\n\n#### [Find SSID](https://www.raspberrypi.com/documentation/computers/configuration.html#getting-wireless-lan-network-details)\n\nIf you don't know the SSID scan the networks in the area to find the SSID you want to connect to\n\n    sudo iwlist wlan0 scan | greo -i ssid\n\n#### Enter Wi-Fi Credentials Interactively\n\nExecute `sudo raspi-config` and then select System Options > Wireless LAN. This option will execute following [commands](https://github.com/RPi-Distro/raspi-config/blob/2be7313596b228c349c829ca7258cbdbd4b5deb1/raspi-config#L2308).\n\n#### [Enter Wi-Fi Credentials Manually](https://www.raspberrypi.com/documentation/computers/configuration.html#adding-the-network-details-to-your-raspberry-pi)\n\nCreate configuration for Wi-Fi\n\n    wpa_passphrase <ssid> <password> | sudo tee -a /etc/wpa_supplicant/wpa_supplicant.conf\n\nIf you are connecting to a hidden network, an extra option `scan_ssid` in the `wpa_supplicant.conf` file `network` section need to be present. You should use tab instead of space when indenting. By default, `wpa_passphrase` also include the plain text version of your password, but commented out. You should delete this line for extra security. The final network section should look like\n\n    network={\n        ssid=\"<your hidden SSID>\"\n        scan_ssid=1\n        psk=\"<your encrypted 32 byte hexadecimal PSK>\"\n    }\n\nand ensure the following lines are at the top of `wpa_supplicant` file, you need to double check the country is correct.\n\n    ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\n    update_config=1\n    country=<2 letter ISO 3166-1 country code>\n\nthen reconfigure the interface with\n\n    wpa_supplicant -c /etc/wpa_supplicant/wpa_supplicant.conf -i wlan0 -d\n\n#### Verify the network\n\nYou can verify the connection by running following command\n\n    ping 8.8.8.8\n\nIf you find the network is unavailable, try to wait 2 minutes or reboot, them verify with `ping` again.\n\n## [Configure Static IP Address](https://www.raspberrypi.com/documentation/computers/configuration.html#static-ip-addresses)\n\nIf you want a static IP address, so each time you restart your Raspberry Pi device or disconnect and reconnect to the network you can find your device with same IP address. You need to add the following lines to `/etc/dhcpcd.conf`.\n\n    interface <your-interface-name>\n    static ip_address=192.168.0.10/24\n    static routers=192.168.0.1\n    static domain_name_servers=192.168.0.1 8.8.8.8\n\nYou can find your interface name with `ip link`, router IP address with `ip route show default`. You need to ensure the static IP address is outside of DHCP dynamically assigned IP address range, you can verify this information in your router. You can verify your IP address with `ip -a address`.\n\n## [Setting up SSH Server](https://www.raspberrypi.com/documentation/computers/remote-access.html#ssh)\n\n### [Enable SSH Server](https://www.raspberrypi.com/documentation/computers/remote-access.html#enabling-the-server)\n\nEnter `sudo raspi-config` in a terminal window. Select `Interfacing Options` and then `SSH`. This option will execute following [commands](https://github.com/RPi-Distro/raspi-config/blob/2be7313596b228c349c829ca7258cbdbd4b5deb1/raspi-config#L841)\n\n### [Configure Passwordless SSH Access](https://www.raspberrypi.com/documentation/computers/remote-access.html#passwordless-ssh-access)\n\nGenerate new SSH keys using the `ssh-keygen` command on the client. Copy your public key to your Raspberry Pi. `cat ~/.ssh/id_rsa.pub | ssh <USERNAME>@<IP-ADDRESS> 'mkdir -p ~/.ssh && cat >> ~/.ssh/authorized_keys'`.\n\n### [Disable Password Authentication](https://www.raspberrypi.com/documentation/computers/configuration.html#using-key-based-authentication)\n\nEdit `/etc/ssh/sshd_config` and change these options to `no`\n\n    ChallengeResponseAuthentication no\n    PasswordAuthentication no\n    UsePAM no\n\n### Change SSH Server Port\n\nBy default, SSH server listen to `22`. Edit `/etc/ssh/sshd_config` and change `Port` to other value e.g. `2222`\n\n### Disable IPv6 SSH Access\n\nEdit `/etc/ssh/sshd_config` and change `AddressFamily` to `inet` which means IPv4 only.\n\n### Listen To Local Network Only\n\nEdit `/etc/ssh/sshd_config` and change `ListenAddress` to private network static address that you have configured before. When you specify IP address explicitly in sshd config file, [network interfaces must be prepared prior to sshd start](https://unix.stackexchange.com/a/499712). Execute `sudo systemctl edit sshd` and add following content.\n\n    [Unit]\n    After=network-pre.target network.target network-online.target auditd.service dhcpcd.service\n\n    [Service]\n    Restart=on-failure\n    RestartSec=5\n    RestartPreventExitStatus=\n\nThen execute `sudo systemctl enable systemd-networkd-wait-online`.\n\n### Apply Latest SSH Configuration\n\nRestart the ssh system with `sudo service ssh reload`.\n\n## [Firewall Configuration](https://www.raspberrypi.com/documentation/computers/configuration.html#install-a-firewall)\n\n    sudo apt install ufw\n    sudo ufw default deny incoming\n    sudo ufw default allow outgoing\n    sudo ufw allow in on eth0 from 192.168.0.0/24 to 192.168.0.10 port 2222 proto tcp\n    sudo ufw enable\n    sudo ufw status numbered verbose\n\n## Disable Wi-Fi & Bluetooth\n\nAdd these 2 lines to `/boot/config.txt`\n\n    dtoverlay=disable-wifi\n    dtoverlay=disable-bt\n\nHere you can find the documentations of [`disable-wifi`](https://github.com/raspberrypi/firmware/blob/afc7631bc9a58f94a157503955881ab27a6287d0/boot/overlays/README#L736-L739) and [`disable-bt`](https://github.com/raspberrypi/firmware/blob/afc7631bc9a58f94a157503955881ab27a6287d0/boot/overlays/README#L727-L733).\n\n## Configure Fan\n\nWhen you buy a Raspberry Pi 4B you can also optionally buy a case with fan e.g. the official [Raspberry Pi 4 Case Fan](https://www.raspberrypi.org/products/raspberry-pi-4-case-fan/). You can simply connect the fan wires to 5V and ground pin but Raspberry Pi support advance fan control base on temperature. Run `sudo raspi-config`, select `Performance Options` then select `Fan`, introduce the GPIO that will be enabled when temperature exceeds the limit you introduced. You need to reboot after the configuration change. Run the command [`pinout`](https://www.raspberrypi.org/documentation/computers/os.html#gpio-pinout) to find the location of GPIO and connect it to base gate of a transistor, connect negative wire of fan to ground. If you are using NPN transistor then connect emitter gate to positive wire of fan and 5V to collector gate of transistor. If you are using PNP transistor then connect collector gate to positive wire of fan and 5V to emitter gate of transistor. You can use protoboard to arrange the wires. You can verify current temperature using command `vcgencmd measure_temp`.\n\n## [NAS with Samba and External Disk through USB 3.0](https://www.raspberrypi.com/documentation/computers/remote-access.html#samba-smbcifs)\n\n### Mount External Disk\n\nAttach the external disk to the USB 3.0 port and run following to ensure the device is recognized\n\n    sudo blkid\n    sudo fdisk -l\n\nIf your disk is `NTFS` file system then we need to install following package\n\n    sudo apt install ntfs-3g\n\nOnce you know where the drive is located, run following command, the type argument might be different depending on your drive file system\n\n    sudo mkdir -p /mnt/shared\n    sudo mount -t ntfs-3g /dev/<what you have found in previous step e.g. sda1> /mnt/shared\n\nTo permanently mount at startup, add the following line to `/etc/fstab`\n\n    /dev/<what you have found in previous step e.g. sda1>    /mnt/shared   ntfs    default  0  0\n\n### Setup Shared Folder on Raspberry\n\nInstalling Samba Support with following commands\n\n    sudo apt update\n    sudo apt install samba samba-common-bin cifs-utils\n\nSharing a folder is easy\n\n    sudo useradd -M -s /sbin/nologin samba\n    sudo passwd samba\n    sudo chmod 700 /mnt/shared\n    sudo chown samba /mnt/shared\n    sudo smbpasswd -a samba\n\nEdit the Samba configuration file `/etc/samba/smb.conf` and add the following lines\n\n    [shared]\n        path = /mnt/shared\n        read only = no\n        guest ok = no\n        security = user\n        valid users = samba\n        writable = yes\n        vfs objects = recycle\n        recycle:repository = .recycle/%U\n        recycle:touch = yes\n        recycle:keeptree = yes\n        recycle:versions = yes\n\nIn the same file change these two properties to\n\n    interfaces = 192.168.0.10/24 eth0\n    workgroup = <your workgroup name here - you find it in Windows client machine System Properties - Computer Name>\n\nApply the change\n\n    sudo service smbd reload\n\nAllow the Samba traffic through firewall\n\n    sudo ufw allow in on eth0 from 192.168.0.0/24 to 192.168.0.10 app Samba\n\n### Mount Samba Shared Folder\n\n    net use Z: \\\\192.168.0.10\\shared /SAVECRED /PERSISTENT:YES\n\nIt will ask for username and password, you should introduce `samba` as username.\n\n### Remove Samba Shared Folder\n\n    net use  Z: /delete\n\n## [NAS Backup with Dropbox and Rclone](https://rclone.org/dropbox/)\n\n### [Get your own Dropbox App ID](https://rclone.org/dropbox/#get-your-own-dropbox-app-id)\n\n### Install Rclone\n\n    sudo apt update\n    sudo apt install rclone\n\n### Configure Rclone\n\n    rclone config\n\nFollow the instructions until you need to visit an URL, you will need to use SSH local forward\n\n    sudo ufw default allow FORWARD\n    sudo service ufw restart\n    ssh -p 2222 -N -L 127.0.0.1:53682:127.0.0.1:53682 sgao@192.168.0.10\n\nOpen the link that is mentioned in following [workaround](https://github.com/rclone/rclone/issues/4792#issuecomment-731758924) to avoid expiration issue. You can verify that refresh token is present\n\n    cat ~/.config/rclone/rclone.conf\n\nRollback the `ufw` configuration\n\n    sudo ufw default deny FORWARD\n    sudo service ufw restart\n\n### [Backup with Dropbox](https://rclone.org/commands/rclone_sync/)\n\nWe first verify the setup is correct\n\n    rclone lsd dropbox:\n\nThen sync from Dropbox to a local folder\n\n    mkdir -p /mnt/shared/Dropbox\n    rclone copy dropbox: /mnt/shared/Dropbox\n\nFinally, keep the folder synchronized with Dropbox\n\n    rclone sync /mnt/shared/Dropbox dropbox:\n\n### Keep Dropbox Synced With Local\n\nInstall the `inotify-tools`\n\n    sudo apt update\n    sudo apt install inotify-tools\n\nSave following commands to `~/dropbox-sync-from-local-to-remote.sh`\n\n    #!/bin/bash\n    # Execute first time after the machine is boot\n    rclone sync /mnt/shared/Dropbox dropbox:\n    inotifywait -m -q -r -e create -e modify -e move -e delete --format '%w%f:%e' /mnt/shared/Dropbox | \\\n    while read; do \\\n        path=$(echo $REPLY | cut -d \":\" -f 1); \\\n        event=$(echo $REPLY | cut -d \":\" -f 2); \\\n        echo \"$path was changed because of event $event\"; \\\n        echo \"Skipping $(timeout 3 cat | wc -l) further changes\"; \\\n        rclone sync --delete-during /mnt/shared/Dropbox dropbox:; \\\n    done\n\nWe need to add execute permission to the script file\n\n    chmod +x ~/dropbox-sync-from-local-to-remote.sh\n\nCreate a `Cron` job at boot time by executing `crontab -e` and then add following line to the task list\n\n    @reboot sleep 10 && ~/dropbox-sync-from-local-to-remote.sh >> ~/dropbox-sync-from-local-to-remote.log 2>&1\n\n### Keep Local Synced With Dropbox\n\nSave following commands to `~/dropbox-sync-from-remote-to-local.sh`\n\n    rclone sync dropbox: /mnt/shared/Dropbox --backup-dir /mnt/shared/Dropbox-Backup --suffix .$(date +\"%Y-%m-%d-%H-%M-%S\")\n\nCreate a `Cron` job by executing `crontab -e` and then add following line to the task list\n\n    */5 * * * * ~/dropbox-sync-from-remote-to-local.sh >> ~/dropbox-sync-from-remote-to-local.log 2>&1\n\n## [Upgrade Raspberry Pi OS](https://www.raspberrypi.org/documentation/computers/os.html#upgrading-from-operating-system-versions)\n\nModify the file `/etc/apt/sources.list` and `/etc/apt/sources.list.d/raspi.list` and replace every occurrence of the word `buster` to `bullseye`.\n\n    sudo apt update\n    sudo apt -y dist-upgrade\n    sudo reboot\n\n### Remove Unused Software After Upgrade\n\n    sudo apt purge geany thonny\n    sudo apt autoremove\n\n## Desktop Environment Configuration\n\nRaspberry Pi OS provides desktop environment, and we can also upgrade from lite version to desktop version, but it is recommended that if you need to use desktop environment, download the desktop version directly.\n\n### [Install Desktop Environment from Lite Version](https://gist.github.com/kmpm/8e535a12a45a32f6d36cf26c7c6cef51)\n\n    sudo apt install -y lightdm raspberrypi-ui-mods rpi-chromium-mods\n    sudo apt install -y arandr # https://www.raspberrypi.org/forums/viewtopic.php?t=265472#p1613632\n    sudo apt install --no-install-recommends xinit\n\nAfter the installation you can enter the desktop environment using `startx` from TTY console.\n\n### Boot Raspberry Pi OS with desktop\n\nIf you boot with a desktop image, you will see a GUI windows that guide you through the initial setup, e.g. configuring the country, language, keyboard, Wi-Fi.\n\n### [Configure Display Setting](https://www.raspberrypi.org/documentation/computers/configuration.html#hdmi-configuration)\n\nEdit `/boot/config.txt` and ensure following parameters has the following values:\n\n    hdmi_drive=2 # https://www.raspberrypi.org/documentation/computers/configuration.html#troubleshooting-your-hdmi-2\n    hdmi_group=2 # https://www.raspberrypi.org/documentation/computers/config_txt.html#hdmi_group\n    hdmi_mode=82 # https://www.raspberrypi.org/documentation/computers/config_txt.html#hdmi_mode\n\n### [Rotate your Display in GUI](https://www.raspberrypi.org/documentation/computers/configuration.html#rotating-your-display)\n\nYou can check you available display with:\n\n    xrandr -q\n\nThen use the name of display you get previously to rotate the screen\n\n    xrandr --output [name of display e.g. HDMI-1] --rotate left\n\n## Reference\n\n* [raspi-config](https://github.com/RPi-Distro/raspi-config/blob/master/raspi-config)\n* [Samba Wiki: Setting up Samba as a Standalone Server](https://wiki.samba.org/index.php/Setting_up_Samba_as_a_Standalone_Server)\n* [How can I have a Trash/Recycle Bin for my Samba shares?](https://unix.stackexchange.com/a/112066)\n* [Command-Line Reference: Net use](https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/gg651155(v=ws.11))\n* [Enabling Recycle Bin like functionality in Samba](https://lonewolfonline.net/enabling-recycle-bin-functionality-samba/)\n* [Samba VFS recycle bin](https://www.samba.org/samba/docs/current/man-html/vfs_recycle.8.html)\n* [Rclone: Simple two-way dropbox sync](https://www.ralphminderhoud.com/blog/rclone-two-way-dropbox-sync/)\n",
        "slug": "RaspberryPi4B-Setup",
        "date": "2022-07-10 13:00:17",
        "lang": "en",
        "tags": [
          "Linux",
          "Debian",
          "Raspberry",
          "Raspberry Pi 4",
          "Prestashop"
        ],
        "path": "/RaspberryPi4B-Setup/index.html"
      },
      {
        "title": "How to Create a Windows7 Vagrant Base Box",
        "markdownContentSource": "\n# How to Create a Windows 7 Vagrant Base Box?\n\n## Introduction\n\nSince Vagrant 1.6 release, Vagrant add support for Windows VMs. We will go through the steps to follow to create a Windows 7 Vagrant base box.\n\n## Pre-requisites\n\n* VirtualBox/VMWare\n* Vagrant 1.6 or above\n* Windows 7 ISO file\n* VirtualBox Guest Additions\n* A valid license key for the version of Windows 7 to be installed\n\n## Create New Virtual Machine\n\nStart VirtualBox and create a new Windows 7 virtual machine. Name it whatever you like, though keep it simple as we will override this name later in Vagrantfile. We will follow most of the advice given in the Vagrant documentation regarding base boxes.\n\n* Disk Space: Choose a virtual hard disk (VDI) dynamically allocated and choose a high upper limit for the size. The default max-size is pretty low (32 GB).\n* Memory: Choose a low value here e.g. 1 GB, the reason is users that use the base box can always increase the memory in their Vagrantfile. I chose 2048 MB, but 1024 MB would do as well.\n* CPU: Leave this at the default of 1. Again this is something users can override in the Vagrantfile, so try not to require a higher number to start with.\n* Disable any unnecessary hardware like audio. Leave 2D/3D acceleration disabled.\n\nThe idea behind these choices is to create your base box as light on resources as possible with as much room as possible for later customization. When creating a new machine in VirtualBox in the wizard a lot of options are auto-configured, so we may need to edit the machine after it is created, going into Settings, to change some options.\n\n* Settings > General > Advanced: Enable Host-to-Guest shared clipboard.\n* Settings > General > System > Motherboard: Disable Floppy boot.\n* Settings > General > Audio: Disable audio.\n\nYou can follow the New Virtual Machine Wizard with Custom (advanced) configuration to create the Windows 7 virtual machine. Select similar options as VirtualBox, although some options might be different.\n\n* Select the version of Windows to install\n* Full name: vagrant\n* Password: vagrant\n* Firmware type: BIOS\n* Network connection: NAT\n* Disk: Store virtual disk as a single file\n\nUsers of your base box can always modify these options in their Vagrantfile, using the [VirtualBox specific configuration](https://www.vagrantup.com/docs/providers/virtualbox/configuration) or [VMWare specific configuration](https://www.vagrantup.com/docs/providers/vmware/configuration).\n\n## Install Windows 7\n\nYou should have your Windows 10 ISO ready at hand, then configure the ISO file to boot from in VirtualBox, skip prompts to enter a license key and this can always be entered later through Control Panel or command line, after this, installation is straight forward.\n\nTowards the end of the installation you will be prompted to sign in your Microsoft account, again skip this. You want to create a local admin account on your clean install named vagrant with password also vagrant. This is required if you want Vagrant to automatically connect to and provision machines on your base box.\n\nLet the Windows setup complete, you will be presented with a login prompt for your user vagrant. Go ahead and login to the Windows 10 desktop. When connecting to the network windows will ask you if the network you are connected to is public or private. Choose private, otherwise you will get issues with **WinRM** in the next step.\n\n## Prepare Windows For Vagrant\n\n* Install VirtualBox Guest Additions/VMware Tools on your base box from the menu.\n* Control Panel > Programs and Features > Turn Windows features on or off.\n* Disable UAC: Control Panel > User Accounts > Change user account control settings, drop the slider to the bottom - Never notify.\n* Turn off System Protection： Control Panel > System > Advanced System Settings > System Protection > Turn off protection.\n* Enable **WinRM** service：Open command prompt as Administrator and paste each line and press enter. The last line configures **WinRM** services to start automatically. This will allow Vagrant to connect to the box and control it. **WinRM** is the alternative to ssh for windows boxes.\n----\n\n    winrm quickconfig -q\n    winrm set winrm/config/winrs @{MaxMemoryPerShellMB=\"512\"}\n    winrm set winrm/config @{MaxTimeoutms=\"1800000\"}\n    winrm set winrm/config/service @{AllowUnencrypted=\"true\"}\n    winrm set winrm/config/service/auth @{Basic=\"true\"}\n    sc config WinRM start= auto\n\n* Relax the Powershell execution policy: Run Powershell as Administrator and execute this command: `Set-ExecutionPolicy -ExecutionPolicy Unrestricted`, choose Yes to any prompts. This will allow Powershell scripts to provision your base box without annoying restrictions.\n* Enable remote connection to your box: Control Panel > System > Advanced System Settings > Remote > Allow remote connections to this computer.\n* Turn off Windows Update: Control Panel > System > Turn automatic updating on or off > Never check for updates\n* The Advanced tab > Performance Settings, tweak visual effects to performance, then further on the Advanced tab limit the maximum virtual memory paging file size to 1024 MB. Restart if prompted.\n* Finally, use disk clean to reduce the box size\n\n## Exporting Your VirtualBox Virtual Machine as Base Box\n\n    vagrant package --base <Name of the VM in the Virtualbox GUI> --output /path/to/output/Windows7.box\n\n## Exporting Your VMWare Virtual Machine as Base Box\n\nVagrant also support to create boxes for VMware provider. You will need to first install the Vagrant VMware support following this [guide](https://www.vagrantup.com/docs/providers/vmware/installation). Create a `metadata.json` file inside **vmwarevm** folder with following content.\n\n    {\n        \"provider\": \"vmware_desktop\"\n    }\n\nRemove any extraneous files from the **vmwarevm** folder and package it. Prior to packaging up a box, we should shrink the hard drives as much as possible. This can be done with\n\n    vmware-vdiskmanager -d /path/to/vmwarevm/main.vmdk\n    vmware-vdiskmanager -k /path/to/vmwarevm/main.vmdk\n    cd /path/to/vmwarevm\n    tar cvzf /path/to/output/Windows7.box ./*\n\n## Test Your Base Box Locally\n\n    vagrant box add /path/to/output/Windows7.box --name Windows7\n    vagrant init Windows7\n    vagrant up --provider=vmware_desktop\n\n## Publish Your Base Box\n\nOpen https://app.vagrantup.com/boxes/new and follow the instruction to create a Box. Once the Box is created, select `Add a provider`, and then choose corresponding provider and upload the box file. You can also use [my box distribution](https://app.vagrantup.com/vitamina/boxes/Windows7).\n\n## Use Your Base Box\n\nYou can use `vagrant up` command with the `--provider` flag to specify the provider you want to use and then specify the base box distribution with `config.vm.box = \"vitamina/Windows7\"`.\n\n## Reference\n\n* [Creating a Windows 10 Base Box for Vagrant with VirtualBox](https://huestones.co.uk/2015/08/creating-a-windows-10-base-box-for-vagrant-with-virtualbox/)\n* [Vagrant: Creating a Base Box](https://www.vagrantup.com/docs/providers/virtualbox/boxes)\n* [Vagrant: CLI package command](https://www.vagrantup.com/docs/cli/package)\n* [Vagrant: Creating a New Vagrant Box](https://www.vagrantup.com/vagrant-cloud/boxes/create)\n* [Vagrant Windows Machine](https://github.com/fabiohbarbosa/vagrant-windows7)\n* [In search of a lightweight windows vagrant box](http://www.hurryupandwait.io/blog/in-search-of-a-light-weight-windows-vagrant-box)\n* [Vagrant Box for Windows 10](https://github.com/jeffskinnerbox/Windows-10-Vagrant-Box)\n* [Vagrant Windows 7 Machine](https://github.com/fabiohbarbosa/vagrant-windows7)\n* [Vagrant: Basic Provider Usage](https://www.vagrantup.com/docs/providers/basic_usage#default-provider)\n* [Vagrant: VMware Provider](https://www.vagrantup.com/docs/providers/vmware)\n* [Vagrant: Default Provider](https://www.vagrantup.com/docs/providers/default)\n* [Vagrant: Box File Format](https://www.vagrantup.com/docs/boxes/format)\n* [CREATE A WINDOWS 10 VAGRANT BOX](https://mesd.ag/windows-10-vagrant-box/)\n* [Create Windows 10 Vagrant Base Box](https://softwaretester.info/create-windows-10-vagrant-base-box/)\n* [Reusable Windows VMs with Vagrant](https://dev.to/jeikabu/reusable-windows-vms-with-vagrant-2h5c)\n* [Creating a Windows Box with Vagrant 1.6](https://dennypc.wordpress.com/2014/06/09/creating-a-windows-box-with-vagrant-1-6/)\n",
        "slug": "Windows7-Vagrant",
        "date": "2022-03-26 11:55:38",
        "lang": "en",
        "tags": ["Vagrant", "winrm"],
        "path": "/Windows7-Vagrant/index.html"
      },
      {
        "title": "Diagonalization",
        "markdownContentSource": "\n## What is diagonalization?\n\nFor a $n \\times n$ matrix $A$ the diagonalization is the process to find a diagonal matrix $D$ and a invertible $n \\times n$ matrix $P$ that holds following equality $A = PDP^{-1}$\n\n## Why we need diagonalization?\n\nWe often need to calculate the power of the matrix, this operation $A^{k}$ is expensive to calculate when $A$ and $k$ is big enough. However, if the matrix $A$ is diagonalizable. The power operation is then cheaper because $A^{k} = (PDP^{-1})^k = PD^{k}P^{-1}$ and the powers of diagonal matrix is straight forward.\n\n$$\nD^{k} = \n\\begin{bmatrix}\n\\lambda_{1}^{k} & 0 & \\dots & 0\\\\\n0 & \\lambda_{2}^{k} & \\dots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & \\dots & \\lambda_{n}^{k}\\\\\n\\end{bmatrix}\n$$\n\n## The relationship of diagonalization with homogeneous system\n\nBecause $A = PDP^{-1} \\implies AP = PD$, if we expand the calculation\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\na_{11} & \\dots & a_{1n}\\\\\n\\vdots & \\ddots & \\vdots\\\\\na_{n1} & \\dots & a_{nn}\\\\\n\\end{bmatrix}\n*\n\\begin{bmatrix}\np_{11} & \\dots & p_{1n}\\\\\n\\vdots & \\ddots & \\vdots\\\\\np_{n1} & \\dots & p_{nn}\\\\\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\np_{11} & \\dots & p_{1n}\\\\\n\\vdots & \\ddots & \\vdots\\\\\np_{n1} & \\dots & p_{nn}\\\\\n\\end{bmatrix}\n*\n\\begin{bmatrix}\n\\lambda_{1} & \\dots & 0\\\\\n\\vdots & \\ddots & \\vdots\\\\\n0 & \\dots & \\lambda_{n}\\\\\n\\end{bmatrix}\\\\\n\\begin{bmatrix}\n(a_{11}p_{11}+...+a_{1n}p_{n1}) & \\dots & (a_{11}p_{1n}+...+a_{1n}p_{nn})\\\\\n\\vdots & \\ddots & \\vdots\\\\\n(a_{n1}p_{11}+...+a_{nn}p_{n1}) & \\dots & (a_{n1}p_{1n}+...+a_{nn}p_{nn})\\\\\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\np_{11}\\lambda_{1} & \\dots & p_{1n}\\lambda_{n}\\\\\n\\vdots & \\ddots & \\vdots\\\\\np_{n1}\\lambda{1} & \\dots & p_{nn}\\lambda{n}\\\\\n\\end{bmatrix}\n\\end{aligned}\n$$\n\nWe will have the following observation:\n\n$$\n\\begin{pmatrix}\na_{11}p_{1j}+...+a_{1n}p_{nj}\\\\\n\\vdots\\\\\na_{n1}p_{1j}+...+a_{nn}p_{nj}\\\\\n\\end{pmatrix}\n=\n\\begin{pmatrix}\np_{1j}\\lambda_{j}\\\\\n\\vdots\\\\\np_{nj}\\lambda_{j}\\\\\n\\end{pmatrix}\n\\implies\n(A-\\lambda_{j}I)\n\\cdot\n\\begin{pmatrix}\np_{1j}\\\\\n\\vdots\\\\\np_{nj}\\\\\n\\end{pmatrix}\n=\n\\vec{0}\n$$\n\nThe above observation is important because it is the same as solving a homogeneous system of equations \n\n$$\n(A-\\lambda_{j}I) \\cdot \\vec{p}_{j} = \\vec{0}\n$$\n\nwhich $\\vec{p}_{j}$ is\n\n$$\n\\begin{pmatrix}\np_{1j}\\\\\n\\vdots\\\\\np_{nj}\\\\\n\\end{pmatrix}\n$$\n\n$\\vec{p}_{j}$ is called **eigenvector**, $\\lambda{j}$ is the corresponding **eigenvalue**.\n\n## Geometric meaning of eigenvalues and eigenvectors\n\nFrom $(A-\\lambda I) \\cdot \\vec{p} = \\vec{0}$ we know that $A\\vec{p} = \\lambda\\vec{p}$, so from geometrical perspective, eigenvector $\\vec{p}$ only suffer stretch for a given transformation $A$ and $\\lambda$ its corresponding stretch ratio.\n\nIf we draw in a 2D space, the effect $v^{\\prime}-v$ of applying the transformation of a $2 \\times 2$ matrix $A$, where $v$ is the original vector and $v^{\\prime}$ the vector after the transformation.\n\n![Eigenvectors](Eigenvectors.png)\n\nWe notice the vectors with the same direction as eigenvectors (red arrows) haven't changed the direction after the transformation.\n\nIf we use eigenvectors as basis, then the transformation is easier because we only need to multiply each component per corresponding eigenvalue. $P$ is actually the linear transformation that change basis formed by eigenvectors to standard basis and $P^{-1}$ from standard basis to basis formed by eigenvectors.\n\n## When a matrix is diagonalizable\n\nA $n \\times n$ matrix $A$ is diagonalizable if and only if $A$ has $n$ linearly independent eigenvectors.\n\n## What matrices are not diagonalizable\n\n* A rotation matrix is not diagonalizable over real fields\n* Nonzero [nilpotent matrices](https://en.wikipedia.org/wiki/Nilpotent_matrix)\n\n## How to find all the eigenvalues and corresponding eigenvectors\n\nFrom the previous conclusion $(A-\\lambda_{j}I) \\cdot \\vec{p}_{j} = \\vec{0}$ and **invertible matrix theorem**, then a number $\\lambda$ is an eigenvalue of $A$ if and only if $f(\\lambda) = det(A-\\lambda I)= 0$, $f(\\lambda)$ is called the characteristic polynomial of $A$.\n\nThe process of finding all the eigenvalues and corresponding eigenvectors is following\n\n1. Find all the roots $\\lambda$ of the characteristic polynomial.\n2. For each $\\lambda$, compute corresponding eigenvectors by finding linearly independent non-trivial solutions of the homogeneous system of equations $(A-\\lambda I) \\cdot \\vec{p} = \\vec{0}$\n\n## Application of diagonalization\n\nOne of important application of diagonalization is [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis). In PCA we need to calculate eigenvectors and eigenvalues of covariance matrix. Since covariance matrix is [symmetric matrix hence it is always diagonalizable](https://en.wikipedia.org/wiki/Symmetric_matrix#Real_symmetric_matrices).\n\n## Reference\n\n* [Interactive Linear Algebra. 5.1 Eigenvalues and Eigenvectors](https://textbooks.math.gatech.edu/ila/eigenvectors.html)\n* [Interactive Linear Algebra. 5.2 The Characteristic Polynomial](https://textbooks.math.gatech.edu/ila/characteristic-polynomial.html)\n* [Interactive Linear Algebra. 5.4 Diagonalization](https://textbooks.math.gatech.edu/ila/diagonalization.html)\n* [如何理解矩阵特征值？ - 闫星光的回答 - 知乎](https://www.zhihu.com/question/21874816/answer/45434660)\n* [What kind of matrices are non-diagonalizable?](https://math.stackexchange.com/questions/472915/what-kind-of-matrices-are-non-diagonalizable)\n",
        "slug": "Diagonalization",
        "date": "2022-01-09 11:36:05",
        "lang": "en",
        "tags": ["Math", "Diagonalization", "Matrix", "Linear Algebra"],
        "path": "/Diagonalization/index.html"
      },
      {
        "title": "The Process Behind Compiling and Executing C Code",
        "markdownContentSource": "\n## Introduction\n\nCompiling and executing a C program is a multi-stage process. In this post I’ll walk through each stages of compiling and executing the following C program with filename `test.c`:\n\n    #include <stdio.h>\n\n    #define LOOP_TIMES 10\n\n    int main(int argc, char *argv[])\n    {\n        for (int i = 0; i < LOOP_TIMES; i++)\n        {\n            printf(\"Hello World #%i!\\n\", i);\n        }\n        return 0;\n    }\n\nOur testings are performs on Debian Bullseye AMD64, intermediate result may vary depending on the OS and hardware.\n\n## Preprocessing\n\nThe first stage of compilation is called preprocessing. In this stage, the C pre-processor is responsible for handling pre-processor directives (lines starting with a `#` character). These pre-processor directives form a simple macro language with its own syntax and semantics. This language is used to reduce repetition in source code, e.g. lines with `#include` are replaced by the contents of the referenced file (with different search rules for names in quotes versus those in angle brackets). Names introduced with `#define` are systematically replaced with their definitions throughout the program, `#if` and its relatives are processed to conditionally omit code, etc...\n\nTo get the result of the preprocessing stage, we can pass `-E` option to `gcc`\n\n    gcc -E -o test.i test.c\n\nThe output after preprocessing stage in my machine look like following\n\n    // ... omitted for brevity\n    # 873 \"/usr/include/stdio.h\" 3 4\n\n    # 2 \"test.c\" 2\n\n\n\n\n    # 5 \"test.c\"\n    int main(int argc, char *argv[])\n    {\n        for (int i = 0; i < 10; i++)\n        {\n            printf(\"Hello World #%i!\\n\", i);\n        }\n        return 0;\n    }\n\n## Compilation\n\nIn this stage, the actual compiler translates pre-processed source into assembly language. These form an intermediate human-readable language. The existence of this step allows for C code to contain inline assembly instructions and for different assemblers to be used. To get the result of the compilation stage, pass the `-S` option to `gcc`:\n\n    gcc -S -o test.s test.i\n\nThe output after compilation stage in my machine look like following\n\n        .file\t\"test.c\"\n        .text\n        .section\t.rodata\n    .LC0:\n        .string\t\"Hello World #%i!\\n\"\n        .text\n        .globl\tmain\n        .type\tmain, @function\n    main:\n    .LFB0:\n        .cfi_startproc\n        pushq\t%rbp\n        .cfi_def_cfa_offset 16\n        .cfi_offset 6, -16\n        movq\t%rsp, %rbp\n        .cfi_def_cfa_register 6\n        subq\t$32, %rsp\n        movl\t%edi, -20(%rbp)\n        movq\t%rsi, -32(%rbp)\n        movl\t$0, -4(%rbp)\n        jmp\t.L2\n    .L3:\n        movl\t-4(%rbp), %eax\n        movl\t%eax, %esi\n        leaq\t.LC0(%rip), %rdi\n        movl\t$0, %eax\n        call\tprintf@PLT\n        addl\t$1, -4(%rbp)\n    .L2:\n        cmpl\t$9, -4(%rbp)\n        jle\t.L3\n        movl\t$0, %eax\n        leave\n        .cfi_def_cfa 7, 8\n        ret\n        .cfi_endproc\n    .LFE0:\n        .size\tmain, .-main\n        .ident\t\"GCC: (Debian 10.2.1-6) 10.2.1 20210110\"\n        .section\t.note.GNU-stack,\"\",@progbits\n\n## Assembly\n\nDuring this stage, The assembler converts the assembly language source to an unlinked relocatable object file in ELF format. The output contains the actual instructions to be run by the target processor. However, an unlinked relocatable object file is not executable yet: it may require definitions from other files, including libraries. To get the result of the assembly stage, pass the `-c` option to `gcc`:\n\n    gcc -c -o test.o test.s\n\nor we can manually invoke `as`\n\n    as -o test.o test.s\n\nRunning the above command will produce an unlinked relocatable object file in ELF format named `test.o`. We can inspect the ELF sections with `readelf -a test.o | less` and to see the content of specific section we can use `readelf -x .text test.o`.\n\n## Linking\n\nThe object files generated in the assembly stage is composed of machine instructions that the processor understands, but some pieces of the program are out of order or missing. The linker resolves all the references in a set of object files or archive so that functions in some pieces can successfully call functions in other ones, and then produces an executable. To get the final executable use following command, `-v` option give us detail information of linking process.\n\n    gcc -v -o test.elf test.o\n\nWe can also manually invoke linker separately using `ld` to get the final executable.\n\n    GLIBC_LIB_DIR=\"/usr/lib/x86_64-linux-gnu\"\n    GCC_LIB_DIR=\"/usr/lib/gcc/x86_64-linux-gnu/10\"\n    STARTFILES=\"$GLIBC_LIB_DIR/crt1.o $GLIBC_LIB_DIR/crti.o\"\n    ENDFILES=\"$GLIBC_LIB_DIR/crtn.o\"\n    ld -o test.elf -dynamic-linker /lib64/ld-linux-x86-64.so.2 $STARTFILES test.o $GLIBC_LIB_DIR/libc.so $ENDFILES\n\nThe final executable is also a ELF file.\n\n## ELF\n\nExecutable and Linkable Format (ELF) is a common standard file format used in UNIX system for executable files, object code, shared libraries, and core dumps.\n\n## Execution\n\nAt first, it seems when a program is executed, it starts with the [`int main(int argc, char *argv[])`](https://en.cppreference.com/w/c/language/main_function), however it is not quite true.\n\n### Load Executable with Interpreter\n\nFirstly, when we try to run a program, it trigger an [`execve` system call](https://elixir.bootlin.com/linux/v5.15.6/source/fs/exec.c#L2063) to the kernel. The kernel [allocates the structure `linux_binprm` for a new process](https://elixir.bootlin.com/linux/v5.15.6/source/fs/exec.c#L1891), [open the executable file from disk](https://elixir.bootlin.com/linux/v5.15.6/source/fs/exec.c#L1810), [find the corresponding interpreter for the executable](https://elixir.bootlin.com/linux/v5.15.6/source/fs/exec.c#L1766), in case of our C program executable in ELF format is then [executed](https://elixir.bootlin.com/linux/v5.15.6/source/fs/exec.c#L1725) with [ELF loader](https://elixir.bootlin.com/linux/v5.15.6/source/fs/binfmt_elf.c#L823).\n\n### Load Dynamic Linker\n\nThe ELF loader [read program headers table of executable which contains a field `INTERP`](https://elixir.bootlin.com/linux/v5.15.6/source/fs/binfmt_elf.c#L873). For dynamically linked program `INTERP` is the path to dynamic linker. We can use `readelf --program-headers test.elf` to see the program headers table and use `readelf -x .interp test.elf` to see the value of `INTERP`, its value is `/lib64/ld-linux-x86-64.so.2` in my machine. The kernel [opens](https://elixir.bootlin.com/linux/v5.15.6/source/fs/binfmt_elf.c#L898) and [reads the dynamic linker executable in ELF format](https://elixir.bootlin.com/linux/v5.15.6/source/fs/binfmt_elf.c#L917).\n\n### Auxiliary Vector\n\nKernel uses a special structure called the [auxiliary vector or auxv](https://elixir.bootlin.com/linux/v5.15.6/source/include/linux/mm_types.h#L506) to comminicate with dymanic linker. Kernel [prepares `auxv`](https://elixir.bootlin.com/linux/v5.15.6/source/fs/binfmt_elf.c#L1257) and pass `auxv` by putting on the stack for the newly created program. Thus, when the dynamic linker starts it can use its stack pointer to find the all the startup information required. It contains system specific information that may be required, such as the default size of a virtual memory page on the system or hardware capabilities. We can request the dynamic linker to show some debugging output of the auxv by specifying the environment value `LD_SHOW_AUXV=1`\n\n### Call Dynamic Linker with Program Entry Point\n\nKernel [looks for the `e_entry` field](https://elixir.bootlin.com/linux/v5.15.6/source/fs/binfmt_elf.c#L1193) from the `ELF` header of our program executable which contains the entry point address which by default is symbol `_start`. We can examine the entry point with `objdump -f test.elf`.  We can use option `--entry=<symbol name>` of `ld` to change entry point to other symbol.\n\nKernel [adds the value of `e_entry` to auxv](https://elixir.bootlin.com/linux/v5.15.6/source/fs/binfmt_elf.c#L267). Kernel then [starts the execution](https://elixir.bootlin.com/linux/v5.15.6/source/fs/binfmt_elf.c#L1313) from the [entry point address as specified by dynamic linker](https://elixir.bootlin.com/linux/v5.15.6/source/fs/binfmt_elf.c#L1215).\n\n### Dynamic Linker\n\nInvestigating the dynamic linker with command `objdump -f /lib64/ld-linux-x86-64.so.2` and `objdump --disassemble --section=.text /lib64/ld-linux-x86-64.so.2` we found the entry point of dynamic linker is [function `_dl_rtld_di_serinfo`](https://elixir.bootlin.com/glibc/glibc-2.31/source/elf/dl-load.c#L2263). It does some linking process on the fly by loading any libraries as specified in the dynamic section of the program executable in ELF format and then continue execution from our program executable entry point address which was passed in.\n\n### Kernel Library\n\nTo avoid the overheads of system calls by triggering a trap to the processor which is slow. Kernel loads a shared library (ref: [#1](https://elixir.bootlin.com/linux/v5.15.6/source/fs/binfmt_elf.c#L1252), [#2](https://elixir.bootlin.com/linux/v5.15.6/source/include/linux/elf.h#L31), [#3](https://elixir.bootlin.com/linux/v5.15.6/source/arch/x86/entry/vdso/vma.c#L394)) into the address space of every newly created process which contains a function that makes system calls for you. When the kernel starts the dynamic linker it adds an entry `AT_SYSINFO_EHDR` to the `auxv` structure (ref: [#1](https://elixir.bootlin.com/linux/v5.15.6/source/fs/binfmt_elf.c#L255), [#2](https://elixir.bootlin.com/linux/v5.15.6/source/arch/x86/include/asm/elf.h#L352)) which is the address in the memory that the special kernel library lives in. When the dynamic linker starts it can look for the `AT_SYSINFO_EHDR` pointer, and if found load that library for the program. The program has no idea this library exists; this is a private arrangement between the dynamic linker and the kernel.\n\nThe programmers make system calls indirectly through calling functions in the standard C library. The standard C library can check to see if the special kernel binary is loaded, and if so use the functions within that to make system calls. If the kernel determines the hardware is capable, this will use the fast system call method.\n\n### The role of `_start` function\n\nAs you might have already noticed, in the linking section we have to include somes extras files, this is because [the symbol `_start`](https://elixir.bootlin.com/glibc/glibc-2.31/source/sysdeps/x86_64/start.S#L58) is defined in `crt1.o` (Some systems use [crt0.o](https://en.wikipedia.org/wiki/Crt0), while some use `crt1.o` and a few even use `crt2.o` or higher). It takes care of bootstrapping the initial execution of the program, e.g. setup arguments, prepare environment variables for program execution etc. What exactly that entails is highly [`libc` implementation](https://en.wikipedia.org/wiki/C_standard_library#Implementations) dependent. The objects are provided by [different implementations of libc](http://www.etalabs.net/compare_libcs.html) and cannot be mixed with other ones.\n\nThe following code is disassembled version of `_start` with `objdump --disassemble=_start test.elf`:\n\n    0000000000401040 <_start>:\n              401040:\t      31 ed                \txor    %ebp,%ebp\n              401042:\t      49 89 d1             \tmov    %rdx,%r9\n              401045:\t      5e                   \tpop    %rsi\n              401046:\t      48 89 e2             \tmov    %rsp,%rdx\n              401049:\t      48 83 e4 f0          \tand    $0xfffffffffffffff0,%rsp\n              40104d:\t      50                   \tpush   %rax\n              40104e:\t      54                   \tpush   %rsp\n              40104f:\t      49 c7 c0 10 11 40 00 \tmov    $0x401110,%r8        # __libc_csu_fini\n              401056:\t      48 c7 c1 b0 10 40 00 \tmov    $0x4010b0,%rcx       # __libc_csu_init\n              40105d:\t      48 c7 c7 71 10 40 00 \tmov    $0x401071,%rdi       # our main function\n              401064:\t      ff 15 86 2f 00 00    \tcallq  *0x2f86(%rip)        # 403ff0 <__libc_start_main@GLIBC_2.2.5>\n              40106a:\t      f4                   \thlt    \n\nOn glibc 2.31, `_start` [initializes very early ABI requirements](https://elixir.bootlin.com/glibc/glibc-2.31/source/sysdeps/x86_64/start.S#L59) (like the stack or frame pointer), [setting up the `argc`/`argv`/`env` values](https://elixir.bootlin.com/glibc/glibc-2.31/source/sysdeps/x86_64/start.S#L85), and then [pass pointers](https://elixir.bootlin.com/glibc/glibc-2.31/source/sysdeps/x86_64/start.S#L107) of [__libc_csu_init](https://elixir.bootlin.com/glibc/glibc-2.31/source/csu/elf-init.c#L67), [__libc_csu_fini](https://elixir.bootlin.com/glibc/glibc-2.31/source/csu/elf-init.c#L95) and main function to [`__libc_start_main`](https://refspecs.linuxbase.org/LSB_5.0.0/LSB-Core-generic/LSB-Core-generic/baselib---libc-start-main-.html) which in turn does more general bootstrapping before finally calling the real main function.\n\nThe [implementation of `__libc_start_main`](https://elixir.bootlin.com/glibc/glibc-2.2.5/source/sysdeps/generic/libc-start.c#L49) is quite complicated as it needs to be portable across the very wide number of systems and architectures that `glibc` can run on. It does a number of specific things related to setting up the C library which the most of the programmers don't need to worry about. \n\n### Initialization and Termination Routines\n\n`init` and `fini` are two special parts of code in shared libraries that may need to be called before the library starts, and before the library is unloaded respectively. This might be useful for library programmers to setup variables when the library is started, or to clean up at the end. `__libc_start_main` [call the `__libc_csu_init`](https://elixir.bootlin.com/glibc/glibc-2.2.5/source/sysdeps/generic/libc-start.c#L122) before calling our main function and [register `__libc_csu_fini` as a callback to be called before program exit](https://elixir.bootlin.com/glibc/glibc-2.2.5/source/sysdeps/generic/libc-start.c#L114) with [__cxa_atexit](https://refspecs.linuxbase.org/LSB_5.0.0/LSB-Core-generic/LSB-Core-generic/baselib---cxa-atexit.html). What `__libc_csu_init`/`__libc_csu_fini` do is simply loop the list of [init](https://elixir.bootlin.com/glibc/glibc-2.31/source/csu/elf-init.c#L88)/[fini](https://elixir.bootlin.com/glibc/glibc-2.31/source/csu/elf-init.c#L100) function and invokes them.\n\nIn order to traverse the list of `init` functions, two symbols `__init_array_start` and `__init_array_end` is defined during the linking process and exported as part of ELF symbol table `.symtab`.\n\nWe can use `__attribute__((constructor))` and `__attribute__((destructor))` (ref: [#1](https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#Common-Function-Attributes)) to add [initialization and termination routines](https://gcc.gnu.org/onlinedocs/gccint/Initialization.html) to our program, e.g.\n\n    void __attribute__((constructor)) program_init(void)\n    {\n        printf(\"init\\n\");\n    }\n\n    void __attribute__((destructor)) program_fini(void)\n    {\n        printf(\"fini\\n\");\n    }\n\nIn the new realease of `glibc` the process of `fini` was changed as part of this [commit](https://sourceware.org/git/?p=glibc.git;a=commitdiff;h=9dcafc559763e339d4a79580c333127033e39c11).\n\n### Call Main Function\n\nOnce `__libc_start_main` has completed with the initialization it finally [calls the main function](https://elixir.bootlin.com/glibc/glibc-2.2.5/source/sysdeps/generic/libc-start.c#L129)! Remember that it had the stack setup initially with the arguments and environment pointers from the kernel; this is how main gets its `argc`, `argv[]`, [envp[]](https://stackoverflow.com/a/10321474/9980245) arguments.\n\n### Exit\n\nWhen the main function returns `__libc_start_main` call [`void exit(int exit_code)`](https://en.cppreference.com/w/c/program/exit) with return value of main function as exit code. The [implementation of `exit`](https://elixir.bootlin.com/glibc/glibc-2.31/source/stdlib/exit.c#L137) is trigger a syscall exit_group (ref: [#1](https://elixir.bootlin.com/glibc/glibc-2.31/source/sysdeps/unix/sysv/linux/_exit.c#L31), [#2](https://sourceware.org/git/?p=glibc.git;a=blob;f=sysdeps/unix/sysv/linux/sysdep.h;h=3ef72dc805a6e6246f8fafd935108ff3c10591b1;hb=HEAD#l42), [#3](https://sourceware.org/git/?p=glibc.git;a=blob;f=sysdeps/unix/sysv/linux/i386/sysdep.h;h=ec12c84ca877b87f0eaa7711900465e74d546565;hb=HEAD#l370), [#4](https://sourceware.org/git/?p=glibc.git;a=blob;f=sysdeps/unix/sysv/linux/i386/sysdep.h;h=ec12c84ca877b87f0eaa7711900465e74d546565;hb=HEAD#l308), [#5](https://sourceware.org/git/?p=glibc.git;a=blob;f=sysdeps/unix/sysv/linux/i386/sysdep.h;h=ec12c84ca877b87f0eaa7711900465e74d546565;hb=HEAD#l384), [#6](https://sourceware.org/git/?p=glibc.git;a=blob;f=sysdeps/unix/sysv/linux/i386/arch-syscall.h;h=2512508b7daa8ed23d52cb5441e8ae36b5ef52da;hb=HEAD#l58)) to immediately stops the current process.\n\n## Writing program without `startfiles`\n\nNow we know how the call to the `main` is made. We can override the `_start` function to make it call our `main()`.\n\n    #include <stdio.h>\n    #include <stdlib.h>\n\n    #define LOOP_TIMES 10\n\n    void _start()\n    {\n        exit(main());\n    }\n\n    int main(void)\n    {\n        for (int i = 0; i < LOOP_TIMES; i++)\n        {\n            printf(\"Hello World #%i!\\n\", i);\n        }\n        return 0;\n    }\n\nNow we have to force `gcc` to use our implementation of `_start()`.\n\n    gcc -nostartfiles -o test.elf test.c\n\nWe can also manually invoke `ld`:\n\n    gcc -c -o test.o test.c\n    GLIBC_LIB_DIR=\"/usr/lib/x86_64-linux-gnu\"\n    GCC_LIB_DIR=\"/usr/lib/gcc/x86_64-linux-gnu/10\"\n    ld -o test.elf -dynamic-linker /lib64/ld-linux-x86-64.so.2 test.o $GLIBC_LIB_DIR/libc.so\n\n## Reference\n\n* [The Four Stages of Compiling a C Program](https://www.calleluks.com/the-four-stages-of-compiling-a-c-program/)\n* [Computer Science from the Bottom Up - Chapter 8. Behind the process - Starting a process](https://www.bottomupcs.com/starting_a_process.xhtml)\n* [What happens when you compile?](https://www.cs.tufts.edu/comp/40/docs/compiling-notes.html)\n* [GAS: Explanation of .cfi_def_cfa_offset](https://stackoverflow.com/a/7535848/9980245)\n* [segfault when linking with ld](https://www.linuxquestions.org/questions/programming-9/segfault-when-linking-with-ld-758599/)\n* [How to build a C program using a custom version of glibc and static linking?](https://coderedirect.com/questions/161733/how-to-build-a-c-program-using-a-custom-version-of-glibc-and-static-linking)\n* [Linking a C program directly with ld fails with undefined reference to `__libc_csu_fini`](https://stackoverflow.com/a/6658194/9980245)\n* [Linking a dynamically linked executable with ld](https://stackoverflow.com/a/6656952/9980245)\n* [What is the difference between crtbegin.o, crtbeginT.o and crtbeginS.o?](https://stackoverflow.com/a/27786892/9980245)\n* [ld(1) - Linux man page](https://linux.die.net/man/1/ld)\n* [readelf(1) — Linux manual page](https://man7.org/linux/man-pages/man1/readelf.1.html)\n* [GCC - Options for Linking: -nostartfiles](https://gcc.gnu.org/onlinedocs/gcc/Link-Options.html#index-nostartfiles)\n* [GCC Options for Code Generation Conventions](https://gcc.gnu.org/onlinedocs/gcc/Code-Gen-Options.html)\n* [gcc(1) — Linux manual page](https://man7.org/linux/man-pages/man1/gcc.1.html)\n* [When is the gcc flag -nostartfiles used?](https://stackoverflow.com/a/62167438/9980245)\n* [How do I tell GCC not to link with the runtime library and the standard library?](https://stackoverflow.com/a/34237063/9980245)\n* [Executing main() in C/C++ – behind the scene](https://www.geeksforgeeks.org/executing-main-in-c-behind-the-scene/)\n* [objdump(1) - Linux man page](https://linux.die.net/man/1/objdump)\n* [Objcopy elf to bin file](https://stackoverflow.com/a/49685103/9980245)\n* [objcopy(1) - Linux man page](https://linux.die.net/man/1/objcopy)\n* [BFD](https://sourceware.org/binutils/docs/ld/BFD.html)\n* [Wikipedia: Executable and Linkable Format](https://en.wikipedia.org/wiki/Executable_and_Linkable_Format)\n* [elf(5) — Linux manual page](https://man7.org/linux/man-pages/man5/elf.5.html)\n* [elf.h](https://elixir.bootlin.com/linux/v5.15.3/source/tools/objtool/include/objtool/elf.h#L79)\n* [How can I examine contents of a data section of an ELF file on Linux?](https://stackoverflow.com/a/30452453/9980245)\n* [exec(3) - Linux man page](https://linux.die.net/man/3/exec)\n* [execve(2) — Linux manual page](https://man7.org/linux/man-pages/man2/execve.2.html)\n* [Executing a flat binary file under Linux](https://stackoverflow.com/questions/1283342/executing-a-flat-binary-file-under-linux)\n* [load_flat_binary](https://elixir.bootlin.com/linux/v5.15.6/source/fs/binfmt_flat.c#L104)\n* [What is the difference between exit and return?](https://stackoverflow.com/a/3463562/9980245)\n* [What is the use of _start() in C?](https://stackoverflow.com/a/29694977/9980245)\n* [Syscall implementation of exit()](https://stackoverflow.com/a/46903734/9980245)",
        "slug": "Compiling-Executing-C",
        "date": "2021-12-18 22:28:10",
        "lang": "en",
        "tags": ["C", "Compilation", "ELF", "crt0", "exit"],
        "path": "/Compiling-Executing-C/index.html"
      },
      {
        "title": "Java Concurrent Programming",
        "markdownContentSource": "\n## Introduction\n\nAt the beginning, a program ran from **start** to **finish**, it had access to **all resources** on the system. Nowadays with **operating systems** we are allowed to run various programs simultaneously in processes **independent of the hardware architecture** we have.\n\n### Reason\n\n1. Take advantage of the **IO operations waiting time** to do other tasks.\n2. A better strategy, sharing resources instead of waiting them.\n3. Divide into various **independent tasks**.\n\n### Process\n\nThe execution of a computer program. Each **process** had its own memory space for instructions and data. The same computer program can run multiple times, with some **overlap** and **simultaneously**.\n\n### Sequential Model\n\nThey executed the instructions **sequentially**, interacting with the outside to through I/O where it is **perfectly defined** what instruction will run next.\n\n![Sequential Model](sequential-processing.png)\n\n### Concurrent Process\n\nP1 and P2 are said to be two processes **concurrent** if the first instruction of one of them run between the first and the last instruction of the other. They can be independent, competing the resources or cooperative. The interations between concurrent processes are carried out through the **synchronization** and **communication**. The order of instructions is not guaranteed. The results of executions may vary.\n\n![Concurrent Process](concurrent-process.png)\n\n### Multiprocess\n\nThey run in different processors at the same time.\n\n![Multiprocess](multiprocess.png)\n\n### Multiprogramming\n\nThey are assigned to the same processor and execute in different time slots. You can take advantage of the processor while a process waits for input/output.\n\n![Multiprogramming](multiprogramming.png)\n\n### Race Condition\n\nIn the concurrent context, multiple control flows can access, modify the same resource at the same time and produces **unexpected result**.\n\n### Atomic Operation\n\nAn atomic statement is an instruction that execute as a single indivisible unit.\n\n#### Fine-Grained\n\nMachine Level Intruction\n\n#### Coarse-Grained\n\nA set of instruction execute in conjunction as a single unit.\n\n### Critical Section\n\nIt is **code segment** that **accesses shared variables** and has to be executed as an atomic action.\n\n#### Preprotocol/Postprotocol\n\nThe pre-protocol and post-protocol are the sequences of instructions that the processes must execute to ensure that the instructions in the critical section are executed in compliance with the requirements.\n\n### Mutual Exclusion\n\n**Mutual exclusion** is used to prevent **race conditions**. hence prevents simultaneous access to a shared resource. **Only one of the processes can access** the resource at same time and the others have to wait. When a process **releases** the exclusive access resource and another process was waiting, the waiting process will access the resource. Mutual exclusion allows creating coarse-grained atomic statements.\n\n## Abstractions\n\nWe use abstractions that allow us to abstract from details about system architecture.\n\n1. Each process is considered to **run on its own processor**.\n2. The relative speeds of each process are ignored, making it possible to consider only the sequences of instructions being executed.\n3. The sequences of execution of the **atomic actions** of all processes are considered to be **interleaved in a single sequence** completely without interference.\n\nAll the abstractions can be summarized as the study of the sequences of interleaved execution of the atomic instructions of the sequential processes.\n\n## Thread\n\nThey allow **different control flows** of a program **coexist** within the same process. They share resources like memory, but each has its **own program counter, stack and local variables**. They are sometimes called light processes, and many operating systems consider them the basic units planning. Operating Systems take care of scheduling threads and assigning to different processors as they're available to run simultaneously and asynchronous with respect to each other. They share **process stask**, so everyone has access to them, allowing share data more efficiently.\n\n* Create: When an instance of Thread is created (via new Thread(Runnable target)), it does not start executing right.\n* Start: The threads start running when invokes the **start()** method of the Thread class.\n\nA program ends its execution when\n\n* All its threads have finished their execution\n* The **System.exit()** method is executed\n\nA thread ends its execution\n\n* When all your statements have been executed\n* When an unchecked exception is raised (RuntimeException) in the **run()** method\n* User cancellation\n* Timeout\n* Events that trigger in other threads\n  * Interruption with **Thread.interrupt()**\n    * We should periodically check if another thread has interrupted this thread.\n    * We will receive InterruptedException if the theread is blocked.\n  * etc...\n\nWhen a thread finishs it should release resources appropriately, close connections, and leave objects in a stable state.\n\nIf A thread wants to wait for another thread to end then invokes the **join()** method on the Thread class object that represents that thread.\n\n### Daemon\n\nDaemon threads ends automatically when all non-daemon threads in a program have ended. A thread is a demon if the thread that creates it is also a demon. The **isDaemon()** and **setDaemon()** methods allow you to change this property of threads.\n\n### Priority\n\nThe priority of a new thread is the same as the parent thread. The SO will respect the Java thread priority as far as possible.\n\n* **setPriority(int p)**: Set the priority. Its value must be between Thread.MIN_PRIORITY and Thread.MAX_PRIORITY\n* **int getPriority()**: Returns the priority of the thread.\n\n### Group\n\nThreads can be grouped into groups represented by **ThreadGroup**. A ThreadGroup can have other groups of threads inside, creating a tree structure.\n\n* Limit the priority of the threads that contain\n* Manage certain properties of threads together\n\n### Properties\n\n* **Thread.currentThread()**\n* **.getName()**\n* **isAlive()**\n* **getState()**: Returns the state of the thread (new, executing, waiting ...)\n\n## Conditional Synchronization\n\nOccurs when one or more process must wait for a certain condition to be met to continue its execution. That condition must be set by another process.\n\n### Barrier Synchronization\n\n**Barrier** is a conditional synchronization in which the processes have to wait for the rest of the processes to reach the same point in order to continue their execution.\n\n## Properties of Corrections\n\nWe use them to judge if a concurrent algorithm is correct. They must be complied with in any possible intercalation of atomic instructions.\n\n### Safety\n\n1. **Mutual Exclusion**\n2. **Absense of Deadlock** (when multiples threads wait to each other and forms a cycle)\n\n### Liveness\n\nWe must avoid a parallel program enter a state in which it stops makeing forward progress.\n\n1. **Starvation**\n2. **Freedom from deadlock**\n3. **Absense of Livelock**\n\n### Absence of Unnecessary Delays\n\n### Fairness\n\n1. Linear Waiting\n2. FIFO\n3. Priority\n4. Random\n\n## Parallelism\n\nIn the sequential programming you take a sequential algorithm and specify it as a sequence of steps. The parallelism is the study of which of these steps can run in parallel with each and how they should be coordinated. We use some notation here. **fork** when followed by a statement that causes the parent task to create a new child task to execute the body of the asynchronously with the remainder of the parent task statement, and **join** that specifies at the end of finish scope you're guaranteed all asynchronous sub-tasks will have completed before can proceed. **fork** and **join** constructs may be arbitrarily nested.\n\n### Computation Graph (CGs)\n\nWe can use **computation graph** to model the task relationship between each task. The **computation graph** which model the **execution of a parallel program** as a **partially ordered set**. CGs consists of: \n\n* A set of vertices or nodes, in which each node represents a **step** consisting of an arbitrary sequential computation.\n* A set of directed edges that represent **ordering constraints among steps**.\n\nFor fork join programs, it is useful to partition the edges into three cases:\n\n1.  **Continue edges** that capture sequencing of steps within a task.\n2.  **Fork edges** that connect a fork operation to the first step of child tasks.\n3.  **Join edges** that connect the last step of a task to all join operations on that task.\n\nIt helps us reason about which statements can execute in parallel. We ask \"Is there a path of directed edges from one statement to another?\". So for example, there's path from S2 and S4. So that tells us that S2 and S4 cannot run in parallel with each other. But between S2 and S3, we can see there's a parallel execution that's possible, because these's no path of directed edges between S2 and S3. \n\n**CGs** can be used to define **data races**, an important class of bugs in parallel programs. We say that a data race occurs on location $L$ in a computation graph, $G$, if there exist steps $S_1$ and $S_2$ in $G$ such that there is no path of directed edges from $S_1$ to $S_2$ or from $S_2$ to $S_1$ in $G$, and both $S_1$ and $S_2$ read or write _L_ (with at least one of the accesses being a write, since two parallel reads do not pose a problem).\n\n#### Work and Span\n\nCGs can also be used to reason about the **performance** of a parallel program as follows:\n\n* Define  **WORK(G)** to be the sum of the execution times of all nodes in CG  **G**,\n* Define  **SPAN(G)** to be the length of a longest path in  $G$, when adding up the execution times of all nodes in the path. The longest paths are known as  **critical paths**.\n* Given the above definitions of **WORK** and **SPAN**, we define the **ideal parallelism** of Computation Graph $G$ as the ratio, $\\frac{WORK(G)}{SPAN(G)}$.\n* The ideal parallelism is an upper limit on the speedup factor that can be obtained from parallel execution of nodes in computation graph _G_. Note that ideal parallelism is only a function of the parallel program, and does not depend on the actual parallelism available in a physical computer.\n\nSo in this case, the **work** would be 1 plus 10 plus 10 plus 1. That's 22. And the **span** which is 12 in this case. So the **ideal parallelism** is 2\n\n![Computation Graph](computation-graph.png)\n\n#### Time of execution and Speedup\n\nWe defined $T_{P}$ as the execution time of a CG on $P$ processors. The definition of the execution time on $P$ processors actually depends on the schedule. Suppose we have greedy schedule in which a processor is not permitted to be idle if a CG node is available to be scheduled on it. Given any $P$ processors,  $Span = T_{\\infty} \\le T_P \\le T_{1} = Work$. We then defined the parallel speedup for a given schedule of a CG on $P$ processors as $\\text{Speedup}(P) = \\frac{T_1}{T_P}$​ and observed that $\\text{Speedup}(P)$ must be less than the number of processors $P$, and also less than the ideal parallelism, $\\frac{Work}{Span}$. Our goal in parallel algorithms is to generate computation graphs with ideal parallelism that's much larger than the number of processors that you have, so that you have the flexibility of running that parallel program on numerous processors.\n\n#### Amdahl's Law\n\nLet's assume that $q$ is the fraction of $WORK$ in a parallel program that must be executed **sequentially**, the **span** must be at least $q \\cdot \\text{Work}$, then the maximum speedup that can be obtained for that program for any number of processors $P$ is going to be bounded over by $\\text{Speedup}(P) \\le \\frac{1}{q}$.\n\nThis observation follows directly from a lower bound on parallel execution time $\\text{SPAN}(G) \\le T_p$. If fraction $q$ of $WORK(G)$ is sequential, it must be the case that $q \\cdot \\text{WORK}(G) \\le \\text{Span}(G) \\le T_p$. Therefore, $\\text{Speedup}(P) = \\frac{T_1}{T_P} \\le \\frac{\\text{WORK}(G)}{q \\cdot \\text{WORK}(G)} = \\frac{1}{q}$ since $T_1​ = WORK(G)$ for greedy schedulers.\n\nWhat they mean is that there's some part of the computation that's being done inherently sequentially that is going to limit the speedup. Amdahl’s Law reminds us to watch out for sequential bottlenecks both when designing parallel algorithms and when implementing programs on real machines. Even if $q=10\\%$ then best possible speedup must be $\\le 10 = \\frac{1}{q}$, regardless of the number of processors available.\n\n### Forkjoin\n\nParallel versions of classic algorithms divide and conquer. Divide a task into smaller sub-tasks that can be executed concurrently.\n\n#### Fork\n\nA task split itself into smaller subtasks a task into smaller sub-tasks which can be executed concurrently.\n\n![Fork](fork.png)\n\n### Join\n\nWhen a task has split itself up into subtasks, the task waits until the subtasks have finished executing. Once the subtasks have finished executing, the task may join all the results into one result.\n\n![Join](join.png)\n\n### ForkJoinPool\n\nForkJoinPool is similar to the ExecutorService but designed to work efficiently with fork/join task division.\n\n* **new ForkJoinPool()**: The default constructor will create a pool with as many threads as there are processors available.\n* **new ForkJoinPool(int parallelism)**: We can also specify the number of threads\n* **ForkJoinPool.commonPool()**: return a common pool\n\nThe order matter, we should create first with **fork()** and then **join()**.\n\n[Fork Join Sum](https://github.com/vitaminac/code/blob/9e9c769b30e3eb9c21e449b9b641a020250e76fb/rubbish/src/main/java/concurrente/Tema5.java#L193-L221)\n\nThe idea is that you perform all the computations in L and R in parallel, wait for them to complete and then proceed next.\n\n### MapReduce\n\nThe main idea of MapReduce is similar to Fork/Join.\n\n* Reduce task size and assign them to multiple computers concurrently.\n* The results are retrieved and integrated to create the final result.\n\n#### Map\n\nYou select one data set and transform it into another, where each element is divided into key value pairs.\n\n#### Reduce\n\nSelect Map output as input and combine those pairs into smaller sets of tuples\n\n![Map Reduce](map-reduce.png)\n\n## Future\n\nA Java Future represents the result of an asynchronous computation. When the asynchronous task is created, a Java Future object is returned. This Future object functions as a handle to the result of the asynchronous task. Once the asynchronous task completes, the result can be accessed via the Future object returned when the task was started.\n\n### Get Result\n\nAs mentioned earlier, a Java **Future** represents the result of an asynchronous task. To obtain the result, you call **get()** methods on the Future. If you call the get() method before the asynchronous task has completed, the get() method will block until the result is ready.\n\n### Functional Parallelism\n\nThe future is carefully defined to avoid the possibility of a race condition and it is suited for functional parallelism. So if we write our tasks as pure functions calls like $F(X)$ so that if you call $F(X)$ multiple times with the same input, you will always get the same output $Y$. From that observation, we see that if pure function $G$ and pure function $H$ only depend on the output of $F$ then these two computations could actually execute in either order. Future of $F$ act as a wrapper of the output value $F$ and it is final. If we pass the future of output value from $F$, then even if $G$ and $F$ run in parallel when they actually need the output value from $F$ they will get blocked and wait until the value is available. From the computation graph perspective, the join edges arise from the get operations of future object.\n\n![Future Get](Future-Get.png)\n\n## Memorization\n\nThe memoization pattern lends itself easily to parallelization using futures by modifying the memoized data structure X to store Future\\<X\\>\n\n## Determinism\n\n### Functional Determinism\n\nA parallel program is said to be **functionally deterministic** if it always computes the same answer when given the same input.\n\n### Structural Determinism\n\nThe idea behind **structural determinism** is it always computes the same computation graph, when given the same input.\n\n### Data Race\n\nA data race is an unsafe access to the same piece of data from two independently parallel executions without some mechanism in place to avoid the conflict. The presence of data races often leads to functional and/or structural nondeterminism because a parallel program with data races may exhibit different behaviors for the same input, depending on the relative scheduling and timing of memory accesses involved in a data race. In general, the **absence of data races is not sufficient to guarantee determinism**. However, all the parallel constructs **fork**, **join**, **future** were carefully selected to ensure if a parallel program is written using these constructs and is guaranteed to be **data-race freedom**  that's the **absence of data races**, then it implies both **functional determinism** and **structural determinism**.\n\n### Nondeterministic\n\nFurthermore, there may be cases of **nondeterministic** programs in which different executions with the same input may generate different outputs, but all the outputs may be **acceptable** in the context of the application.\n\n## Parallelism Loop\n\nThe most general way express parallelism loop is to think of each iteration of a parallel loop as an **fork** task, with a **join** construct encompassing all iterations.\n\n### Barriers in Parallel Loops\n\nThe barriers extend a parallel loop by dividing its execution into a sequence of **phases**. While it may be possible to write a separate **parallelism loop** for each phase, it is both more convenient and more efficient to instead insert barriers in a single **parallelism  loop**.\n\n### Iteration Grouping\n\nWe observed that this approach creates  **n** tasks, one per  **parallelism loop** iteration, which is wasteful when **n** is much larger than the number of available processor cores. To address this problem, we learned a common tactic used in practice that is referred to as  **iteration grouping**, and focuses on reducing the number of tasks created to be closer to the number of processor cores, to reduce the overhead of parallel execution. There are two well known approaches for iteration grouping: **block**  and **cyclic**. The **block** form maps consecutive iterations to the same group, whereas **cyclic** maps iterations in the same congruence class $i \\mod ng$ to the same group.\n\n## Phaser\n\n**Barrier** may take some computation time and it is ideal if we can overlap the computation with other computation and the **span** can be lower. So **phaser** come in, **arriveAndAwaitAdvance** can be used to implement a barrier through **phaser** object. To facilitate the **split-phase barrier** (also known as a **fuzzy barrier**) we use two separate APIs from Java Phaser class — **.arrive()** and **awaitAdvance()**. Together these two APIs form a barrier, but we now have the freedom to insert a computation to be performed in parallel with the barrier between the two calls.\n\n### Point-to-Point Synchronization with Phasers and Dataflow Synchronization\n\nWe can use multiple phasers to create the dependencies graph between them.\n\n### Pipeline Parallelism\n\nLet $n$ be the number of input items and $p$ the number of stages in the pipeline, $WORK = n × p$_ is the total work that must be done for all data items, and $SPAN = n + p − 1$ for the pipeline. Thus, the ideal parallelism is $WORK/SPAN = np / (n + p − 1)$. When $n$ is much larger than $p$ then the ideal parallelism approaches $PAR = p$ in the limit. The synchronization required for pipeline parallelism can be implemented as follows\n\n\t// Code for pipeline stage i\n\twhile ( there is an input to be processed ) {\n\t  // wait for previous stage, if any \n\t  if (i > 0) ph[i - 1].awaitAdvance(); \n\t  \n\t  process input;\n\t  \n\t  // signal next stage\n\t  ph[i].arrive();\n\t}\n\n## Producer/Consumer\n\n* **Producer**: produces a message.\n* **Consumer**: consumes and remove the message.\n* Each producer generates a single data each time.\n* A consumer can only consume one data.\n* All the product will be process.\n* You cannot consume the same product twice.\n\n## Reader/Writer\n\nSome threads may read and some may write, with the constraint that no thread may access the shared resource for either reading or writing while another thread is in the act of writing to it.\n\n* **Readers**: Processes which are not required to exclude one another. Any number of readers may simultaneously read the resource.\n* **Writers**: Processes which exclude all the other readers when writing a resource. As long as a reader is reading, no writer can access the DB.\n* Requirement: There may be several writers working, although these will have to be synchronized so that the writing is carried out one by one\n* Requirement: Writers have priority. No reader can access the DB when there are writers who wish to do so.\n\n## Dining Philosopher Problem\n\n* Preprotocol: The philosopher sits down in front of his plate and takes the forks on either side of his plate one by one.\n* Critical Section: Eat\n* Postprotocol: When finished, leave the two forks in their original position\n* Requirement: Every philosopher who eats, at some point is satisfied and ends\n* Requirement: A philosopher can only eat when he has both forks\n* Requirement: The forks are picked up and put down one by one\n* Requirement: Two philosophers cannot have the same fork simultaneously\n* Requirement: If several philosophers try to eat at the same time, one of them must succeed\n* Requirement: In the absence of competition, a philosopher who wants to eat must do so without unnecessary delay.\n\n### Deadlock\n\n[Deadlock](https://github.com/vitaminac/code/blob/194f39a2c674b803dad8fb64ff6897bd271fe955/judge/src/main/java/geeksforgeeks/concurrent/DiningPhilosopher.java#L18-L44)\n\n### Livelock\n\n[Livelock](https://github.com/vitaminac/code/blob/194f39a2c674b803dad8fb64ff6897bd271fe955/judge/src/main/java/geeksforgeeks/concurrent/DiningPhilosopher.java#L46-L81)\n\n### Solution\n\n[DiningPhilosopher solution](https://github.com/vitaminac/code/blob/194f39a2c674b803dad8fb64ff6897bd271fe955/judge/src/main/java/geeksforgeeks/concurrent/DiningPhilosopher.java#L83-L112)\n\n## Volatile\n\nReading and writing **simple type** variable are atomic instructions, unless the variable is of type **long** or **double**. For modern computer, usually there is a cache for each processor. Although the change to primitive variable is atomic, the write is not neccesary **visible** to other processor. To do so, you have to declare the variable as **volatile** which guarantee the visibility for all the primitive types for all processors. \n\nIt is not necessary to use volatile if the threads use some synchronization method. If the shared resources are mutually exclusive with semaphore, the correct values are guaranteed to be read. If one process writes an resource and unlocks another process, the other process will read the written value.\n\n## Busy Waiting\n\nWhen we use volatile variable for synchronization. Busy waiting is very inefficient and generally should be avoided.\n\n### Dekker's algorithm\n\n#### Mandatory Alternation - Unnecessary Delays\n\nA turn variable is used that indicates the process that can enter the critical section.\n\n    private static volatile int turn;\n\n    private static void p1() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            while (turno != 1);\n            \n            /* Critical Section */\n\n            /* Postprotocol */\n            turn = 2;\n            \n            /* No critical Section */\n        }\n    }\n\n    private static void p2() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            while (turn != 2);\n\n            /* Critical Section */\n\n            /* Postprotocol */\n            turn = 1;\n\n            /* No critical Section */\n        }\n    }\n\n#### No Mutual Exclusion\n\nWe can use a boolean variable for each process that indicates if said process is in the critical section.\n\n    private static volatile boolean p1cs;\n    private static volatile boolean p2cs;\n\n    private static void p1() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            while (p2cs);\n            p1cs = true;\n\n            /* Critical Section */\n\n            /* Postprotocol */\n            p1cs = false;\n\n            /* No critical Section */\n        }\n    }\n\n    private static void p2() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            while (p1cs);\n            p2cs = true;\n\n            /* Critical Section */\n\n            /* Postprotocol */\n            p2cs = false;\n            \n            /* No critical Section */\n        }\n    }\n\nBoth processes can execute the critical section instructions at the same time.\n\n#### Deadlock\n\nLet we request the access before we enter the critical section.\n\n    private static volatile boolean intent_p1;\n    private static volatile boolean intent_p2;\n    \n    private static void p1() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            intent_p1 = true;\n            while (intent_p2);\n\n            /* Critical Section */\n            \n            /* Postprotocol */\n            intent_p1 = false;\n\n            /* No critical Section */\n        }\n    }\n\n    private static void p2() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            intent_p2 = true;\n            while (intent_p1);\n\n            /* Critical Section */\n            \n            /* Postprotocol */\n            intent_p2 = false;\n\n            /* No critical Section */\n        }\n    }\n\n#### Starvation - Livelock - No Mutual Exclusion\n\nYields your right to enter the critical section if you discover that there is competition with another process.\n\n    private static volatile boolean intent_p1;\n    private static volatile boolean intent_p2;\n\n    private static void p1() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            intent_p1 = true;\n            while (intent_p2) {\n                intent_p1 = false;\n                intent_p2 = true;\n            }\n\n            /* Critical Section */\n            \n            /* Postprotocol */\n            intent_p1 = false;\n\n            /* No critical Section */\n        }\n    }\n\n    private static void p2() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            intent_p2 = true;\n            while (intent_p1) {\n                intent_p2 = false;\n                intent_p1 = true;\n            }\n\n            /* Critical Section */\n            \n            /* Postprotocol */\n            intent_p2 = false;\n\n            /* No critical Section */\n        }\n    }\n\nAlternatively giving the right and never enter the critical section.\n\n#### Final Solution\n\nIt is a combination of the 1st and 4th approach.\n\n[Dekker's algorithm](https://github.com/vitaminac/code/blob/31d59e0182225178c7eec5631b3661adf3cf650a/judge/src/main/java/geeksforgeeks/concurrent/DekkerAlgorithm.java)\n\n## Semaphore\n\nA semaphore is a Abstract Data Type that support two operations.\n\n1. The **acquire()** method decreases the number of semaphore permissions or hangs until someone release a permission when is zero.\n2. The **release()** method increases the number of semaphore permissions. If there are blocked processes at the semaphore, unblock one of them FIFO or Randomly and continues its execution\n\n* **acquire()**: Acquires the given number of permits from this semaphore, blocking until all are available, or the thread is interrupted.\n* **acquireUninterruptibly()**: Acquires a permit from this semaphore, blocking until one is available.\n* **tryAcquire()**: Acquires a permit from this semaphore, only if one is available at the time of invocation. return false if it is unavailable.\n* **tryAcquire(long timeout, TimeUnit unit)**: Acquires a permit from this semaphore, if one becomes available within the given waiting time and the current thread has not been interrupted, return false if no one is available within the timeout.\n* **drainPermits()**: Acquires and returns all permits that are immediately available.\n* **getQueueLength()**: Returns an estimate of the number of threads waiting to acquire.\n* **hasQueuedThreads()**: Queries whether any threads are waiting to acquire.\n\n### Development Process\n\n1. Define process architecture (number of processes and type)\n2. Sequential implementation\n3. Determine sync points in code\n   1. Conditional Synchronization or Mutual Exclusion\n   2. Number of Semaphores Needed\n   3. Can all processes be blocked together?\n   4. Can any of them be unlocked?\n4. Define necessary semaphores to control synchronization and write acquire() and release()\n5. Variable management\n   1. Initialization of boolean and counters\n   2. Under Mutual Exclusion if they are shared\n\n### Conditional Synchronization With Semaphore\n\n    private static volatile boolean continue = false;\n    private static Semaphore semaphore = new Semaphore(0);\n\n    private static void p1() {\n        /* task p1.1 */\n        continue = true;\n        /* task p1.2 */\n    }\n\n    private static void p2() {\n        /* task p2.1 */\n        while (!continue);\n        /* task p2.2 */\n    }\n\n    private static void p1_semaphore() {\n        /* task p1.1 */\n        semaphore.release();\n        /* task p1.2 */\n    }\n\n    private static void p2_semaphore() {\n        /* task p2.1 */\n        semaphore.acquire();\n        /* task p2.2 */\n    }\n\n### Mutual Exclusion with Semaphore\n\n    private static Semaphore sem = new Semaphore(1);\n\n    private static void p() throws InterruptedException {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            sem.acquire();\n\n            /* Critical Section */\n\n            /* Postprotocol */\n            sem.release();\n\n            /* No Critical Section */\n        }\n    }\n\n### Barrier with Semaphore First Example\n\nThe processes have to wait for everyone to have written the letter 1 before writing the 2.\n\n#### First Incorrect Approach\n\n    private static volatile int nProcess = 0;\n    private static Semaphore sem = Semaphore(0);\n    private static void p() throws InterruptedException{\n        System.out.println(\"1\");\n        nProcess += 1;\n        if (nProcess < N_PROCESSES) sem.acquire();\n        else for (int i = 0; i < N_PROCESSES - 1; i++) sem.release();\n        System.out.println(\"2\");\n    }\n\n**nProcess** is not under mutual exclusion\n\n#### Second Incorrect Approach\n\n    private static volatile int nProcess = 0;\n    private static Semaphore sem = Semaphore(0);\n    private static Semaphore lock = Semaphore(1);\n    private static void p() throws InterruptedException{\n        System.out.println(\"1\");\n        lock.acquire();\n        nProcess += 1;\n        lock.release();\n        if (nProcess < N_PROCESSES) sem.acquire();\n        else for (int i = 0; i < N_PROCESSES - 1; i++) sem.release();\n        System.out.println(\"2\");\n    }\n\nQuery nProcess outside the mutual exclusion may result multiple processes release the sem which will leave the sem in the unpredictable state.\n\n#### First Correct Approach\n\n    private static volatile int nProcess = 0;\n    private static Semaphore sem = Semaphore(0);\n    private static Semaphore lock = Semaphore(1);\n    private static void p() throws InterruptedException{\n        System.out.println(\"1\");\n        lock.acquire();\n        nProcess += 1;\n        if (nProcess < N_PROCESSES) {\n            lock.release();\n            sem.acquire();\n        }\n        else {\n            lock.release();\n            for (int i = 0; i < N_PROCESSES - 1; i++) {\n                sem.release();\n            }\n        }\n        System.out.println(\"2\");\n    }\n\nWe release after the query\n\n#### Second Correct Approach\n\n    private static volatile int nProcess = 0;\n    private static Semaphore sem = Semaphore(0);\n    private static Semaphore lock = Semaphore(1);\n    private static void p() throws InterruptedException{\n        System.out.println(\"1\");\n        lock.acquire();\n        nProcess += 1;\n        if (nProcess == N_PROCESSES) {\n            for (int i = 0; i < N_PROCESSES; i++) {\n                sem.release();\n            }\n        }\n        lock.release(); \n        sem.acquire();\n        System.out.println(\"2\");\n    }\n\n### Barrier with Semaphore\n\n[SemaphoreBarrier](https://github.com/vitaminac/code/blob/a286df7d1569887f7adbafd4d2aec83ff0ee4c3a/core/src/main/java/core/concurrent/SemaphoreBarrier.java)\n\n### K Mutual Exclusion\n\nWhen the number of processes that can run the critical section at once is N > 1, it is implemented with semaphore assigning initially a K value at the semaphore.\n\n    private static final int N;\n    private static final Semaphore sem = new Semaphore(N);\n\n    private static void p() throws InterruptedException {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            sem.acquire();\n            \n            /* Critical Section */\n\n            /* Postprotocol */\n            sem.release();\n            \n            /* No Critical Section */\n        }\n    }\n\n### Buffer\n\nIn producer-consumer problem\n\n* A buffer will be used to store the data produced before being consumed\n* Producer should block if the buffer is full\n* Consumers should block when they have no data to consume.\n* Control variables must be under mutual exclusion\n\n[SemaphoreArrayBlockingQueue](https://github.com/vitaminac/code/blob/e34b48b84814921fe2947e9123a1d138227c4762/core/src/main/java/core/concurrent/SemaphoreArrayBlockingQueue.java)\n\n### Reader/Writer With Semaphore\n\n[Semaphore_DB_Lock](https://github.com/vitaminac/code/blob/c47e3f3e58e612ed98dbd1f1cb5d2a53c26f8350/core/src/main/java/core/concurrent/Semaphore_DB_Lock.java)\n\n## Lock\n\n### Structured locks\n\nStructured locks can be used to enforce mutual exclusion and avoid data races. A major benefit of structured locks is that their acquire and release operations are implicit, since these operations are automatically performed by the Java runtime environment when entering and exiting the scope of a **synchronized** statement or method, even if an exception is thrown in the middle.\n\nThe synchronized keyword is put into the method and current object acts as a lock. Synchronized statements and methods are reentrant. If a thread that has acquired the lock in a synchronized method, calls another synchronized method, picks up the lock again, it does not stay locked. This allows the reuse of synchronized methods. But they have some limitations.\n\n* A synchronized block makes no guarantees about the sequence in which threads waiting to entering it are granted access.\n* Mutual exclusion cannot be acquired in one method and released in another.\n* You cannot specify a maximum waiting time to acquire the lock.\n* You cannot create an extended mutual exclusion.\n\n#### Synchronized Block\n\n\n    private static int x = 0;\n    private static Object lock = new Object();\n    \n    private static void inc() {\n        for (int i = 0; i < N; i++) {\n            synchronized (lock) {\n                x = x + 1;\n            }\n        }\n    }\n\n    private static void dec() {\n        for (int i = 0; i < N; i++) {\n            synchronized (lock) {\n                x = x - 1;\n            }\n        }\n    }\n\n#### Synchronized Method\n\n    public class Counter {\n        private int x = 0;\n        \n        public synchronized void inc() {\n            x = x + 1;\n        }\n        \n        public int getValue() {\n            return x;\n        }\n    }\n\n### Lock Interface\n\n**java.util.concurrent.locks.Lock** provide explicit **lock()** and **unlock()** operations on unstructured locks can be used to support a hand-over-hand locking pattern that implements a non-nested pairing of lock/unlock operations which cannot be achieved with **synchronized** statements/methods.\n\n    lock.lock();\n    try {\n        /* Critical Section */\n    } finally {\n        lock.unlock()\n    }\n\n* **lock()**: locks the Lock instance if possible. If the Lock instance is already locked, the thread calling lock() is blocked until the Lock is unlocked.\n* **lockInterruptibly()**: Acquires the lock unless the current thread is interrupted.\n* **tryLock()**: Acquires the lock only if it is not held by another thread at the time of invocation.\n* **tryLock(long time, TimeUnit unit)**: Acquires the lock if it is not held by another thread within the given waiting time and the current thread has not been interrupted.\n* **unlock()**: Unlocks the Lock instance. Typically, a Lock implementation will only allow the thread that has locked the Lock to call this method.\n\n### ReentrantLock\n\n* **ReentrantLock(boolean fair)**: Creates an instance of ReentrantLock with the given fairness policy.\n* **getQueueLength()**: Returns an estimate of the number of threads waiting to acquire this lock.\n* **isHeldByCurrentThread()**: Queries if this lock is held by the current thread.\n* **isLocked()**: Queries if this lock is held by the current thread.\n* **isFair()**: Returns true if this lock has fairness set true.\n\n### ReadWriteLock\n\nImplementation of extended mutual exclusion of readers and writers.  It allows multiple threads to read a certain resource, but only one to write it, at a time.\n\n* **Read Lock**: If no threads have locked the ReadWriteLock for writing, and no thread have requested a write lock (but not yet obtained it). Thus, multiple threads can lock the lock for reading.\n* **Write Lock**: If no threads are reading or writing. Thus, only one thread at a time can lock the lock for writing.\n\n\n    private ReadWriteLock readWriteLock = new ReentrantReadWriteLock();\n    private void reader() {\n        readWriteLock.readLock().lock();\n        \n        // multiple readers can enter this section\n        // if not locked for writing, and not writers waiting\n        // to lock for writing.\n\n        readWriteLock.readLock().unlock();\n    }\n    private void writer() {\n        readWriteLock.writeLock().lock();\n\n        // only one writer can enter this section,\n        // and only if no threads are currently reading.\n\n        readWriteLock.writeLock().unlock();\n    }\n\n### StampedLock\n\n## Monitor\n\n* Provides mutual exclusion\n* Provides conditional synchronization allowing one process to be locked and another process to unlock it.\n* Provide a mechanism for threads to temporarily give up for a exclusive access in order to wait for some condition to be met, before regaining exclusive access and resuming their task.\n\nA monitor consists of a **lock** and **condition variables**. In Java every object can act as a **monitor**, the mutual exclusion is define by mutual exclusion by **synchronized** on the object and the condition variables are provided with following method in the synchronized object.\n\n* **wait()**: Causes the current thread to wait until another thread invokes the **notify()** method or the **notifyAll()** method for this object\n* **notify()**: Wakes up a single thread that is waiting on this object's monitor.\n* **notifyAll()**: Wakes up all threads that are waiting on this object's monitor.\n\nThose methods can only be invoked within a synchronized statement or synchronized method. When a thread is locked in the condition variables the mutual exclusion is released automatically.\n\n[MonitorBarrier](https://github.com/vitaminac/code/blob/b55abe7deed23e639ca6b55e671b3728b43f307f/core/src/main/java/core/concurrent/MonitorBarrier.java)\n\nIt is not possible to notify a particular thread. Because unexpected activations can occur, we have to implement a protection mechanism in **wait()**. It is recommended to block the threads with a guard condition when notify all threads and all will be blocked again except the one that meets the condition.\n\nWith locks we can have several conditions rather the single one, we can create a condition with **newCondition()** that return a object with following methods.\n\n* **await()**\n* **signal()**\n* **signalAll()**\n\nIt is necesarry to have **await** inside a loop due to the expected activation.\n\n[LockBarrier](https://github.com/vitaminac/code/blob/62a4cd718375cfb4725a671a7212a2583be2df5c/core/src/main/java/core/concurrent/LockBarrier.java)\n\n## Exchanger\n\nExchanger allows two threads to exchange objects with each other\n\n    public V exchange (V e) throws InterruptedException\n\n* The first thread that executes exchange() is blocked until the other thread also executes that method.\n* When the second thread executes exchange() both threads exchange the values passed as a parameter and continue their execution.\n\n------------------------------------------------------------------------------\n\n    private Exchanger<Integer> exchanger = new Exchanger<Integer>();\n\n------------------------------------------------------------------------------\n\n    public static void producer() throws InterruptedException {\n        for(int i = 0; i < N; i++) {\n            exchanger.exchange(i);\n        }\n    }\n\n------------------------------------------------------------------------------\n\n    public static void consumer() throws InterruptedException {\n        for(int i = 0; i < N; i++) {\n            exchanger.exchange(i);\n        }\n    }\n\n## CountDownLatch\n\nOne or more threads invoke **await()** and that blocks them waiting for **countDown()** is invoked as many times as specified in the object's constructor.\n\n## CyclicBarrier\n\n**CyclicBarrier(int parties, Runnable barrierAction)**: Number of barrier threads and code executed when the barrier is tripped.\n\n**await()**: blocks the thread until the other threads arrive.\n\n## Concurrent Collections\n\nAll actions performed on the collection must be synchronized.\n\n    public String deleteLast(List<String> list) {\n        synchronized (list) {\n            int lastIndex = list.size() - 1;\n            return list.remove(lastIndex);\n        }\n    }\n\n### Synchronization Wrappers\n\n* Collections.synchronizedList\n* Collections.synchronizedMap\n* Collections.synchronizedSet\n\n### BlockingQueue\n\nIt is a **thread-safe** queue which is thread safe to put elements into, and take elements out of from. If the queue is full, the **put(E)** methods are locked until there is space. If the queue is empty then **take()** will block the thread until one element is available.\n\n![BlockingQueue](blocking-queue.png)\n\n* **put(e)**: Blocks until operation can be performed\n* **offer(e, time, unit)**: Blocks and returns false if the operation is not performed in the indicated time\n* **take()**: Blocks until operation can be performed\n* **poll(time, unit)**: Bloquea y devuelve null si no se realiza la operación en el tiempo indicado.\n\nImplementations:\n\n* ArrayBlockingQueue\n* LinkedBlockingQueue\n* PriorityBlockingQueue\n* SynchronousQueue\n  * just allow a single element\n* DelayQueue\n  * keeps items in the queue for a specified time\n* LinkedBlockingDeque\n* TransferQueue\n  * calling **transfer(E e)** will guarantee that all existing queue items will be processed before the transferred item\nan element\n  * LinkedTransferQueue\n\n\n    public class Producer implements Runnable {\n\n        protected BlockingQueue queue = null;\n\n        public Producer(BlockingQueue queue) {\n            this.queue = queue;\n        }\n\n        public void run() {\n            try {\n                queue.put(\"1\");\n                Thread.sleep(1000);\n                queue.put(\"2\");\n                Thread.sleep(1000);\n                queue.put(\"3\");\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n\n    public class Consumer implements Runnable {\n\n        protected BlockingQueue queue = null;\n\n        public Consumer(BlockingQueue queue) {\n            this.queue = queue;\n        }\n\n        public void run() {\n            try {\n                System.out.println(queue.take());\n                System.out.println(queue.take());\n                System.out.println(queue.take());\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n\n### ConcurrentMap\n\nIt represents a extended Map's interface which is capable of handing concurrent access to it.\n\n* **compute(K key, remappingFunction)**: map for the specified key and its current mapped value (or null if there is no current mapping\n* **putIfAbsent(K key, V value)**: If the specified key is not already associated with a value, associate it with the given value.\n* **replace(K key, V value)**: Replaces the entry for a key only if currently mapped to some value.\n* **replace(K key, V oldValue, V newValue)**: Replaces the entry for a key only if currently mapped to a given value.\n\nImplementations:\n\n* **ConcurrentHashMap**\n* **ConcurrentSkipListMap**\n\n### CopyOnWrite\n\nCopy-on write collections allow secure concurrent access because they are unchanging objects. When they are modified, a copy is created for subsequent readings. As it is expensive to make the copy when it is modified, this structure is designed for cases where reads are much more common than writes\n\n#### CopyOnWriteArrayList\n\n#### CopyOnWriteArraySet\n\n### ConcurrentSkipListSet\n\n### ConcurrentLinkedQueue\n\n### ConcurrentLinkedDeque\n\n## Thread Pool\n\nA thread pool is responsible for managing the execution of a group of threads. They contain a queue that is responsible for managing and waiting for tasks to run. Threads are running continuously, checking for a new task in the queue to perform.\n\n![Thread Pool](ThreadPool.png)\n\n### Executor\n\nInterface that allows launching new tasks.\n\n### ExecutorService\n\nImplements the Executor interface, adding the functionality of thread life cycle management\n\n* **newSingleThreadExecutor()**: Create a single thread.\n* **newFixedThreadPool**: Create a thread pool that contains the number of threads we need.\n* **newCachedThreadPool()**\n  * Create a thread pool that will launch new threads when necessary, but will try to reuse old ones when they become available.\n  * They are recommended for applications that perform very short tasks\n  * Threads that have not been used for more than 60 seconds end and are removed from the pool.\n* Each task is executed using the **execute()** method of the ExecutorService that we have created.\n* It is necessary to finish each ExecutorService that we use with**shutdown()**.\n\n    ExecutorService executor = Executors.newSingleThreadExecutor();\n    for (int i = 0; i < 10; i++) {\n        executor.execute(() -> Thread.sleep(100));\n    }\n    executor.shutdown();\n\n\nIf we need a task to return a result, we have to use **Callable** interface. We use **submit()**, which execute the task and it a Future\\<T\\>.\n\n    public class RunnableExample implements Runnable {\n        @Override\n        public void run() {\n            // Task code\n        }\n    }\n    public class CallableExample implements Callable<T> {\n        @Override\n        public T call() throws Exception {\n            // Task code\n            return null;\n        }\n    }\n\n#### Shutdown\n\n The ExecutorService needs to be shut down when you are finished using it. If not, it will keep the JVM running, even when all other threads have been shut down.\n\n### ScheduledExecutorService\n\nThe java.util.concurrent.ScheduledExecutorService is an ExecutorService which can schedule tasks to run after a delay, or to execute repeatedly with a fixed interval of time in between each execution.\n\n    ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(4);\n\n#### schedule(Callable task, long delay, TimeUnit timeunit)\n\nThis method schedules the given Callable for execution after the given delay.\n\nThe method returns a ScheduledFuture which you can use to either cancel the task before it has started executing, or obtain the result once it is executed.\n\n#### schedule(Runnable task, long delay, TimeUnit timeunit)\n\nThis method works like the method version taking a Callable as parameter, except a Runnable cannot return a value, so the ScheduledFuture.get() method returns null when the task is finished.\n\n#### scheduleAtFixedRate(Runnable task, long initialDelay, long period, TimeUnit timeunit)\n\nThis method schedules a task to be executed **periodically**. The task is executed the first time after the **initialDelay**, and then recurringly every time the period expires.\n\nIf any execution of the given task throws an exception, the task is no longer executed. If no exceptions are thrown, the task will continue to be executed until the **ScheduledExecutorService is shut down**.\n\nIf a task takes longer to execute than the period between its scheduled executions, the next execution will start after the current execution finishes. The scheduled task will not be executed by more than one thread at a time.\n\n#### scheduleWithFixedDelay(Runnable task, long initialDelay, long period, TimeUnit timeunit)\n\nThis method works very much like scheduleAtFixedRate() except that the period is interpreted differently.\n\nIn the scheduleAtFixedRate() method the period is interpreted as a delay between the start of the previous execution, until the start of the next execution.\n\nIn this method, however, the period is interpreted as the delay between the end of the previous execution, until the start of the next. The delay is thus between finished executions, not between the beginning of executions.\n\n## Reference\n\n![Java Concurrent](java-concurrent.png)\n* [Java Concurrency and Multithreading Tutorial](http://tutorials.jenkov.com/java-concurrency/index.html)\n* [Java Concurrency Utilities](http://tutorials.jenkov.com/java-util-concurrent)\n* [Overview of the java.util.concurrent](https://www.baeldung.com/java-util-concurrent)\n* [Parallel Programming in Java](https://www.coursera.org/learn/parallel-programming-in-java)",
        "slug": "Java-Concurrent-Programming",
        "date": "2021-10-06 23:09:55",
        "lang": "en",
        "tags": [
          "Java",
          "Concurrent Programming",
          "Parallelism",
          "Java Stream"
        ],
        "path": "/Java-Concurrent-Programming/index.html"
      },
      {
        "title": "Client-Centric Consistency Models",
        "markdownContentSource": "\n## Eventual consistency\n\nThere is a special class of distributed data stores that characterized by the **lack of simultaneous updates**, most of the operations involve reading data. These data stores offers a very **weak consistency model**, called **eventual consistency**. Eventually consistency means in the absence of updates all replicas converge toward identical copies. Eventual consistency essentially requires only that updates are guaranteed to propagate to all replicas. It is often acceptable to propagate an update after some time, and then the question is how fast updates should be made available to read-only processes. Concurrent update are often relatively easy to solve when **assuming that only a small group of processes can perform updates**. Eventual consistency is therefore often **cheap to implement**.\n\n## Client-centric consistency models\n\nEventual consistent data stores work fine **as long as clients always access the same replica**. However, problems arise when different replicas are accessed over a short period of time. Assume the user performs several update operations and then disconnects again. Later, he accesses the database again, possibly after moving to a different location or by using a different access device. At that point, the user may be connected to a different replica than before. If the updates performed previously have not yet been propagated, the user will notice inconsistent behavior. In particular, he would expect to see all previously made changes, but instead, it appears as if nothing at all has happened.\n\nBy introducing special client-centric consistency models, the previous problem can be alleviated in a relatively cheap way. In essence, client-centric consistency provides guarantees for a **single client**.\n\nTo explain these models, we consider a data store that is physically distributed across multiple machines. When a process accesses the data store, it generally connects to the nearest available copy, although any copy will be possible. All read and write operations are performed on that local copy. Updates are eventually propagated to the other copies. To simplify matters, we assume that data items have an associated owner, which is the only process that is permitted to modify that item. In this way, we avoid write-write conflicts.\n\n### Notation\n\nLet $x_i[t]$ denote the version of data item x at local copy $L_i$ at time $t$. $x_i[t]$ is the result of a series of write operations denoted as $WS(x_i[t])$ at $L_i$ that took place since initialization until time $t$. If the same operations have been replicated at local copy $L_j$ at a later time $t_2$, we write $WS(x_i[t_i];x_j[t_2])$. If the ordering of operations or the timing is clear from the context, the time index will be omitted.\n\n### Monotonic Reads\n\nIn a **monotonic-read consistent** data store, if a process reads the value of a data item x, any successive read operation on x by that process will always return that same value or a more recent value.\n\nAs an example where monotonic reads are useful, consider a distributed email database, when a user open mailbox in San Francisco and notice a new email is present. When the user later flies to New York and opens his mailbox again, monotonic-read consistency guarantees that the messages that were in the mailbox in San Francisco will also be in the mailbox when it is opened in New York.\n\n![Monotonic reads examples](monotonic-reads.png)\n\nIn the figure (a), process $P$ first performs a read operation on $x$ at $L_1$ shown as $R(x_1)$, returning the value of $x_1$. This value results from the write operations $WS(x_1)$ performed at $L_1$. Later, $P$ performs a read operation on $x$ at $L_2$, shown as $R(x_2)$. To guarantee monotonic-read consistency, all operations in $W(x_1)$ should have been propagated to $L_2$ before the second read operation takes place. In other words, we need to know for sure that $WS(x_1)$ is part of $WS(x_2)$, which is expressed as $WS(x_1;x_2)$.\n\nIn the figure (b), shows a situation in which monotonic-read consistency is not guaranteed. After process $P$ has read $x_1$ at $L_1$. it later performs the operation $R(x_2)$ at $L_2$. However, only the write operations $WS(x_2)$ have been performed at $L_2$. No guarantees are given that this set also contains all operations contained in $WS(x_1)$.\n\n### Monotonic Writes\n\nIn a **monotonic-write consistent** data store, a write operation by a process on a data item $x$ is completed before any successive write operation on x by the same process. In other words, no matter where that operation was initialized, for each node the new write of same process must wait for old write propagate to local copy.\n\n![Monotonic writes example](monotonic-writes.png)\n\nIn the figure (a), a process $P$ performs a write operation on $x$ at local copy $L_1$. Later, $P$ performs another write operation on $x$, but this time at $L_2$. To ensure monotonic-write consistency, it is necessary that the previous write operation at $L_1$ has already been propagated to $L_1$. This is why we have $W(x_1)$ at $L_2$ and before $W(x_2)$.\n\nIn the figure (b), in which monotonic-write consistency is not guaranteed. What is missing is the propagation of $W(x_1)$ to copy $L_2$. By the definition of monotonic-write consistency, write operations by the same process are performed in the same order as they are initialized.\n\n### Read Your Writes\n\nIn a **read-your-writes consistent** data store, the effect of a write operation by a process on data item $x$ will always be seen by a successive read operation on $x$ by the same process. In other words, a write operation is always completed before a successive read operation by the same process, no matter where that read operation takes place.\n\nRead your writes consistency is useful when updating web documents and subsequently viewing the effects.\n\n![Read your writes](read-your-writes.png)\n\nIn the figure (a), process $P$ performed a write operation $W(x_1)$ and later a read operation at a different local copy. Read-your-writes consistency guarantees that the effects of the write operation can be seen by the succeeding read operation. This is expressed by $W(x_1;x_2)$, which states that $W(x_1)$ is part of $WS(x_2)$\n\nIn the figure (b) $W(x_1)$ has been left out of $WS(x_2)$, meaning that the effects of the previous write operation by process $P$ have not been propagated to $L_2$.\n\n### Writes Follow Reads\n\nIn a **writes-follow-reads consistent** data store, a write operation by a process on a data item $x$ following a previous read operation on $x$ by the same process is guaranteed to take place on the same or a more recent value of $x$ that was read. In other words, any successive write opeartion by a process on a data item will be performed on a copy of $x$ that is up to date with the value most recently read by that process.\n\nWrites-follow-reads consistency can be used to guarantee that user can only post a reaction to an article only after they have seen the original article.\n\n![Writes follow reads](writes-follow-reads.png)\n\nIn the figure (a), a process reads $x$ at local copy $L_1$ and the value is also propagates to $L_2$, where the same process later performs a write operation.\n\nIn the figure (b), no guarantees are given that the operation performed at $L_2$, because the write operation is performed on a copy that doesn't receive the update that is on the $L_1$.\n\n### Reference\n\n* Distributed Systems: Principles and Paradigms 2nd edition, pag.288-295\n",
        "slug": "Client-Centric-Consistency-Models",
        "date": "2021-07-03 15:06:02",
        "lang": "en",
        "tags": [
          "Client-Centric Consistency Models",
          "Monotonic Reads",
          "Monotonic Writes",
          "Read Your Writes",
          "Writes Follow Reads"
        ],
        "path": "/Client-Centric-Consistency-Models/index.html"
      },
      {
        "title": "STP",
        "markdownContentSource": "\n## Motivation\n\nThe Spanning Tree Protocol (STP) is a layer 2 protocol that runs on bridges and switches. The main purpose of STP is to ensure that you can build a loop-free networks when you have redundant links. Redundant links are important to provide fault tolerance if an active link fails. However, if redundant links are used to connect switches, it creates a switching loop resulting in [broadcast storm](https://en.wikipedia.org/wiki/Broadcast_storm) and MAC table instability.\n\n## Introduction\n\nAs the name suggests, STP creates a spanning tree within a network connected by layer-2 bridges, those links that are part of spanning tree are set up as preferred links. These preferred links is used for transmission unless it fails. STP also forces redundant links into a standby state. The configuration leaves a single active path between any two network nodes. If a link in the forwarding state becomes unavailable, STP reconfigures the network and reroutes data through the activation of the appropriate standby redundant link.\n\nIn following network, a redundant link is created between Switch A and Switch B. However, this setup creates the possibility of a bridging loop. For example, a broadcast or multicast packet that transmits from Station M and is destined for Station N simply continues to circulate between both switches.\n\n![Backup link](backup-link.png)\n\nHowever, when STP runs on both switches, the network logically looks like this:\n\n![Backup link when STP is enabled](backup-link-stp.png)\n\n## Implementation\n\nSTP designates one layer-2 switch as **root bridge** that becomes the focal point in the network. All other decisions in the network, such as which port to block and which port to put in forwarding mode, are made from the perspective of this **root bridge**.\n\nBefore you configure STP, you select a switch to be the **root bridge**. This switch does not need to be the most powerful switch, but choose the most centralized switch on the network. The **backbone switches** is often chosen because these switches typically do not connect to end stations, also, moves and changes within the network are less likely to affect these switches.\n\nOnce **root bridge** is selected, the switches comply to these rules:\n\n1. All ports of the root switch must be in forwarding mode.\n2. Each switch determines the best path to get to the root. The switches determine this path by a comparison of the information in all the [BPDUs](https://en.wikipedia.org/wiki/Bridge_Protocol_Data_Unit) that the switches receive on all ports. The port that receives least number of information in the [BPDUs](https://en.wikipedia.org/wiki/Bridge_Protocol_Data_Unit) is the root port and set to forwarding to mode.\n3. All the other ports of all the switches must be placed in blocking mode. The rule only applies to ports that connect to other bridges or switches. STP does not affect ports that connect to workstations or PCs. These ports remain forwarded.\n4. STP calculates the path cos based on the bandwidth of the links between switches. The port with lowest path cost to the root bridge becomes the root port.\n\n## Reference\n\n* [Understanding and Configuring Spanning Tree Protocol (STP) on Catalyst Switches](https://www.cisco.com/c/en/us/support/docs/lan-switching/spanning-tree-protocol/5234-5.html)\n* [Wiki: Spanning Tree Protocol](https://en.wikipedia.org/wiki/Spanning_Tree_Protocol)",
        "slug": "STP",
        "date": "2021-05-15 19:05:00",
        "lang": "en",
        "tags": ["Spanning Tree Protocol", "STP"],
        "path": "/STP/index.html"
      },
      {
        "title": "Testing",
        "markdownContentSource": "\n# Testing\n\n## Unit Test\n\nThe purpose of unit test is to validate that each unit of the software code performs as expected. A unit may be an individual function, method, procedure, module, or object. Unit testing is done during the development of an application by the developers. If proper unit testing is done in early development, then it saves time and money in the maintenance phase.\n\n![Unit Test](unit-testing.png)\n\n## Integration Test\n\nIntegration tests determine if independently developed units of software work correctly when they are connected to each other. It was performed by activating many modules and running higher level tests against all of them to ensure they operated together.\n\n![Integration Test](integration-test.png)\n\n## Contract Test\n\nOne of the most common cases of using a test double is when you are communicating with an external service. Typically, such services are being maintained by a different team, they may be subject to slow, and unreliable networks, and maybe unreliable themselves. The test double is indeed not an accurate representation of the external service, e.g. what happens if the external service changes its contract?\n\n![Contract Test](contract-test.png)\n\nA contract change may break a production application, triggering an emergency fix and an urgent conversation with the supplier team. A good way to deal with this is to continue to run your own tests against the double, but in addition to periodically run a separate set of contract tests. These check that all the calls against your test doubles return the same results as a call to the external service would. A failure in any of these contract tests implies you need to update your test doubles, and probably your code to take into account the service contract change.\n\n## End-to-End Test\n\nE2E tests are smoke test with a very limited range of paths tested that validates entire software from starting to the end along with its integration with external services.  \n\n## Load Test\n\nLoad Test is a non-functional software test in which the performance of software application is tested under a specific expected load. It determines how the software application behaves while being accessed by multiple users simultaneously. The goal of Load Testing is to improve performance bottlenecks and to ensure stability and smooth functioning of software application before deployment.\n\n## Snapshot Test\n\nSnapshot tests ensure your serializable output (e.g. React tree) does not change unexpectedly. On test runs, we will compare the output with the previous snapshot, the test will fail if they do not match: either the change is unexpected, or the snapshot needs to be updated because of the new version of the code.\n\nSnapshots identify unexpected interface changes within your application – whether that interface is an API response, UI, logs, or error messages.\n\n* False positive/false negative result in CI: The snapshot should be committed alongside code changes, so CI will run the test with update to date snapshots, and it should be reviewed as part of your code review process. It can also provide a lot of additional context during code review in which reviewers can study your changes better.\n* Snapshots take a lot of time to review: We should ensure our snapshots are readable by keeping them focused, short.\n* Snapshot tests fail unexpectedly: Tests should be deterministic, running the same tests multiple times on a component that has not changed should produce the same results every time.\n* Meaning of each snapshot: Give descriptive names for each snapshot. This makes it easier for reviewers to verify the snapshots during the code review.\n* Lot of unit tests: The aim of snapshot testing is not to replace existing unit tests, but in some scenarios, snapshot testing can potentially remove the need for unit testing for a particular set of functionalities (e.g. rendering of React components).\n\n## Reference\n\n* [IntegrationTest](https://martinfowler.com/bliki/IntegrationTest.html)\n* [ContractTest](https://martinfowler.com/bliki/ContractTest.html)\n* [Unit Testing Tutorial: What is, Types, Tools & Test EXAMPLE](https://www.guru99.com/unit-testing-guide.html)\n* [END-To-END Testing Tutorial: What is E2E Testing with Example](https://www.guru99.com/end-to-end-testing.html)\n* [Load Testing Tutorial: What is? How to? (with Examples)](https://www.guru99.com/load-testing-tutorial.html)\n* [Snapshot Testing](https://jestjs.io/docs/snapshot-testing)",
        "slug": "Testing",
        "date": "2021-04-24 11:27:04",
        "lang": "en",
        "tags": [
          "Integration Test",
          "Contract Test",
          "End-to-End Test",
          "Unit Test",
          "Load Test",
          "Snapshot Test"
        ],
        "path": "/Testing/index.html"
      },
      {
        "title": "Authenticated Encryption",
        "markdownContentSource": "\n## Glossary\n\n### Cryptographic Hash Function\n\nIt is a **one-way function** that maps data of arbitrary size (often called the **message**) to a bit array of a fixed size (the **hash value**, **hash**, or **message digest**). Ideally, it should be infeasible to invert and the only way to find a message that produces a given hash is to attempt a brute-force search of possible inputs to see if they produce a match.\n\n### Salt\n\nSalt is random data that is used as an additional input to a one-way function. Salts defend against attacks that use precomputed tables (e.g. rainbow table) as they can make the size of table needed for a successful attack extremely large.\n\n#### Reuse Salt\n\nReusing the same salt for numerous inputs is dangerous because it allows precomputed table which simply apply same salt to all the items in the brute force dictionary.\n\n#### Short Salt\n\nIf a salt is too short, an attacker may precompute a table of all combinations of every possible salts and every likely plaintext.\n\n### Message Authentication Code\n\nMAC is cryptographic checksum that are used to detect when an attacker has tampered with a message, so we can confirm that the message come from the trusted sender(ourselves). The MAC value protects a message integrity as well as its authenticity.\n\n### Comparison\n\n| Security Goal   | Hash | MAC       | Digital Signature |\n|-----------------|------|-----------|-------------------|\n| Integrity       | Yes  | Yes       | Yes               |\n| Authentication  | No   | Yes       | Yes               |\n| Non-Repudiation | No   | No        | Yes               |\n| Keys            | N/A  | Symmetric | Asymmetric        |\n\n## Example Application: Tamper-Proof Query Parameter\n\nSince URLs can easily be changed by even the most novice user, you need to validate to ensure that the user has not modified the query parameter to an unacceptable state.\n\nThe standard approach is a proper authentication and authorization control system in the backend or avoid passing such critical information through query parameter by using data store or external service.\n\nHowever, there are time when important state needs to be passed through the query parameter and, under no circumstances, should be it be able to be modified by the end user. In order to preserve the integrity of query parameter you will have to apply some security mechanisms.\n\n### Approach 1: Cryptographic Hash Function\n\nYou can use a cryptographic hash function to sign the value of query parameter that you do not want to be edited by user and append this signature to the query parameter. We can validate the value of query parameter by applying the same hash function to ensure that it matches to the signature we have included previously.\n\nHowever, this approach presents a problem, even the hash function is private to us, user might end up realizing the hash function we are using, e.g. sha256. Then the user can forge the value of query parameter and update the signature manually.\n\n### Approach 2: Cryptographic Hash Function With Salt\n\nTo mitigate user generating signature of query parameter, we can add the additional input (salt) before hashing. However, this approach still present certain problems, e.g. vulnerable to reply attack, authenticated users could cut and paste valid query parameter value to other users or cached by search engine, to prevent this from happening we need to create our own protocol, e.g. ensure the salt value is bind to each user e.g. session id, or make it expire after a period of time e.g. timestamp. However, most of the time salt value is not meant to be completely invisible to user, e.g. we can often retrieve session id from the browser cookie or other header and saving salt for each user might be expensive since the length of salt should be long enough and different for each user to prevent precomputed dictionary attack.\n\n### Approach 3: Message Authentication Code\n\nWe can use MAC to provide better data integrity and authenticity, MAC resist signature forgery under chosen-plaintext attacks while cryptographic hash functions doesn't. However, MAC doesn't provide confidentiality of query parameter value, it is still visible to the user, same problem with reply attack and doesn't provide confidentiality of query parameter value.\n\n### Approach 4: Authenticated Encryption\n\nIn addition to the benefits of previous approach it provides confidentiality of message. Authenticated encryption schemes can recognize improperly-constructed ciphertexts and refuse to decrypt them.\n\n#### Approach 4.1: Encrypt-then-MAC\n\n![EtM](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Authenticated_Encryption_EtM.png/220px-Authenticated_Encryption_EtM.png)\n\n## Reference\n\n* [Passing Tamper-Proof Query Parameters](http://aspnet.4guysfromrolla.com/articles/083105-1.aspx)\n* [Wikipedia: Message Authentication Code](https://en.wikipedia.org/wiki/Message_authentication_code)\n* [Wikipedia: HMAC](https://en.wikipedia.org/wiki/HMAC)\n* [Wikipedia: Block_cipher_mode_of_operation](https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation)\n* [Wikipedia: Authenticated Encryption](https://en.wikipedia.org/wiki/Authenticated_encryption)\n* [Wikipedia: Salt](https://en.wikipedia.org/wiki/Salt_(cryptography))\n* [Wikipedia: Rainbow table](https://en.wikipedia.org/wiki/Rainbow_table)\n* [Stack Exchange: Difference between salt and MAC](https://crypto.stackexchange.com/a/10758)\n* [Wikipedia: Digital Signature Forgery](https://en.wikipedia.org/wiki/Digital_signature_forgery)\n* [Wikipedia: Chosen-Plaintext Attack](https://en.wikipedia.org/wiki/Chosen-plaintext_attack)\n* [Stack Exchange: What are the differences between a digital signature, a MAC and a hash?\n](https://crypto.stackexchange.com/a/5647)\n* [Stack Exchange: What is the difference between a mac and a digital signiture\n](https://security.stackexchange.com/a/32134)\n* [RFC: An Interface and Algorithms for Authenticated Encryption](https://tools.ietf.org/html/rfc5116)\n* [Royal Holloway University: Authenticated Encryption](https://www.cosic.esat.kuleuven.be/school-iot/slides/AuthenticatedEncryptionII.pdf)",
        "slug": "Authenticated-Encryption",
        "date": "2021-03-28 11:21:16",
        "lang": "en",
        "tags": [
          "Mac",
          "HMAC",
          "Message Authentication Code",
          "AE",
          "Authenticated Encryption",
          "Salt"
        ],
        "path": "/Authenticated-Encryption/index.html"
      },
      {
        "title": "Information Security",
        "markdownContentSource": "\n# Information Security\n\nInformation security(infosec), is the practice of protecting information by mitigating information risks like\n\n* unauthorized/inappropriate access to data\n* unlawful use\n* disclosure\n* disruption\n* deletion\n* corruption\n* modification\n* inspection\n* recording\n* devaluation of information\n\nInformation security's primary focus is efficient implementation of the CIA triad without hampering organization productivity.\n\n## CIA\n\n### Confidentiality\n\nThe information is disclosed to unauthorized individuals, entities, or processes.\n\n### Integrity\n\nThe data cannot be modified in an unauthorized or undetected manner.\n\n### Availability\n\nThe information must be available when it is needed, high availability system must prevent service disruptions due to power outages, hardware failures, and system upgrades.\n\n### Non-Repudiation\n\nIt implies that author of the action can not later deny having performed the action, e.g. send the message.\n\n## AAA\n\nAAA refers to a common security framework for mediating network and application access.\n\n### Authentication\n\nThe act of verifying a claim of identity.\n\n### Authorization\n\nOnce authentication process is completed, the authorization process will determine what informational resources they are permitted to use.\n\n### Accounting\n\nAccounting measures the different aspects of usage of informational resources that users consume during their activity. Usage information is used for authorization control, billing, trend analysis, resource utilization, and capacity planning activities.\n\n## Reference\n\n* [Wikipedia: Information security](https://en.wikipedia.org/wiki/Information_security)\n* [Wikipedia: AAA](https://en.wikipedia.org/wiki/AAA_(computer_security))",
        "slug": "Information-Security",
        "date": "2021-03-28 11:11:17",
        "lang": "en",
        "tags": [
          "InfoSec",
          "CIA",
          "AAA",
          "Information Security",
          "Confidentiality",
          "Integrity",
          "Availability",
          "Non-Repudiation",
          "Authentication",
          "Authorization",
          "Accounting"
        ],
        "path": "/Information-Security/index.html"
      },
      {
        "title": "Minimalist Linux Workspace",
        "markdownContentSource": "\n## Download Image\n\nDownload [Debian image](https://cdimage.debian.org/debian-cd/current/amd64/iso-cd/), let we choose the net-install ISO since we don't need any heavy desktop enviroment e.g. KDE, GNOME, etc... .\n\n## Create Boot Device\n\nThere are a ton of tools which can burn ISO to a USB or CD, e.g. [Rufus](https://rufus.ie/), simply follow the instruction and create the boot device.\n\n## Installation\n\nPlug the USB into your computer reboot and following the instruction to complete the installation. Remember to use a different partition for **/home** which is helpful for reinstallation in the future.\n\n## Update Sources List\n\n* https://wiki.debian.org/SourcesList\n* https://backports.debian.org/Instructions/\n\n## Configure Openbox\n\n    sudo apt install xinit openbox menu xfce4-terminal\n    ehoc \"exec openbox-session\" >> ~/.xinitrc\n    echo \"startx\" >> ~/.bash_profile\n\n## Install Asian Fonts\n\n    sudo apt install fonts-arphic-ukai fonts-arphic-uming fonts-ipafont-mincho fonts-ipafont-gothic fonts-unfonts-core\n    sudo fc-cache -fv\n\n## Enable Sound\n\n    sudo install alsa-utils\n    amixer set Master 100% unmute\n\n## Configure Dev Tools\n\n* https://wiki.debian.org/Java\n* https://wiki.debian.org/VisualStudioCode\n* https://wiki.debian.org/Chromium\n\n## References\n\n* https://wiki.debian.org/Openbox\n* https://unix.stackexchange.com/a/149368",
        "slug": "Minimalist-Linux-Workspace",
        "date": "2021-01-31 18:25:16",
        "lang": "en",
        "tags": ["Linux", "Debian"],
        "path": "/Minimalist-Linux-Workspace/index.html"
      },
      {
        "title": "Testable Design",
        "markdownContentSource": "\n# Guidelines for Testable Design\n\n## Testability issues\n\nA programmer struggling to write a test is facing off against one of a few common show-stoppers. They mostly fall under two categories: restricted access, and inability to substitute certain parts of the implementation.\n\n### Can’t instantiate a class\n\nThis mostly happens with third-party libraries that weren’t designed with testability in mind. Another common cause for the inability to instantiate a class is that constructor depends on some static initialization.\n\n### Can’t substitute a collaborator\n\nIf the method is supposed to engage in an interaction with collaborating objects, but you find yourself incapable of intercepting the interaction you’re interested in. This might be because the collaborator is hard-wired into the method/class you’re testing and can’t be substituted with a test double.\n\n## Avoid complex private methods\n\nYou should strive to write private method so that you don’t feel the need to test your private methods directly, make your public methods read well, it should be perfectly fine that they get tested\nonly through those public methods.\n\n## Avoid final methods\n\nThe main purpose of marking a method as final is to ensure that it isn’t overwritten by a subclass, but often burden of worse testability outweigh the benefit of having the final.\n\n## Avoid static methods\n\nA static method is easy to write, but you’ll have a hard time later if you ever need to stub it out in a test. Instead, create an object that provides that functionality through an instance method.\n\n## Use new with care\n\nEvery time you “new up” an object you’re nailing down its exact implementation. For that reason, your methods should only instantiate objects you won’t want to substitute with test doubles, it should be passed into the method somehow rather than instantiated from within that method.\n\n## Avoid logic in constructors\n\nConstructors are hard to bypass because a subclass’s constructor will always trigger at least one of the superclass constructors. Hence, you should avoid any test-critical code or logic in your constructors.\n\n## Avoid the Singleton\n\nThere are situations where you want just one instance of a class to exist at runtime. But the Singleton pattern tends to prevent tests from creating different variants. If you do find yourself with a static Singleton accessor, consider making the singleton **getInstance()** method return an interface instead of the concrete class. An interface is much easier to fake. The much testable design would be that doesn’t enforce a single instance but rather relies on the programmers’ agreement or modern dependency injection framework.\n\n## Favor composition over inheritance\n\nInheritance does allow you to reuse code but it also brings a rigid class hierarchy that inhibits testability. If your intent is to reuse functionality it’s often better to do that by means of composition: using another object rather than inheriting from its class.\n\n## Wrap external libraries\n\nWrap the untestable piece of code of the external library behind your own interface, a thin layer that is testable that's test-friendly and makes it easy to substitute the actual implementation.\n\n## Reference\n\n* [Effective Unit Testing](https://www.manning.com/books/effective-unit-testing)",
        "slug": "Testable-Design",
        "date": "2021-01-24 20:30:40",
        "lang": "en",
        "tags": ["Testable Design"],
        "path": "/Testable-Design/index.html"
      },
      {
        "title": "SOLID Design Principles",
        "markdownContentSource": "\n# SOLID Design Principles\n\nIn object-oriented computer programming, SOLID is acronym and stands for a set of five design principles documented by Robert C. Martin. Keep your code in line with SOLID make software designs more clear, and you're much more likely to end up with a modular flexible and maintainable. SOLID also mesh well with testability.\n\n## Single Responsibility Principle\n\nIt states that there should never be more than one\nreason for a class to change. Every module, class, or function should have responsibility over a single part of the functionality provided by the software, and will have only one reason for modification. Writing test for code that follows the SRP is essentially expressing our understanding of the problem and specifying the expected behavior. We can test the class independently. When we add some more functionality, the old tests shouldn't fail.\n\n## Open–Closed Principle\n\nThe open/closed principle states software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification. This can be achieved by abstraction, polymorphism and several design patterns e.g. strategy, composite, decorator, factory method. Classes that delegate specific responsibilities to other objects allow tests to substitute a test double when needed to simulate a specific scenario.\n\n![Strategy Pattern](strategy.png)\n\n## Liskov Substitution Principle\n\nIt states that if S is a subtype of T, then objects of type T may be replaced with objects of type S without altering any of the desirable properties of the program. LSP emphasize the hierarchies should exist for the right reason, embodying a valid abstraction. Class hierarchies that follow the LSP enable the use of contract tests, tests written for parent class/interface can be executed against all derived/implementation.\n\n![Liskov Substitution](liskov-substitution.png)\n\n## Interface Segregation Principle\n\nIt states that no client should be forced to depend on methods it does not use, we should split interfaces that are very large into smaller and more specific ones so that clients will only have to know about the methods that are of interest to them. Small interfaces keep test simple for each interface, it also improve testability by making it easier to write and use test doubles.\n\n## Dependency inversion principle\n\nThe dependency inversion principle says code should depend on abstractions not on concretions, it suggests that a class shouldn't instantiate its own collaborators but rather have their interfaces passed in. The dependency inversion principle is a specific form of decoupling software modules. When following this principle, the conventional dependency relationships from high-level module to low-level module are reversed, thus rendering high-level modules independent of the low-level module implementation details. The principle states:\n\n1. High-level modules should not depend on low-level modules. Both should depend on abstractions(interfaces).\n2. Abstractions should not depend on details. Details (classes) should depend on abstractions.\n\nThe both high-level and low-level objects must depend on the same abstraction. The interaction between a high level module and a low-level one, the interaction should be thought of as an abstract interaction between them.\n\nThe high-level class defines its own adapter interface which is the abstraction that the other high-level classes depend on. The adaptee implementation also depends on the adapter interface abstraction while it can be implemented by using code from within its own low-level module. The high-level has no dependency on the low-level module since it only uses the low-level indirectly through the adapter interface by invoking polymorphic methods to the interface which are implemented by the adaptee and its low-level module, for example **plugin**.\n\nFor writing tests. the ability to pass collaborators in make possible the injection of test double fairly easy.",
        "slug": "SOLID",
        "date": "2021-01-10 17:14:33",
        "lang": "en",
        "tags": [
          "SOLID",
          "Object Oriented Programming",
          "Object Oriented Design",
          "Agile Software Development",
          "Software Evolution"
        ],
        "path": "/SOLID/index.html"
      },
      {
        "title": "Test Case Design Techniques",
        "markdownContentSource": "\n| Approach/Implementation | White Box                 | Black Box         |   |   |\n|-------------------------|---------------------------|-------------------|---|---|\n| Structural              | Control-flow<br>Data-flow |                   |   |   |\n| Fault-based             | Mutation                  |                   |   |   |\n| Error-based             |                           | Functional Random |   |   |\n\n## Control-flow Techniques\n\n* Generate test cases taking into account the program control structure.\n* Based on the concept of **coverage**.\n  * Statement coverage\n  * Branch coverage\n  * Condition coverage\n  * Desision/condition coverage\n  * Multiple condition coverage\n\n### Example\n\n    IF (A OR B) THEN\n        print C\n\n### Statement Coverage\n\nRequire each statement in the program been executed.\n\n* (A OR B) = TRUE\n\ne.g.\n\n1. T1: A=TRUE, B=FALSE\n\n### Branch Coverage\n\nRequire each breanch of each control structure been executed.\n\n* (A OR B) = TRUE\n* (A OR B) = FALSE\n\ne.g.\n\n1. T1: A=TRUE, B=FALSE\n2. T2: A=FALSE, B=FALSE\n\n### Conditiona Coverage\n\nRequire each boolean sub-expression evaluated both to true and false.\n\n* A = TRUE\n* A = FALSE\n* B = TRUE\n* B = FALSE\n\ne.g.\n\n1. T1: A=TRUE, B=FALSE\n2. T2: A=FALSE, B=TRUE\n\n### Decision/Condition Coverage\n\nRequires that both decision and condition coverage been satisfied.\n\n* A = TRUE\n* A = FALSE\n* B = TRUE\n* B = FALSE\n* (A OR B) = TRUE\n* (A OR B) = FAKSE\n\ne.g.\n\n1. T1: A=TRUE, B=FALSE\n2. T2: A=FALSE, B=TRUE\n3. T3: A=FALSE, B=FALSE\n\n### Multiple Condition Coverage\n\nRequire all the combinations of boolean sub-expressions should be evaluated\n\n1. A=TRUE, B=FALSE\n2. A=FALSE, B=TRUE\n3. A=FALSE, B=FALSE\n4. A=TRUE,B=TRUE\n\n### Basic Path Testing\n\nAnalyze the sequences of statements from the program input to output - **paths** of a program and drwa the control flow graph.\n\n#### Flowchart\n\n* Nodes: represent zero, one or more statements.\n* Edges: link two nodes\n* Regions: areas delimited by edges and nodes\n* Predicate nodes: bifurcation node\n\n![Flowchart](flowchart.png)\n\n#### Cyclomatic Complexity\n\n* C(G) represents complexity\n  * C(G) = Number of regions\n  * C(G) = Edges - Nodes + 2\n  * C(G) = Number of predicate nodes + 1\n\n#### Determine Basic Set of Independent Paths - Equivalent to Decision/Consition Coverage\n\n![Example](example-independent-paths.png)\n\n1. 1 - 2 - 4 - 5 - 8 - 10\n2. 1 - 2 - 3 - 2 - 4 - 5 - 8 - 10\n3. 1 - 2 - 4 - 6 - 8 - 10\n4. 1 - 2 - 4 - 6 - 7 - 8 - 10\n5. 1 - 2 - 4 - 5 - 8 - 9 - 10\n6. 1 - 2 - 4 - 5 - 8 - 9 - 2 - 4 - 5 - 8 - 10\n\n#### Modern Practice\n\n* Check coverage using modern IDE\n\n## Functional Techniques\n\n* Program is viewed as a black box\n* They try to run test cases related to possible program faults\n\n### Equivalence Partitioning\n\n* Divides the program input domain into data classes used to derive test cases.\n* An equivalence class represents a set of valid or invalid states for program input conditions.\n* Each input condition is examined and divided into two or more groups, identifying two types of classes\n* An input condition typically refers to the set of values a given input can take\n* Two types of equivalence classes are defined:\n  * Valid equivalence classes: Represent valid program inputs.\n  * Invalid equivalence classes: Represent incorrect input values.\n\n![Equivalence Partitioning](equivalence-partitioning.png)\n\n#### Value Range\n\n* One valid and two invalid classes (above, below)\n* E.g.: counter range is from 1 to 999\n  * Valid class: 1 <= counter <= 999\n  * Invalid class: counter < 1\n  * Invalid class: a counter > 999\n\n#### Number of Values\n\n* One valid and two invalid classes\n* E.g.: a car can have from one to six owners in its lifetime\n  * Valid class: “from one to six owners”\n  * Invalid class: “there are no owners”\n  * Invalid class: “over six owners\n\n#### Logical/Boolean Situation\n\n* One of each type\n* Sometimes a logical condition cannot be reduced to yes/no\n* E.g: the first character of the identifier should be a letter\n  * Valid class: “identifier starting with letter” \n  * Invalid class: identifier not starting with letter\n    * starting with a number\n    * starting with a symbol\n\n##### Set of input values\n\n* If there are reasons to believe that the program will treat each group member differently, identify a valid class for each member, and an invalid class\n  * E.g. vehicle type is: bus, truck, car or motorbike\n    * 4 Valid classes: “bus”, “truck”, “car”, “motorbike”\n    * Invalid class: any other vehicle type, e.g., “cycle”, “mobile house”\n\n##### Other\n\n* Generally, if there is any reason for believing that the program does not treat the items of an equivalence class equally\nE.g.: some phone prefixes are not admitted\n* Divide the equivalence class into smaller equivalence classes for each item type\n\n#### Equivalence Partitioning's Example\n\nBanking application, where the user can perform some bank transactions. The information for accessing the transactions requires the following input:\n\n##### Bank Code\n\nBlank or three-digit number. The first digit of the three-digit number should be greater than 1.\n\n* Valid Equivalence Class\n  * Blank\n  * 3-digit number ranged from 200 <= Bank code <= 999\n\n##### Branch Code\n\nA four-digit number, the first of which is greater than 0.\n\n* Valid Equivalence Class\n  * 4-digit number, 1000 <= Branch code <=9999 \n* Invalid Equivalence Class\n  * Non-numerical value\n  * Number of more than 4 digits\n  * Number of less than 4 digits\n  * 4-digit number, Branch code < 1000\n\n##### Account Number\n\nFive-digit number.\n\n* Valid Equivalence Class\n  * Any five-digit number\n* Invalid Equivalence Class\n  * Non-numerical value\n  * Number of more than five digits\n  * Number of less than five digits\n\n##### Password\n\nFive-position alphanumerical value.\n\n* Valid Equivalence Class\n  * Any five-position alphanumerical character string\n* Invalid Equivalence Class\n  * Non-alphanumerical character string\n  * String of less than five positions\n  * String of more than five positions\n\n##### Order\n\nEither blank or one of the following two strings: “Checkbook” or “Transactions”\n\n* Valid Equivalence Class\n  * \"Checkbook\"\n  * \"Transactions\"\n* Invalid Equivalence Class\n  * Any string that is not the empty or the same as the valid strings\n\n##### Generate Test Cases\n\n* Write a case to cover as many uncovered valid classes as possible until the test cases cover all the valid equivalence classes\n* Write a case to cover just one uncovered invalid classes until the test cases cover all the invalid equivalence classes\n\n### Boundary Value Analysis\n\n* Instead of selecting just any test case in the valid and invalid classes, you test the edges between equivalence partitions\n* Instead of focusing just on the input domain, it designs the test cases also taking into account the output domain\n\n![Boundary Value Analysis](boundary-value-analysis.png)\n\n#### Value Range\n\nDesign test cases for the two bounds of the range, and another two cases for situations just above and just below these bounds\n\n#### Number of Values\n\nDesign two test cases for the minimum and maximum values, as well as another two test cases for values just above the maximum and just below the minimum\n\n#### Order Set\n\nPay attention to the first and last elements of the set.\n\n#### Output\n\nApply the above rules to output data as well\n\n## Blackbox Testing Technique\n\n### Decision Table Technique\n\n* Decision table technique is widely used for black box testing. \n* This is a systematic approach where various input combinations and their respective system behavior are captured in a tabular form. \n* Decision table technique is appropriate for the functions that have a logical relationship between two and more than two inputs.\n\n| <!-- -->                        | <!-- -->          | <!-- -->          | <!-- -->          | <!-- -->          | <!-- -->          | <!-- -->          |\n|---------------------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|\n| Input 1 (Tow Possible Values)   | Possible Value 1  | Possible Value 2  | Possible Value 1  | Possible Value 2  | Possible Value 1  | Possible Value 2  |\n| Input 2 (Three Possible Values) | Possible Value 1  | Possible Value 1  | Possible Value 2  | Possible Value 2  | Possible Value 3  | Possible Value 3  |\n| Expected Result                 | Expected Result 1 | Expected Result 2 | Expected Result 3 | Expected Result 4 | Expected Result 5 | Expected Result 6 |\n\n### State Transition Technique\n\n* State transition testing is used where some aspect of the system can be described in what is called a 'finite state machine'.\n* This method tests whether the function is following state transition specifications on entering different inputs.\n* Any system where you get a different output for the same input, depending on what has happened before, is a finite state system.\n* We usually use **state diagram** to represent such system.\n* The transitions from one state to another are determined by the rules of the 'machine'.\n\n#### Transition Model\n\n* the states that the software may occupy\n* the transitions from one state to another\n* the events that cause a transition\n* the actions that result from a transition\n\n#### PIN Example\n\n![PIN Transition Diagram](pin-example-transition-diagram.png)\n![PIN Transition Table](pin-example-transition-table.png)\n\n#### Web Login Example\n\n| State | Login                  | Validation | Next State |\n|-------|------------------------|------------|------------|\n| S1    | First Attempt          | Invalid    | S2         |\n| S2    | Second Attempt         | Invalid    | S3         |\n| S3    | Third Attempt          | Valid      | S4         |\n| S4    | Redirect to Home page  |            |            |\n| S5    | Redirect to Error Page |            |            |\n\n| State | Login                  | Validation | Next State |\n|-------|------------------------|------------|------------|\n| S1    | First Attempt          | Invalid    | S2         |\n| S2    | Second Attempt         | Invalid    | S3         |\n| S3    | Third Attempt          | Invalid    | S5         |\n| S4    | Redirect to Home page  |            |            |\n| S5    | Redirect to Error Page |            |            |\n\n## Test Case Specification\n\n### Test goal\n\nHigh level description of the purpose of the test case.\n\n### Inputs\n\nActual inputs given to the program.\n\n### Expected outputs\n\nDescription of the program output according to the specification.\n\n### Observed outputs\n\nDescription of the program output when it is run\n\n## Failure Identification\n\nFailures are identified by comparing expected outputs against observed outputs.",
        "slug": "Test-Case-Design-Techniques",
        "date": "2020-11-24 12:56:14",
        "lang": "en",
        "tags": [
          "Test Design",
          "Boundary Value Analysis",
          "Equivalence Partitioning"
        ],
        "path": "/Test-Case-Design-Techniques/index.html"
      },
      {
        "title": "Build Simple Bootstrap Linux System Image",
        "markdownContentSource": "\n## Kernel Image for Emulation\n\nWe will create a tiny Linux system with absolute minimum additional software. We reuse host kernel image **vmlinuz** to simply the process.\n\n    cp /vmlinuz vmlinuz\n\n**vmlinux** is a [ELF format](https://en.wikipedia.org/wiki/Executable_and_Linkable_Format) based Linux kernel executable image and **[vmlinuz](https://s905060.gitbooks.io/site-reliability-engineer-handbook/content/anatomy_of_the_initrd_and_vmlinuz.html) (Virtual Memory Linux gZip)** is a compressed version of **vmlinux**. At the beginning of **vmlinuz** is a routine that does some minimal amount of hardware setup and then decompresses the kernel contained within the kernel image and places it into high memory. The routine then calls the kernel and the kernel boot begins.\n\n## Initramfs\n\nWe need a temporary root file system, an **initramfs** when the kernel boots up. An initramfs is a **cpio** compressed archive. Unlike the [old initrd](https://www.kernel.org/doc/html/latest/filesystems/ramfs-rootfs-initramfs.html?highlight=initramfs#ramfs-and-ramdisk), **initramfs** reuse disk cache mechanism of main memory so **initramfs** doesn't require creating a synthetic block device nor having a intermediate file-system driver inside the kernel to interpret the data which may have license problem and avoid unnecessarily copying memory from the fake block device into the page cache. Bootloader places **initramfs** image(s) in a memory location and then pass its location and its size as [parameter](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html) to kernel. The kernel checks for the presence of the **initramfs** and if found it is unpacked into a RAM disk at kernel initialization, mounts it as **/**. Once **initramfs** is extracted, into **rootfs** the kernel checks if it contains a file **/init**. If it does, the kernel executes this **/init** file.\n\n    mkdir simple\n    cd simple\n\n### Shell Environment\n\n    sudo apt install busybox-static\n\nWe have to install **[busybox-static](https://packages.debian.org/buster/busybox-static)**, **[busybox](https://packages.debian.org/buster/utils/busybox)** does not install symbolic links for any of the supported utilities.\n\n    mkdir -p bin sbin usr/bin usr/sbin\n    cp /bin/busybox bin/busybox\n    ln -s busybox bin/sh\n\n### Configure Keyboard Layout for Spanish\n\nWe have to [install the list of keymaps](https://stackoverflow.com/a/34882663/9980245)\n\n    sudo apt install kbd console-data\n\n[Convert to binary format](https://bbs.archlinux.org/viewtopic.php?id=191064) and copy to our **initramfs**\n\n    loadkeys -b /usr/share/keymaps/i386/qwerty/es.kmap.gz > es.kmap\n\n### [Support USB Devices](https://www.cyberciti.biz/faq/usb-drive-not-being-recognized-under-linux/)\n\nWe repeatedly use this command [sudo lsmod | grep usb_storage](https://askubuntu.com/a/521231) to recursively find all modules that are required for USB hot-plug. They are the [following modules](https://unix.stackexchange.com/a/299501) in total\n\n* [ohci-hcd](https://www.kernel.org/doc/html/latest/usb/ohci.html)\n* ohci-pci\n* uas\n* usb_storage\n* usbcore\n* scsi_mod\n* usb_common\n* [sd_mod](https://tldp.org/HOWTO/SCSI-2.4-HOWTO/sd.html)\n* fat\n* vfat\n* [nls_* e.g. nls_cp437, nls_ascii](https://unix.stackexchange.com/q/62086)\n\nWe then use **[sudo modinfo [module name]](https://www.cyberciti.biz/faq/linux-how-to-load-a-kernel-module-automatically-at-boot-time/)** to find each module path and copy to our **initramfs**.\n\n    mkdir -p lib/modules/$(uname -r)\n    cp /lib/modules/$(uname -r)/kernel/drivers/usb/host/ohci-hcd.ko lib/modules/$(uname -r)\n    cp /lib/modules/$(uname -r)/kernel/drivers/usb/host/ohci-pci.ko lib/modules/$(uname -r)\n    cp /lib/modules/$(uname -r)/kernel/drivers/usb/storage/uas.ko lib/modules/$(uname -r)\n    cp /lib/modules/$(uname -r)/kernel/drivers/usb/storage/usb-storage.ko lib/modules/$(uname -r)\n    cp /lib/modules/$(uname -r)/kernel/drivers/usb/core/usbcore.ko lib/modules/$(uname -r)\n    cp /lib/modules/$(uname -r)/kernel/drivers/usb/core/usbcore.ko lib/modules/$(uname -r)\n    cp /lib/modules/$(uname -r)/kernel/drivers/scsi/scsi_mod.ko lib/modules/$(uname -r)\n    cp /lib/modules/$(uname -r)/kernel/drivers/usb/common/usb-common.ko lib/modules/$(uname -r)\n    cp /lib/modules/$(uname -r)/kernel/drivers/scsi/sd_mod.ko lib/modules/$(uname -r)\n    cp /lib/modules/$(uname -r)/kernel/fs/fat/vfat.ko lib/modules/$(uname -r)\n    cp /lib/modules/$(uname -r)/kernel/fs/fat/fat.ko lib/modules/$(uname -r)\n    cp /lib/modules/$(uname -r)/kernel/fs/nls/*.ko lib/modules/$(uname -r)\n\n### Init Script\n\nWe will require a **/init** program, it is typically a shell script. It configures some basic device nodes and directories, mounts the special **/sys** and **/proc** file systems, and starts the processing of hotplug events using mdev and then execute the more-typical /sbin/init program.\n\n    nano init\n\nand put following content\n\n    #!/bin/sh\n    # install symlinks for all of the suppoted utilities\n    # install busybox symbolic links\n    /bin/busybox --install -s\n    #\n    [ -d /dev ] || mkdir -m 0755 /dev\n    [ -d /root ] || mkdir --mode=0700 /root\n    [ -d /sys ] || mkdir /sys\n    [ -d /proc ] || mkdir /proc\n    [ -d /tmp ] || mkdir /tmp\n    mkdir -p /var/lock\n    #\n    mount -t sysfs none /sys -onodev,noexec,nosuid\n    mount -t proc none /proc -onodev,noexec,nosuid\n    #\n    mknod /dev/zero c 1 5\n    mknod /dev/null c 1 3\n    mknod /dev/tty c 5 0\n    mknod /dev/console c 5 1\n    mknod /dev/ptmx c 5 2\n    mknod /dev/tty0 c 4 0\n    mknod /dev/tty1 c 4 1\n    # load USB drivers\n    depmod -a # https://stackoverflow.com/q/45658297/9980245\n    modprobe ohci-pci\n    modprobe uas\n    modprobe sd_mod\n    # initial population and dynamic updates\n    echo \"/sbin/mdev\" > /proc/sys/kernel/hotplug\n    /sbin/mdev -s\n    # change keymaps http://ilikelinux69.github.io/How-to-change-the-keyboard-layout-in-Busybox\n    loadkmap < es.kmap\n    # traditional init program\n    exec /sbin/init\n\nadd the execution permission\n\n    chmod a+x init\n\n[compress into cpio images](https://www.kernel.org/doc/html/latest/admin-guide/initrd.html#compressed-cpio-images)\n\n    find . | cpio --quiet -o -H newc | gzip >../simple.igz\n\nThe result **simple.igz** is our **initramfs**.\n\n#### Install Busybox Symbolic Links\n\nThe **busybox** provide a **--install** options to create symbolic links (**-s**) for all the supported commands in **/bin**, **/sbin**, **/usr/sbin**. Here are the list of command that **busybox** install.\n\n* ls -al **/bin** | grep \"\\->\" | less\n  * arch\n  * ash\n  * cat\n  * chgrp\n  * chmod\n  * chown\n  * cp\n  * cpio\n  * cttyhack\n  * date\n  * dd\n  * df\n  * dmesg\n  * dnsdomainname\n  * dumkmap\n  * echo\n  * ed\n  * egrep\n  * false\n  * fatattr\n  * fgrep\n  * getopt\n  * grep\n  * hunzip\n  * gzip\n  * hostname\n  * ionice\n  * ipcalc\n  * kill\n  * link\n  * linux32\n  * linux64\n  * ln\n  * login\n  * ls\n  * lzop\n  * mkdir\n  * mknod\n  * mktemp\n  * more\n  * mount\n  * mt\n  * mv\n  * netstat\n  * nuke\n  * pidof\n  * ping\n  * ping6\n  * ps\n  * pwd\n  * readlink\n  * resume\n  * rev\n  * rm\n  * rmdir\n  * rpm\n  * run-parts\n  * sed\n  * setpriv\n  * sh\n  * sleep\n  * slat\n  * stty\n  * su\n  * sync\n  * tar\n  * touch\n  * true\n  * umount\n  * uname\n  * uncompress\n  * usleep\n  * vi\n  * watch\n  * zcat\n* ls -al **/sbin** | grep \"\\->\" | less\n  * acpid\n  * adjtimex\n  * arp\n  * blockdev\n  * depmod\n  * devmem\n  * fdisk\n  * freeramdisk\n  * fstrim\n  * getty\n  * halt\n  * hwclock\n  * ifconfig\n  * ifdown\n  * ifup\n  * init\n  * insmod\n  * ip\n  * ipneigh\n  * klogd\n  * loadkmap\n  * logread\n  * losetup\n  * lsmod\n  * mdev\n  * mkdosfs\n  * mke2fs\n  * mkswap\n  * modinfo\n  * modprobe\n  * nameif\n  * pivot_root\n  * poweroff\n  * reboot\n  * rmmod\n  * route\n  * run-init\n  * start-stop-daemon\n  * sulogin\n  * swapon\n  * switch_root\n  * sysctl\n  * syslogd\n  * tc\n  * tunctl\n  * udhcpc\n  * uevent\n  * vconfig\n  * watchdog\n* ls -al **/usr/bin** | grep \"\\->\" | less\n  * [\n  * [[\n  * ar\n  * awk\n  * basename\n  * bc\n  * blkdiscard\n  * bunzip2\n  * bzcat\n  * bzip2\n  * cal\n  * chvt\n  * clear\n  * cmp\n  * crontab\n  * cut\n  * dc\n  * deallocvt\n  * diff\n  * dirname\n  * dos2unix\n  * dkpg\n  * dkpg-deb\n  * du\n  * dumpleases\n  * env\n  * expand\n  * expr\n  * factor\n  * fallocate\n  * find\n  * fold\n  * free\n  * ftpget\n  * ftpput\n  * groups\n  * head\n  * hexdump\n  * hostid\n  * id\n  * killall\n  * last\n  * less\n  * loggger\n  * logname\n  * lsscsi\n  * lzcat\n  * lzma\n  * md5sum\n  * microcom\n  * mkfifo\n  * mkpasswd\n  * nc\n  * nl\n  * nproc\n  * nsenter\n  * nslookup\n  * od\n  * openvt\n  * passwd\n  * paste\n  * patch\n  * printf\n  * realpath\n  * renice\n  * reset\n  * rpm2cpio\n  * seq\n  * setkeycodes\n  * setsid\n  * sha1sum\n  * sha256sum\n  * sha512sum\n  * shred\n  * shuf\n  * sort\n  * ssl_client\n  * strings\n  * svc\n  * svok\n  * tac\n  * tail\n  * taskset\n  * tee\n  * telnet\n  * test\n  * tftp\n  * time\n  * timeout\n  * top\n  * tr\n  * traceroute\n  * traceroute6\n  * truncate\n  * tty\n  * unexpand\n  * uniq\n  * unix2dos\n  * unlink\n  * unlzma\n  * unshare\n  * unxz\n  * unzip\n  * uptime\n  * uudecode\n  * uuencode\n  * w\n  * wc\n  * wget\n  * which\n  * who\n  * whoami\n  * xargs\n  * xxd\n  * xz\n  * xzcat\n  * yes\n\n#### [Initial Population and Dynamic Updates](https://git.busybox.net/busybox/plain/docs/mdev.txt)\n\nWe instruct the kernel to execute /sbin/mdev whenever a device is added or removed so that the device node can be created or destroyed. Then we seed /dev with all the device nodes that were created while the system was booting.\n\n#### Traditional Init Program\n\nAt the end of **/init** we call **/sbin/init** program which coordinates the rest of the boot process and configures the enviroment for the user. It becomes the parent or grandparent of all the processes that start up automatically on the system. In our case **/sbin/init** is our **busybox** console. At this point we have two busybox running one executing the /init and second one - our console **/sbin/init** that is executed by **/init**.\n\n## Test the Constructed Image\n\n    cd ..\n\n### Run with QEMU Emulator\n\nInstall the QEMU\n\n    sudo apt install qemu-system\n\nExecute the kernel with host architecture e.g. x86_64.\n\n    qemu-system-x86_64 -vnc :0 -kernel vmlinuz -initrd simple.igz -append \"root=/dev/ram\" /dev/zero\n\n### Booting a real machine\n\nThe **syslinux** package allows us to construct bootable systems for standard PCs on DOS-formatted storage.\n\n    sudo apt install syslinux\n\nA suitable medium should be chosen to boot from, e.g., a DOS-formatted USB flash drive. The DOS partition of the USB flash drive must be marked bootable. Some USB flash drives might need repartitioning and reformatting with the Linux tools in order to work correctly.\n\n#### [Syslinux Boot Process Overview](https://wiki.archlinux.org/index.php/syslinux#Boot_process_overview)\n\n1. **Stage 1 - Part 1 - Load [MBR](https://en.wikipedia.org/wiki/Master_boot_record#Sector_layout)**: At boot, the BIOS loads the 440 byte [MBR boot code](https://git.kernel.org/pub/scm/boot/syslinux/syslinux.git/tree/mbr/mbr.S?id=4298786ca19e9121568ecd4cc8b79d276ccfd24a) at the start of the disk (**/usr/lib/syslinux/bios/mbr.bin**).\n2. **Stage 1 - Part 2 - [Search active partition](https://git.kernel.org/pub/scm/boot/syslinux/syslinux.git/tree/mbr/mbr.S?id=4298786ca19e9121568ecd4cc8b79d276ccfd24a#n195)**: The MBR boot code looks for the active partition that is marked with boot flag. e.g. **/boot** partition.\n3. **Stage 2 - Part 1 - Execute volume boot record:**  The [MBR boot code executes the Volume Boot Record (VBR) of the /boot partition](https://git.kernel.org/pub/scm/boot/syslinux/syslinux.git/tree/core/diskstart.inc?id=10f6cf6eef0a7da7dad1933efdbfb101155792d0#n154). In the case of Syslinux, the VBR boot code is the starting sector of /boot/syslinux/ldlinux.sys which is created by the extlinux --install command.\n4. **Stage 2 - Part 2 - Execute /boot/syslinux/ldlinux.sys:** - The VBR will [load the rest of /boot/syslinux/ldlinux.sys](https://git.kernel.org/pub/scm/boot/syslinux/syslinux.git/tree/core/diskstart.inc#n154). The sector location of /boot/syslinux/ldlinux.sys should not change, otherwise syslinux will not boot.\n5. **Stage 3 - Load /boot/syslinux/ldlinux.c32:** The /boot/syslinux/ldlinux.sys will load the /boot/syslinux/ldlinux.c32 (core module) that contains the rest of the core part of syslinux that could not be fit into ldlinux.sys (due to file-size constraints). The ldlinux.c32 file should be present in every Syslinux installation and should match the version of ldlinux.sys installed in the partition.\n6. **Stage 4 - Search and Load configuration file:** - Once Syslinux is fully loaded, it [looks for /boot/syslinux/syslinux.cfg and loads](https://git.kernel.org/pub/scm/boot/syslinux/syslinux.git/tree/core/fs/lib/loadconfig.c?id=7d9c9eca562857fed25b4c8ef902e3de968d7631).\n\n#### Config for syslinux\n\nThe syslinux program should be run on the device something similar to /dev/sdx1 for a USB flash drive. You should be careful, as selecting the wrong devide name might overwrite your host system's hard drive. The syslinux loader can be \nconfigured using a file called **syslinux.cfg** \n\n    nano syslinux.cfg\n\nwhich would look something like:\n\n    Default simple\n    timeout 100\n    prompt 1\n    label simple\n      kernel vmlinuz\n      append initrd=simple.igz root=/dev/ram\n\n#### Burning the Bootloader\n\nWe give our user permission that allow **syslinux** to write code to MBR. We add our username to [disk](https://wiki.debian.org/SystemGroups#Groups_without_an_associated_user) group which have **raw access to disks** with [usermod](https://man7.org/linux/man-pages/man8/usermod.8.html).\n\n    sudo usermod -G disk -a $(whoami)\n\nWe execute this command only once, the change is permanent. We now mount the USB devices and copy the remaining archives to normal file system, we can use **dmesg** to find **sdX1**.\n\n    sudo syslinux /dev/sdX1\n    # Mount hand if not magicaly mounted when connected.\n    sudo mkdir /mnt/test\n    sudo mount -t vfat /dev/sdX1 /mnt/test\n    # And then work with /mnt/test\n    sudo cp vmlinuz simple.igz syslinux.cfg /mnt/test\n    sudo umount /mnt/test\n\nThe device may now be removed and booted on an appropriate PC.\n\n#### Test with VirtualBox\n\n1. Download [Plop Boot Manager](https://www.plop.at/en/bootmanager/download.html)\n2. Extract plpbt.iso\n3. From Settings -> Sorage -> Controller: IDE -> Add -> Select plpbt.iso\n4. From Settings -> System -> Boot Order -> Ensure Optical is before any other devices\n5. From Plop boot menu select USB device\n\nWhen we finish booting from USB, we can unplug USB. All content, kernel and initramfs is available in the main memory, we don't use USB anymore. We could not access the content of our USB since there is no corresponding driver for USB and file system.\n\n#### [Mount USB](https://unix.stackexchange.com/a/134516)\n\nSince there is no **[udev](https://wiki.debian.org/udev)** running, we have to create block device node in **/dev** manually. First check with **dmesg** if drivers detect USB and which identifier it has e.g. sda1 and then create our block device manually which following parameters for sda1, if identifier is diferrent you have to check [here](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/admin-guide/devices.txt?id=15ab85695595fbaba1ccccd07f7b0cede57cccf2#n200).\n\n    mknod /dev/sda b 8 0\n    mknod /dev/sda1 b 8 1\n    mkdir -p /mnt/test\n    mount -t vfat /dev/sda1 /mnt/test\n\n## Reference\n\n* [Un Sistema Linux Empotrado Sencillo](http://www.datsi.fi.upm.es/docencia/SEUM/publico/practica_USLES.pdf)\n* [An introduction to the Linux boot and startup processes](https://opensource.com/article/17/2/linux-boot-and-startup)\n* [A DETAILED LOOK AT THE BOOT PROCESS](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/installation_guide/s1-boot-init-shutdown-process)\n* [Disk Group](https://wiki.debian.org/SystemGroups#Groups_without_an_associated_user)\n* [Anatomy of the initrd and vmlinuz](https://s905060.gitbooks.io/site-reliability-engineer-handbook/content/anatomy_of_the_initrd_and_vmlinuz.html)\n* [Ramfs, rootfs and initramfs](https://www.kernel.org/doc/html/latest/filesystems/ramfs-rootfs-initramfs.html?highlight=initramfs#ramfs-and-ramdisk)\n* [Package: busybox](https://packages.debian.org/buster/utils/busybox)\n* [QEMU Example: Deploying initramfs](https://docs.windriver.com/bundle/Wind_River_Linux_Users_Guide_3.0_1/page/497722.html)",
        "slug": "Simple-Bootstrap-Linux-System",
        "date": "2020-11-08 19:15:57",
        "lang": "en",
        "tags": ["Linux", "Embedded System"],
        "path": "/Simple-Bootstrap-Linux-System/index.html"
      },
      {
        "title": "Java Stream",
        "markdownContentSource": "\n## Streams\n\nA stream represents a sequence of elements and can perform a series of operations on them\n\n    strings.stream()\n    .filter(s -> !s.equals(\"\"))\n    .map(String::toUpperCase)\n    .sorted()\n    .forEach(System.out::println);\n\nOperations on streams can be intermediate or final.\n\n* Intermediate: they return a stream to be able process them into chain.\n* Finals: they return either void or a result that does not it is a stream.\n\nThese operations must work without interference and stateless.\n\n* Intermediate operations will not be executed if there is no final operation.\n* The chain operation is followed vertically.\n* Streams are closed as soon as we execute a terminal operation.\n* Parallel streams can improve when we consider a large data size.\n* They are implemented with Fork/Join pool, with all the advantages and disadvantages that this entails.\n\nFrom the viewpoint of parallelism, an important benefit of using Java streams when possible is that the pipeline can be made to execute in parallel by designating the source to be a parallel stream, by simply replacing **.stream()** by **.parallelStream()** or **Stream.of(xxx).parallel**. This form of functional parallelism is a major convenience for the programmer, since they do not need to worry about explicitly allocating intermediate collections, or about ensuring that parallel accesses to data collections are properly synchronized.",
        "slug": "Java-Stream",
        "date": "2020-07-25 22:19:18",
        "lang": "en",
        "tags": [
          "Java",
          "Concurrent Programming",
          "Parallelism",
          "Java Stream",
          "Stream"
        ],
        "path": "/Java-Stream/index.html"
      },
      {
        "title": "MongoDB",
        "markdownContentSource": "\n## Scalability\n\nNoSQL is intented to offer more scalability. A balance between the scalability and the consistency.\n\n### Vertical\n\nFunctional decomposition\n\n### Horizontal\n\nScale by cloing\n\n### Transversal\n\nPartitioning similar things\n\n### Google Evulution\n\n#### Google File System\n\nDistributed file system to provide efficient, **reliable access** to data using large clusters of commodity hardward.\n\n#### MapReduce\n\nIt is a parallel, distributed algorithm on a cluster.\n\n![Map Reduce](map-reduce.png)\n\n#### Percolator\n\nThe process of updating an index is now divided into multiple concurrent transactions, each of which has to preserve invariants.\n\n![Percolator](percolator.png)\n\n#### Pregel\n\nIt is system for large scale graph processing.\n\n#### Dremel\n\nDremel is a distributed system developed for interactively quering large datasets.\n\n## DHT\n\nWe use hash function to select node of the cluster, acts as a routing function\n\n## CAP Theorem\n\nThe CAP theorem, also named Brewer's theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees:\n\n* Consistency: Every read receives the most recent write or an error\n* Availability: Every request receives a non-error response, without the guarantee that it contains the most recent write\n* Partition tolerante: The system continues to operate despite an arbitrary number of messages being dropped.\n\n## PACELC\n\n* P = Partition\n    * A: DynamoDB, Cassandra, Cosmos DB\n    * C: BigTable/HBase, MongoDB, MySQL Cluster\n* E = Else\n    * L: DynamoDB, Cassandra, Cosmos DB\n    * C: BigTable/HBase, MongoDB, MySQL Cluster\n\n## Consensus algorithm\n\nIt is used to achieve aggrement on a single data value among distributed processes or systems or the current state of distributed system. It is used to achieve reliability in a network involving multiple distributed nodes that contain the same information. A tipical use is to select the leader.\n\n1. Even if a single value is proposed, the network eventually recognizes it and choose it.\n2. Once the value is chosen by the network, it cannot be overwritten.\n3. Nodes don’t hear that a value has been chosen, unless it has really been chosen by the network.\n\n### Paxos\n\nThere are two main components Proposers(P) and Acceptors(A). There is also a listener(L), but it is less interesting for the majority of the discussion. Each nodes acts as all three. Proposers propose values based on client requests. Acceptors accept values and when there is majority/quorum, the proposed value is chosen and committed to the log. There could be a lot of of back and forth between P and A before the value gets chosen.\n\n## ACID\n\nThe relational database conforms with ACID principles.\n\n* Atomic: Each transaction is treated as a single unit\n* Consistency: A transaction can only bring the database from one valid state to another\n* Isolation: Concurrent execution of transactions leaves the database in the same state that would have been obtained if the transactions were executed sequentially.\n* Durability: once a transaction has been committed, it will remain committed even in the case of a system failure\n\n## BASE\n\nMany NoSQL database conforms with BASE principles.\n\n* Basic Availability: The system does guarantee availability\n* Soft-state: State of the system may change over time, even without input.\n* Eventually consistent: The system will become consistent over time, given that the system doesn't receive input during that time.\n    * Anti-Entropy Protocol: interchange update between nodes\n    * Reconciliation: All the replica arrive at a stable state.\n    * Liveness\n\n## NoSQL\n\nNoSQL stands for Not Only SQL which provide simple, and flexible and usually schemaless structure. It does not provide the consistency such as we provide with SGBDR.\n\n* Schemaless\n* Distributed\n* Structured Data\n* Avoid Join (it allows scale horizontally)\n\n### Graph database\n\nGraph databases are purpose-built to store and navigate relationships. Relationships are first-class citizens in graph databases, and most of the value of graph databases is derived from these relationships. Graph databases use nodes to store data entities, and edges to store relationships between entities. An edge always has a start node, end node, type, and direction, and an edge can describe parent-child relationships, actions, ownership, and the like. There is no limit to the number and kind of relationships a node can have.\n\n* Amazon Neptune\n* Neo4j\n\n### Key-Value\n\nA key-value database is a type of nonrelational database that uses a simple key-value method to store data. A key-value database stores data as a collection of key-value pairs in which a key serves as a unique identifier. Both keys and values can be anything, ranging from simple objects to complex compound objects. Key-value databases are highly partitionable and allow horizontal scaling at scales that other types of databases cannot achieve. The document model works well with use cases such as catalogs, user profiles, and content management systems where each document is unique and evolves over time. Document databases enable flexible indexing, powerful ad hoc queries, and analytics over collections of documents.\n\n* Amazon DynamoDB\n* Redis\n\n### Semantic\n\n* Triplestores\n\n### Columnar Database\n\nA columnar database is optimized for fast retrieval of columns of data, typically in analytical applications. Column-oriented databases are designed to scale “out” using distributed clusters of low-cost hardware to increase throughput.\n\n* Amazon Redshift\n* Apache Cassandra\n* Apache HBase\n\n### Search-Engine Database\n\nA search-engine database is a type of nonrelational database that is dedicated to the search of data content. Search-engine databases use indexes to categorize the similar characteristics among data and facilitate search capability. Search-engine databases are optimized for dealing with data that may be long, semistructured, or unstructured, and they typically offer specialized methods such as full-text search, complex search expressions, and ranking of search results. \n\n* Elastic Search\n\n### Document-oriented Database\n\nA document database is a type of nonrelational database that is designed to store and query data as JSON-like documents. Document databases make it easier for developers to store and query data in a database by using the same document-model format they use in their application code. The flexible, semistructured, and hierarchical nature of documents and document databases allows them to evolve with applications’ needs.\n\n* MongoDB\n* Amazon DocumentDB\n\nA simple description is key-value database where the value is document, e.g. **JSON** or **XML**.\n\n    {\n    field1: value1,\n    field2: value2,\n    field3: value3,\n    ...\n    fieldN: valueN\n    }\n\n* It is **schemaless** which means more **flexibility** to deal with complex data. \n* we can have **embedded document**.\n* we want to **avoid** using **join**.\n* The difference with key-value is we are allowed to recover document with document's **attributes**.\n* Documents combine structure and content, and there is great freedom in their structure. \n* We can simulate relationship using key of other document.\n* Documents are grouped into **collections**.\n  * We should only store documents of same type in a collection.\n  * If we liked store two types into one collection there must have a type identifier. Query one type of documents may require more time.\n  * CRUD must be common for all document inside the colelction\n* Documents are **polymorphic**, which means they don't have to maintain the same data structure.\n* We may find the **inconsistency** of the referenced document in other document.\n\n### Design Decision\n\n* **Balance** between redundancy and performance, **normalizatión y denormalization**.\n    * **Normalization** reduce the anomalies\n    * **Denormalization** Improve the performance.\n* Priority for **flexibility** and **scalability**\n* Be careful with document size, maximum 16MB\n* identify the indices\n  * index can improve the performance of read\n  * index also reduce the performance of write\n  * read heavy or write heavy\n* Think of **frequent queries**.\n    * If we have three products and all queries use all three products then we should put them into the **same collection**.\n    * Another situation we use same collecon is we know we will have more more types of product.\n* Is it atomic requiered? since we don't support transation across multiple document.\n* Schemaless != No schema, it means flexible\n    * same attribute can contains different data type\n* It suit our developmente because types of queries/applications can change\n\n#### Model Relationships Between Documents\n\n* Embedded Documents\n    * One-to-One\n    * One-to-Few\n    * Fast Retrieval\n    * Denormalization\n    * Two documents must be embedded if your information is always used together.\n* One-to-Many: Embedded Documents\n* Document References\n    * Many to Many\n    * The referential integrity of these documents has to be controlled at the application level\n    * **DBRef**\n* Hierarchy\n    * Store reference to parent document\n    * Store list of children in parent document\n\n## MongoDB\n\nRobust and scalable document oriented NoSQL. **Doesn't support transation** across multiple documents.\n\n* **Schemaless**\n* **Nested Document**\n* **Semi-structured data**\n\n### CRUD\n\n#### Show collections\n\n* Javascript: **db.getCollectionNames()**\n* Console: **show collections**\n\n#### Create\n\n##### Insert\n\n    db.books.insertOne({'title':'Thinking in Java', 'autor':'Bruce Eckel'})\n\n##### Bulk Insert\n\n    db.books.insertMany([{'title':'Thinking in Java', 'autor':'Bruce Eckel'}, {'title':'Effective Java', 'author':'Joshua Bloch'}])\n\nBy default the insertion is **ordered**. If there is a error during the insertiong it will just skip the document and continue with the next.\n\n##### Uniqueness\n\nThe field name **_id** is reserved for use as a primary key; its value must be unique in the collection, is immutable, and may be of any type other than an array, by default an ObjectId is generated. ObjectId is made up of **12 bytes**. The **first four bytes are a timestamp** with the second ones; the next **three bytes** represent the **unique identifier of the machine**; the **next two are the process identifier**; And finally, the **last three bytes** are an **incremental field**. It give us document creation date.                \n\n#### Read\n\n##### Read All\n\n    db.collections.find()\n    db.collections.find().pretty()\n\n##### Filter\n\n    db.books.find({'title':{$eq : 'Thinking in Java'}})\n    db.books.find({$and : [\n        {$or : [{'title':{$regex : '^Thinking'}}, {'title':{$regex : '^Effective'}}]},\n        {'title':{$regex : 'Java$'}}\n    ]})\n\n[Query Selectors](https://docs.mongodb.com/manual/reference/operator/query/)\n\nThe syntax is **{ attribute: { $operador: value } }**\n\n##### Filter with Embedded Document\n\n    db.clients.find({\"direction\": {\"street\":\"C/XX, 99\", \"city\": \"BB\", \"state\":\"SS\", \"ZIP\":\"123456\"}})\n\n##### Filter with Arrays\n\n    db.clients.find({\"tel\":[\"123456789\", \"987654321\"]})\n\n#### Update\n\n    db.books.update({'title':'Thinking in Java'}, {$set: {'ISBN':'978-0131872486'}})\n\n[Update Operators](https://docs.mongodb.com/manual/reference/operator/update/)\n\nthe syntax is **{ $operador: { attribute: value } }**\n\n##### Upsert\n\n If **upsert** set to true, creates a new document when no document matches the query criteria. The default value is false, which does not insert a new document when no match is found.\n\n    db.collection.update(query, update, {upsert: true})\n\n#### Save\n\nUpdates an existing document or inserts a new document, depending on its document parameter.\n\n    db.collection.save(\n        <document>,\n        {\n            writeConcern: <document>\n        }\n    )\n\n#### Delete\n\n##### Single Item\n\n    db.books.remove([{'author':'Bruce Eckel'}])\n\n##### Delete All Documents Inside The Collection\n\n    db.books.drop()\n\nHas better **drop** performance than **remove**\n\n#### Sort\n\n    db.books.find().sort({'title':1, ...more fields to use for sorting})\n\n* ascending: 1\n* descending: -1\n\n#### Count\n\n    db.books.find().count()\n    db.books.count({\"author\":{$eq:\"ABC\"}})\n\n### Indexes\n\nIndexes in MongoDB are generated as a **B-tree**. This increases the speed when searching and sorting when returning results. It is recommended that these fields have a **high cardinality**.\n\n    db.coleccion.createIndex({…})\n    db.coleccion.dropIndex({…})\n    db.coleccion.dropIndexes()\n    db.coleccion.getIndexes()\n\n#### Single Field\n\n    db.coleccion.createIndex({ a: 1 })\n\n#### Compound Index\n\n    db.coleccion.createIndex({ A: 1, B: -1, C: 1 })\n\nThe index that will be generated with the previous statement will group the data first by the field A ascendingly and then by the field B descendingly and the by the field C ascendingly. Composite indexes can be used to query one or more of the fields. In these case we can query A, or A and B together, or A and B and C together. We can only query together if they match the order.\n\n#### Unique Index\n\nThe unique property for an index causes MongoDB to reject duplicate values for the indexed field.\n\n    db.coleccion.createIndex({'name':1}, {\"unique\":true})\n\n#### Sparse Indexes\n\nThe sparse property of an index ensures that the index only contain entries for documents that have the indexed field.\n\n    db.coleccion.createIndex({'name':1}, {\"sparse\":true})\n\n#### Partial Indexes\n\nPartial indexes only index the documents in a collection that meet a specified filter expression.\n\n    db.coleccion.createIndex({'name':1}, {partialFilterExpression: {age: {$gt:18}}})\n\n#### Index Intersection\n\nMongoDB can use the intersection of multiple indexes to fulfill queries.\n\n    db.coleccion.createIndex({'name':1})\n    db.coleccion.createIndex({'age':1})\n    db.coleccion.find({'name;: \"Jorge\", 'age': {$gt:18}})\n\n#### Index on Embedded Document\n\n    db.coleccion.createIndex({ address.city\" : 1})\n\n#### Multikey Indexes\n\nTo index a field that holds an array value, MongoDB creates an index key for each element in the array. These multikey indexes support efficient queries against array fields. The limitation is only one field can be array type.\n\n    db.coll.createIndex({<field>: < 1 or -1 > })\n\n#### Covered Query\n\nA covered query is a query that can be satisfied entirely using an index and does not have to examine any documents. An index covers a query when all of the following apply\n\n* all the fields in the query are part of an index\n* all the fields returned in the results are in the same index.\n* no fields in the query are equal to null (i.e. {\"field\" : null} or {\"field\" : {$eq : null}} ).\n\n#### Debug Query\n\n    db.colección.find().explain()\n\n### Aggregation Framework\n\n#### Aggregation Pipeline\n\nAggregation operations process data records and return computed results, it is modeled on the concept of data processing pipelines with [stages operators](https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/) and the [expression operators](https://docs.mongodb.com/manual/reference/operator/aggregation/).\n\n    db.orders.aggregate([\n        {$match: { status: \"A\" }},\n        {$group: { _id: \"$cust_id\", total: { $sum: \"$amount\" }}}\n    ])\n\n{% iframe https://docs.mongodb.com/manual/_images/agg-pipeline.mp4 %}\n\nEach operators receives a set of arguments\n\n    {<operador>:[<argumento1>, <argumento2>...]}\n\nExpressions are used as filters, and usually have no state of their own except the accumulator. But not all the acumudor has the global state e.g. $project.\n\n##### $project\n\nReshapes each document in the stream, such as by adding new fields or removing existing fields.\n\n    { $project: { <specifications> }}\n\n##### $match\n\nFilters the document stream to allow only matching documents to pass unmodified into the next pipeline stage.\n\n    { $match: { <query> } }\n\n##### $sample\n\nRandomly selects the specified number of documents from its input.\n\n    {$sample: {size: <positive integer>}}\n\n##### $sort\n\nReorders the document stream by a specified sort key.\n\n    { $sort: { <field1>: <order>, <field2>:<order> ... }}\n\n##### $limit\n\nPasses the first n documents unmodified to the pipeline where n is the specified limit.\n\n    { $limit: <positive integer> }\n\n##### $skip\n\nSkips the first n documents where n is the specified skip number and passes the remaining documents unmodified to the pipeline.\n\n    { $skip: <positive integer> }\n\n##### $out\n\nWrites the resulting documents of the aggregation pipeline to a collection.\n\n    { $out: \"<output collections>\" }\n\n##### $group\n\nGroups input documents by a specified identifier expression and applies the accumulator expression(s), if specified, to each group.\n\n    {\n      $group:\n        {\n          _id: <expression>, // Group By Expression\n          <field1>: { <accumulator1> : <expression1> },\n        ...\n        }\n    }\n\n##### $unwind\n\nDeconstructs an array field from the input documents to output a document for each element. Each output document replaces the array with an element value\n\n    {$unwind: <array field>}\n\n##### $lookup\n\nPerforms a left outer join to another collection in the same database.\n\n    {\n      $lookup:\n        {\n          from: <collection to join>,\n          localField: <field from the input documents>,\n          foreignField: <field from the documents of the \"from\" collection>,\n          as: <output array field>\n        }\n    }\n\n#### Map-Reduce\n\nMongoDB also provides map-reduce operations to perform aggregation. In general, map-reduce operations have two phases: a map stage that processes each document and emits one or more objects for each input document, and reduce phase that combines the output of the map operation.\n\n![Map-Reduce](https://docs.mongodb.com/manual/_images/map-reduce.bakedsvg.svg)\n\n## Reference\n\n* [MongoDB Getting Started](https://docs.mongodb.com/manual/tutorial/getting-started/)\n* [MongoDB Relationships](https://docs.mongodb.com/manual/applications/data-models-relationships/)\n* [MongoDB Indexes](https://docs.mongodb.com/manual/indexes/)\n* [MongoDB Query Optimization](https://docs.mongodb.com/manual/core/query-optimization/)\n* [MongoDB Aggregation](https://docs.mongodb.com/manual/aggregation/)",
        "slug": "MongoDB",
        "date": "2020-06-07 13:57:06",
        "lang": "en",
        "tags": ["Document-oriented Database", "MongoDB"],
        "path": "/MongoDB/index.html"
      },
      {
        "title": "Matrices in Computer Graphics",
        "markdownContentSource": "\n# Matrix Transformations\n\nThe matrices are used frequently in computer graphics and the matrix transformations are one of the core mechanics of any 3D graphics, the chain of matrix transformations allows rendering a 3D object on a 2D monitor.\n\n## Affine Space\n\nAn affine space is a generalization of vector space where there is not a notion of an origin. Affine space works with points and vectors where translations are allowed.\n\n## Affine transformation\n\nAn affine transformation is a function that maps points from one affine space to another, preserving affine properties like parallelism, ratio of lengths for parallel line segments. Affine transformations include: **scaling**, **rotation**, **translation**, **reflection**, **shearing**. In geometry, an affine transformation can be represented as the composition of a linear transformation plus a translation. If we want to perform any affine transformation using matrix form, the representation of 3D Euclidean vector space is not enough. We can use **4D homogeneous space** to represent the 3D Euclidean affine space. We extend our vectors to four-dimension and using **4x4 matrix** to represent affine transformation.\n\n## 4D Homogeneous Space\n\nThe fourth component in a 4D vector is $w$, sometimes referred to as the **homogeneous coordinate**. Image the standard 2D plane such that any 2D point $(x, y)$ is represented in homogeneous 3D space $(x, y, 1)$. There are an infinite points in homogeneous space $(kx, ky, k)$, $k \\ne 0$, these points form a line through the origin.  For all points that are not in the plane $w=1$, we can project the point onto the standard plane by dividing by $w$. So the homogeneous coordinate $(x, y, w)$ is mapped to the 2D point $(x/w, y/w)$.\n\n![Projecting homogeneous coordinates](Projecting.png)\n\nWhen $w = 0$, we can interpret as a **direction**. The **location** where $w \\ne 0$ are **points** and the **directions** with $w = 0$ are **vectors**. If assume for the moment that $w$ is always 1, any 3 x 3 transformation matrix can be represented in 4D homogeneous space by using the conversion.\n\n$$\n\\begin{bmatrix}\nm_{11} & m_{12} & m_{13} & 0\\\\\nm_{21} & m_{22} & m_{23} & 0\\\\\nm_{31} & m_{32} & m_{33} & 0\\\\\n0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n$$\n\n## Scaling Matrices\n\n![Scaling a 2D object with various factors for k_x and k_y](scaling-2D.png)\n\nGiven $\\vec{k}=(k_i, k_j, k_z)$ is a 3D vector that represent the scale along each axis. The 3D homogeneous scale matrix is\n\n$$\nS(\\vec{k}) =\n\\begin{bmatrix}\nk_x & 0 & 0 & 0\\\\\n0 & k_y & 0 & 0\\\\\n0 & 0 & k_z & 0\\\\\n0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n$$\n\nThe scaled vector will be\n\n$$\np^\\prime = S(\\vec{k})p\n$$\n\n[Scaling](https://github.com/vitaminac/minige/blob/96ea43c5caae8099cb8faa6129f9598614318df4/src/geometry/mat4.cpp#L104-L111)\n\n## Rotation Matrices\n\nIn 3D, rotation occurs about a **axis** and $\\theta$ is the angle using the **right-hand rule**(**counterclockwise direction**). The rotations can also be represented by clockwise direction.\n\n$$\np^\\prime = \\textbf{R}(\\hat{\\textbf{n}},\\theta)p\n$$\n\n$$\n\\textbf{R}_x(\\theta) =\n\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & \\cos\\theta & -\\sin\\theta\\\\\n0 & \\sin\\theta & \\cos\\theta\\\\\n\\end{bmatrix}\n$$\n\n$$\n\\textbf{R}_y(\\theta) =\n\\begin{bmatrix}\n\\cos\\theta & 0 & \\sin\\theta\\\\\n0 & 1 & 0\\\\\n-\\sin\\theta & 0 & \\cos\\theta\\\\\n\\end{bmatrix}\n$$\n\n$$\n\\textbf{R}_z(\\theta) =\n\\begin{bmatrix}\n\\cos\\theta & -\\sin\\theta & 0\\\\\n\\sin\\theta & \\cos\\theta & 0\\\\\n0 & 0 & 1\\\\\n\\end{bmatrix}\n$$\n\n![Rotation](rotation.gif)\n\nFor an arbitrary axis in 3D, the rotation matrix $\\textbf{R}(\\hat{\\textbf{n}}, \\theta)$ is\n\n$$\n\\begin{bmatrix}\nn_x^2 (1 - \\cos \\theta) + \\cos \\theta & n_x n_y (1 - \\cos \\theta) - n_z \\sin \\theta & n_x n_z (1 - \\cos \\theta) + n_y \\sin \\theta\\\\\nn_x n_y (1 - \\cos \\theta) + n_z \\sin \\theta & n_y^2 (1 - \\cos \\theta) + \\cos \\theta & n_y n_z (1 - \\cos \\theta) - n_x \\sin \\theta\\\\\nn_x n_z (1 - \\cos \\theta) - n_y \\sin \\theta & n_y n_z (1 - \\cos \\theta) + n_x \\sin \\theta & n_z^2 (1 - \\cos \\theta) + \\cos \\theta\\\\\n\\end{bmatrix}\n$$\n\n[Rotation](https://github.com/vitaminac/minige/blob/96ea43c5caae8099cb8faa6129f9598614318df4/src/geometry/mat4.cpp#L76-L102)\n\n## Refletion Matrix\n\nReflection (also called mirroring) is a transformation that “flips” the object about a line (in 2D) or a plane (in 3D).\n\n![Reflection](Reflection.png)\n\nReflection can be accomplished by applying a scale factor of $-1$. For the transformation to be linear, the plane must contain the origin.\n\n$$\np^\\prime = \\textbf{R}(\\hat{\\textbf{n}})p\n$$\n\n$$\n\\textbf{R}(\\hat{\\textbf{n}}) =\n\\begin{bmatrix}\n1 - 2 n^2_x & -2 n_x n_y & -2 n_x n_z\\\\\n-2 n_x n_y & 1 - 2 n^2_y & -2 n_y n_z\\\\\n-2 n_x n_z & -2 n_y n_z & 1 - 2 n^2_z\\\\\n\\end{bmatrix}\n$$\n\n## Shearing Matrix\n\nShearing is a transformation that “skews” the coordinate space, stretching it non-uniformly. Angles are not preserved; however, surprisingly, areas and volumes are.\n\n![Shearing](Shearing.png)\n\n$$\n\\textbf{H} =\n\\begin{bmatrix}\n1 & s^y_x & s^z_x\\\\\ns^x_y & 1 & s^z_y\\\\\ns^x_z & s^y_z & 1\\\\\n\\end{bmatrix}\n$$\n\n$$\nx^\\prime = x + s^y_x y + s^z_x z\\\\\ny^\\prime = s^x_y x + y + s^z_y z\\\\\nz^\\prime = s^x_z x + s^y_z y + z\\\\\n$$\n\n## Translation Matrix\n\nWith 4x4 Matrix, we can also express translation as a matrix multiplication that represent the position where we want to move our space to, which we can use to head move the camara or to move objects.\n\n$$\np^\\prime = T(\\vec{d})p\n$$\n\n$$\nT(\\vec{d}) =\n\\begin{bmatrix}\n1 & 0 & 0 & d_x\\\\\n0 & 1 & 0 & d_y\\\\\n0 & 0 & 1 & d_z\\\\\n0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n$$\n\n![Translation](translation.png)\n\n[Translation](https://github.com/vitaminac/minige/blob/96ea43c5caae8099cb8faa6129f9598614318df4/src/geometry/mat4.cpp#L68-L75)\n\n## Compositions of Transformations\n\nWe can chain several transformations together by multiplying matrices in order, the result will be a single matrix that encodes the full transformation.\n\nLet $T$ be translation matrix and $\\vec{d}$ be a translation vector, $L$ as linear transformation about the axes, the corresponding matrix is\n\n$$\nA = T\n\\begin{bmatrix} \nL & 0\\\\\n0 & 1\\\\\n\\end{bmatrix} =\n\\begin{bmatrix} \nL & 0\\\\\n0 & 1\\\\\n\\end{bmatrix}\nT =\n\\begin{bmatrix}\nL & \\vec{d}\\\\\n0 & 1\\\\\n\\end{bmatrix}\n$$\n\nThen we could compute a new point $p^\\prime$ by $p^\\prime = A p$. $L$ can be any combination of scaling, rotation, reflection, shearing matrix but the order of application matter.\n\n## Model Matrix\n\nEvery model in the game lives in its specific vector space, called **model space**. All the vertices are relative to the origin of the **model space**, if we want them to be in any spatial relation we need **model matrix** to transform them into a common space which is called **world space**. Since every object will be in its own position and orientation in the world, we will need a different **model matrix** for each object to scale it, rotate it and move it to the desired position and orientation with appropriate size. When all the objects have been transformed into a common space, their vertices will then be relative to the **world space**.\n\n## View Matrix\n\nWe use **view matrix** to transform into an auxiliary space **view space** is that simplifies a lot the math if we could have the camera centered in the origin and watching down one of the three axis. In OpenGL, by default, the camera is at the coordinate origin, facing towards -z and with the vector up oriented with the y-axis.\n\n![Camera](camera.png)\n![Camera Space](camera-space.png)\n\n## Projection Matrix\n\nThe **view frustum** is the volume of space that is potentially visible to the camera. The view frustum is bounded by six planes, known as the clip planes, top, left, bottom, right, near and far planes. The near and far clip planes, which correspond to certain camera-space values of z. The reason for the far clip plane is prevents rendering of objects beyond a certain distance. A far clip plane can limit the number of objects that need to be rendered in an outdoor environment. The far clip establishes what (floating point) z value in camera space will correspond to the maximum value that can be stored in the depth buffer.\n\n![Perspective View Frustum](perspective-view-frustum.png)\n![Orthographic View Frustum](orthographic-view-frustum.png)\n\nTo facilitate the transformation of points in **view frustum** to pixels, we use **projection matrix** to map the **view frustum** into the **homogeneous clip space**.\n\n### Orthographic Projection\n\nIn the orthographic projection, also known as a parallel projection, the lines from the original point to the resulting projected point on the plane are parallel to the camera's viewing direciton.\n\n![Orthographic Projection](orthographic-projection.png)\n\nThe orthographic frustum is\n\n$$\n\\begin{split}\nM_{\\text{orthographic projection}} &= ST\\\\\n&=\n\\begin{bmatrix}\n\\frac{2}{right - left} & 0 & 0 & 0\\\\\n0 & \\frac{2}{top - bottom} & 0 & 0\\\\\n0 & 0 & \\frac{2}{far - near} & 0\\\\\n0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & 0 & -\\frac{left + right}{2}\\\\\n0 & 1 & 0 & -\\frac{top + bottom}{2}\\\\\n0 & 0 & -1 & -\\frac{far + near}{2}\\\\\n0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n\\\\\n&=\n\\begin{bmatrix}\n\\frac{2}{right - left} & 0 & 0 & -\\frac{right + left}{right - left}\\\\\n0 & \\frac{2}{top - bottom} & 0 & -\\frac{top + bottom}{top - bottom}\\\\\n0 & 0 & -\\frac{2}{far - near} & -\\frac{far + near}{far - near}\\\\\n0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n\\\\\n\\end{split}\n$$\n\n[Orthographic Projection](https://github.com/vitaminac/minige/blob/2011e5b8c7d833e67ddab40aa28a7e511a2aae5b/src/geometry/mat4.cpp#L113-L126)\n\nThe projection on a 2x2 plane parallel to the XY plane that passes through the point z = -D is\n\n$$\n\\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & -D\\\\\n0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n$$\n\n### Perspective Projection\n\nWith perspective projection, the projectors intersect at the **center of projeciton**.\n\n![Center of projeciton](center-of-projection.png)\n\nDue to perpective foreshortening, the projecion on the left is larger than the projection on the right. The left-hand is closer to the projection plane. As we move an object farther away from the center of projection, its orthographic projection remains constant, but the perspective projection gets smaller. The projectors cross the center of projection and the image is inverted when striking the plane.\n\n![Perspective foreshortening](perspective-foreshortening.png)\n\nBy similar triangles, we know\n\n![Projection plane from the side](projection-plane-negative.png)\n\n$$\n\\begin{split}\np\\prime_x &= \\frac{-dp_x}{z}\\\\\np\\prime_y &= \\frac{-dp_y}{z}\\\\\n\\end{split}\n$$\n\nThe $z$ value of all the projected points are the same $-d$. Thus the result of projecting a point $\\textbf{p}$ through the origin onto a plane at $z=-d$ is\n\n$$\np = (x, y, z) \\Rightarrow p^\\prime = (-dx/z, -dy/z, -d)\n$$\n\n![Projection plane from other side](projection-plane-positive.png)\n\nif we move the plane of projeciton to $z=d$, we will have\n\n$$\np^\\prime = [dx/z \\quad, dy/z \\quad d]\n$$\n\nThe projection on a plane parallel to the XY plane that passes through the point z = -D with the camera (center of projection) at the origin facing towards -z and with the vector up oriented with the y-axis is\n\n![Perspective Projection](perspective-projection.png)\n\n$$\n\\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & -\\frac{1}{d} & 0\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx\\\\\ny\\\\\nz\\\\\n1\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx\\\\\ny\\\\\nz\\\\\n-\\frac{z}{d}\\\\\n\\end{bmatrix}\n$$\n\nAnd after the division we will have\n\n$$\n\\begin{bmatrix}\n-\\frac{dx}{z}\\\\\n-\\frac{dy}{z}\\\\\n-d\\\\\n\\end{bmatrix}\n$$\n\nThe perspective frustum is\n\n$$\np^{\\prime} \n=\n\\begin{bmatrix}\np_x^{\\prime}\\\\\np_y^{\\prime}\\\\\np_z^{\\prime}\\\\\nw\\\\\n\\end{bmatrix}\n= M_{\\text{perspective projection}}p =\n\\begin{bmatrix}\n\\frac{2n}{r-l} & 0 & \\frac{r+l}{r-l} & 0\\\\\n0 & \\frac{2n}{t-b} & \\frac{t+b}{t-b} & 0\\\\\n0 & 0 & -\\frac{f+n}{f-n} & -\\frac{2nf}{f-n}\\\\\n0 & 0 & -1 & 0\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\np_x\\\\\np_y\\\\\np_z\\\\\n1\\\\\n\\end{bmatrix}\n$$\n\n[Perspective Frustum](https://github.com/vitaminac/minige/blob/2011e5b8c7d833e67ddab40aa28a7e511a2aae5b/src/geometry/mat4.cpp#L143-L156)\n\nGiven the **field of view $\\alpha$** in y direction and the **aspect ratio $\\beta$** of a display screen is equal to the ratio of $x$ (width) to $y$ (height)\n\n$$\ne = \\frac{1}{\\tan{\\frac{\\alpha}{2}}}\n$$\n\nThe view frustum plane normal directions in OpenGL camera space is\n\n![Camera Space](camera-space-opengl.png)\n![Perspective Frustum](perspective-frustum.png)\n\nBecause of the symmetry along x, y axis, the following relationships hold\n\n$$\nt = \\tan{\\frac{\\alpha}{2}} n = \\frac{n}{e}\\\\\nb = -t\\\\\nr = t \\cdot \\beta = \\frac{n\\beta}{e}\\\\\nl = -r\\\\\n\\frac{r+l}{r-l} = \\frac{t+b}{t-b} = 0\\\\\n\\frac{2n}{r-l} = \\frac{e}{\\beta}\\\\\n\\frac{2n}{t-b} = e\\\\\n$$\n\n$$\n\\begin{bmatrix}\n\\frac{e}{\\beta} & 0 & 0 & 0\\\\\n0 & e & 0 & 0\\\\\n0 & 0 & -\\frac{f+n}{f-n} & -\\frac{2nf}{f-n}\\\\\n0 & 0 & -1 & 0\\\\\n\\end{bmatrix}\n$$\n\n[Perspective Projection](https://github.com/vitaminac/minige/blob/6e5ea57ff4bb7c012a528311f9b05dd46a1bd0e0/src/geometry/mat4.cpp#L128-L141)\n\n### Frustum Culling\n\nAfter applying the projection matrix, each vertex is transformed into **homogeneous clip space**, where its position is given in homogeneous coordinates. $(𝑥,𝑦,𝑧,𝑤)$. In this space, the viewing frustum is defined as a normalized cube called the **canonical view volume** (**CVV**). Any vertex whose **homogeneous clip space** coordinates fall outside these bounds is considered outside the frustum and should be clipped (discarded).\n\n$$\n-𝑤 \\le x \\le 𝑤\\\\\n−w \\le y \\le w\\\\\n−w \\le z \\le w\\\\\n$$\n\nOpenGL pipeline automatically performs clipping after vertex processing. If a vertex is within the range, it is passed to next stage. Otherwise, OpenGL may discard it, depending on the rest of the geometry.\n\n## MVP Matrix\n\nWe can chain all three matrices together to a single matrix $M_{\\text{model, view, projection}} = M_{projection} M_{\\text{view}} M_{\\text{model}}$. We can now perform a 4x4 affine transformation with MVP matrix to map an object from **model space** into **homogeneous clip space** $p^{\\prime} = M_{\\text{model, view, projection}} p$.\n\n## 3D Normalized Device Coordinates\n\nGiven a **homogeneous clip space**, now we subsequently divide $x$, $y$, $z$ by the $w$ coordinate of the **homogeneous clip space**, this operation produces **3D normalized device coordinates** (**NDC**), also commonly known as \"**screen space**\".\n\n![NDC](normalized-device-coordinates.png)\n\nThat is a normalized 6-tuple cube which defines the clipping planes. The dimensions are between -1 and 1 for every axis, anything outside the [1, -1] range is outside the camera view area. The cube is translated so that it is centered at the origin which is defined by having a minimum corner (-1,-1,-1) at left-bottom-near and a maximum corner (1,1,1) at right-top-far.\n\n## Viewport Transformation Matrix\n\nThe **NDC** now represent the 2D positions of points on screen, with X and Y in [−1, 1], together with the depth within the depth buffer range, Z in [−1, 1]. The axis orientation is X = right, Y = up, and Z can be either forward or backward depending on the depth buffer configuration.\n\n![Pipeline](transformation-pipeline.PNG)\n\nTo render on the portion of the output device expressed in pixels (**viewport**), we apply the viewport transform to the normalized device coordinates.\n\n![Viewport](viewport.png)\n\n$$\nV = TS = \n\\begin{bmatrix}\n1 & 0 & 0 & \\frac{r+l}{2}\\\\\n0 & 1 & 0 & \\frac{t+b}{2}\\\\\n0 & 0 & 1 & \\frac{1}{2}\\\\\n0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{r-l}{2} & 0 & 0 & 0\\\\\n0 & \\frac{t-b}{2} & 0 & 0\\\\\n0 & 0 & \\frac{1}{2} & 0\\\\\n0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{r-l}{2} & 0 & 0 & \\frac{r+l}{2}\\\\\n0 & \\frac{t-b}{2} & 0 & \\frac{t+b}{2}\\\\\n0 & 0 & \\frac{1}{2} & \\frac{1}{2}\\\\\n0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n$$\n\n## Reference\n\n* [3D Math Primer For Graphics and Game Development](https://www.amazon.com/gp/product/1568817231/ref=dbs_a_def_rwt_bibl_vppi_i0) pag. 138-144, 176-189\n* [Matrices - Sparky Engine](https://www.youtube.com/watch?v=5-minbbZW14&t=56s&list=PLlrATfBNZ98fqE45g3jZA_hLGUrD4bo6_&index=10)\n* [Fundamentals of Computer Graphics](https://www.amazon.com/Fundamentals-Computer-Graphics-Peter-Shirley/dp/1568814690) pag. 140-144\n* [Mathematics for 3D Game Programming and Computer Graphics, Third Edition](https://www.amazon.com/Mathematics-Programming-Computer-Graphics-Third/dp/1435458869) pag. 75, 76, 101-115\n* [Wikipedia - Orthographic projection](https://en.wikipedia.org/wiki/Orthographic_projection#Geometry)\n* [World, View and Projection Transformation Matrices](http://www.codinglabs.net/article_world_view_projection_matrix.aspx)\n* [3D Transformation](https://www.tutorialspoint.com/computer_graphics/3d_transformation.htm)\n* [The Perspective and Orthographic Projection Matrix](https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/opengl-perspective-projection-matrix)\n* [The Viewport Transformation](http://glasnost.itcarlow.ie/~powerk/GeneralGraphicsNotes/projection/viewport_transformation.html)\n* [Lean OpenGL - Coordinate Systems](https://learnopengl.com/Getting-started/Coordinate-Systems)",
        "slug": "Matrices-in-Computer-Graphics",
        "date": "2020-05-31 13:34:44",
        "lang": "en",
        "tags": [
          "4D Matrices",
          "Translation Matrix",
          "Projection Matrix",
          "Rotation Matrix",
          "Scaling Matrix",
          "Model Matrix",
          "View Matrix",
          "Projecion Matrix",
          "Matrix",
          "Computer Graphics"
        ],
        "path": "/Matrices-in-Computer-Graphics/index.html"
      },
      {
        "title": "Tree Diameter",
        "markdownContentSource": "\n## Definition\n\nThe diameter of a tree is the distance of the longest path between two end nodes $\\arg\\max_{(u, v) \\in G}d(u, v)$, where $d(u, v)$ is the distance function. It has many applications on tree problems and often a high complexity solution can be changed to a linear solution with the property of the tree diameter.\n\n## Finding The Diameter of N-Ary Tree\n\n### Greedy Algorithm\n\n1. Run **BFS** to find the farthest node **u** starting from an arbitrary node said **s**\n2. Then run **BFS** from **u** to find the farthest node **v**\n3. Distance between node **u** and **v** is the diameter of given tree\n\n### Implementation\n\n[PT07Z](https://github.com/vitaminac/code/blob/main/judge/src/main/java/spoj/PT07ZLongestPathInATree.java)\n\n[P1985CowMarathon](https://github.com/vitaminac/code/blob/main/judge/src/main/java/poj/P1985CowMarathon.java)\n\n## Proof\n\n### Properties of Trees\n\n1. Between any 2 nodes in a tree there is exactly one path\n2. Any node can serve as the root of the tree\n\n### Proof by Contradiction\n\n1. Assuming one end of the diameter **u** is known, the other end must be the node **v** furthest from this end. We can find it by using **BFS**, the last discovered node is **v**. \n2. Why we run **BFS** on an arbitrary node **s** will alway end with **u**?\n   1. Let **u** and **v** be any two nodes such that **d(u, v)** is the diameter of the tree. There is a unique path from **u** to **v** because of tree properties.\n   2. Assuming we start at node **s**.\n      1. If **s** is **u**, then we will always end with **v** by the first **BFS**, and the second **BFS** we get **u** again.\n      2. If **s** is not **u** and we end with a node **x** distinct than **u** and **v** using **BFS**.\n         1. If the path of **(x, u)** does not intersect with the path of **(u, v)**, the **d(x, u) + d(u, v) > d(u, v)**, contradiction.\n         2. If the path of **(x, u)** intersect with **(u, v)** at **y**, then **d(x, y) > d(u, y)** since we know when we started the search from **s**, **x** is deeper than **u**, then **d(x, y) + d(y, v) > dis(u, v)**. contradiction.\n\n## Reference\n\n* [MIT - Introduction to Algorithms: Problem Set 9 Solutions](http://courses.csail.mit.edu/6.046/fall01/handouts/ps9sol.pdf)\n* [Algorithm to find diameter of a tree using BFS/DFS](https://cs.stackexchange.com/questions/22855/algorithm-to-find-diameter-of-a-tree-using-bfs-dfs-why-does-it-work)\n* [树的直径及其性质与证明](https://www.cnblogs.com/Khada-Jhin/p/10195287.html)\n* [SPOJ PT07Z - Longest path in a tree](https://www.spoj.com/problems/PT07Z/)\n* [POJ Cow Marathon](http://poj.org/problem?id=1985)\n* [Diameter of a Binary Tree](https://www.geeksforgeeks.org/diameter-of-a-binary-tree/)\n* [Tree](https://en.wikipedia.org/wiki/Tree_(graph_theory))\n",
        "slug": "Tree-Diameter",
        "date": "2020-03-07 10:00:44",
        "lang": "en",
        "tags": ["algorithms"],
        "path": "/Tree-Diameter/index.html"
      },
      {
        "title": "Google Foobar Decrypt Final Message",
        "markdownContentSource": "\nAfter i have finished Google Foobar challenge I get a encrypted message. The ciphertext was xored with my username and then encode with base64, just do the reverse process.\n\n[Decrypt Final Message](https://github.com/vitaminac/code/blob/master/competition/foobar/decrypt_message.py)",
        "slug": "Google-Foobar-Decrypt-Message",
        "date": "2019-12-26 21:43:50",
        "lang": "en",
        "tags": [],
        "path": "/Google-Foobar-Decrypt-Message/index.html"
      },
      {
        "title": "Google Foobar Dodge The Lasers",
        "markdownContentSource": "\n# Beatty sequence\n\nThis problem require to calculate $\\sum_{i=1}^{n} \\lfloor i\\sqrt{2} \\rfloor$. We must take into consideration the precision and the performance. The $n$ can be very large, up to 101 digits. There several aspect. How to take floor more efficiently? How to deal with big number? After a few search I found that there is a specific algorithm. This type of problem involves a mathematical concept: Beatty sequence. In mathematics, a Beatty sequence is the sequence of integers found by taking the floor of the positive multiples of a positive irrational number.\n\n$$\n\\mathcal{B}_r = \\lfloor r \\rfloor ,\\ \\lfloor 2r \\rfloor ,\\ \\lfloor 3r \\rfloor ,\\dots\n$$\n\nLet $\\mathcal{B}_r^{(i)} = \\lfloor i*r \\rfloor$ for some irrational positive number,\n\nand\n\n$S(r, n) = \\sum_{i=1}^{n}{\\mathcal{B}_r^{(i)}}$\n\nIf $r \\ge 2$ we let $s = r - 1$ and we have $S(r, n) = S(s, n) + \\sum_{i=1}^{n} i = S(s, n) + \\frac{n(n+1)}{2}$\n\nIf $1 < r < 2$ there is a theorem that says that if $s$ satisfies $r^{-1} + s^{-1} = 1$, then the sequences $\\mathcal{B}_r$ and $\\mathcal{B}_s$ for $n \\ge 1$ partition $\\mathbb{N}$ (not counting 0).\n\nTherefore, $S(r, n) + S(s, \\lfloor \\frac{\\mathcal{B}_r^{(n)}}{s} \\rfloor) = \\sum_i^{\\mathcal{B}_r^{(n)}} i = \\frac{\\mathcal{B}_r^{(n)}(\\mathcal{B}_r^{(n)} + 1)}{2}$\n\nAnd also $\\lfloor \\frac{\\mathcal{B}_r^{(n)}}{s} \\rfloor = \\lfloor \\mathcal{B}_r^{(n)}(1 - \\frac{1}{r}) \\rfloor = \\mathcal{B}_r^{(n)} - \\lceil \\frac{\\mathcal{B}_r^{(n)}}{r} \\rceil = \\mathcal{B}_r^{(n)} - n$\n\nThen, letting $n^{\\prime} = \\lfloor (r - 1)n \\rfloor = \\mathcal{B}_{r-1}^{n}$\n\nwe have $S(r,n) = \\frac{\\mathcal{B}_r^{(n)}(\\mathcal{B}_r^{(n)} + 1)}{2} - S(s, n^{\\prime}) = \\frac{(n + n^{\\prime})(n + n^{\\prime} + 1)}{2} - S(s, n^{\\prime})$.\n\nBack to the problem, we have $r = \\sqrt{2}$, so we start with $s = 2 + \\sqrt{2}$. We can get a recurrence formula.\n\nLet $n^{\\prime} = \\lfloor (\\sqrt{2} - 1)n \\rfloor$,\n\n$$\n\\begin{split}\nS(\\sqrt{2}, n) \n&= \\frac{(n + n^{\\prime})(n + n^{\\prime} + 1)}{2} - S(2 + \\sqrt{2}, n^{\\prime})\\\\\n&= \\frac{(n + n^{\\prime})(n + n^{\\prime} + 1)}{2} - (n^{\\prime}(n^{\\prime} + 1) - S(\\sqrt{2}, n^{\\prime}))\\\\\n&= nn^{\\prime} + \\frac{n(n+1)}{2} - \\frac{n^{\\prime}(n^{\\prime} + 1)}{2} - S(\\sqrt{2}, n^{\\prime})\n\\end{split}\n$$\n\n[Dodge The Lasers](https://github.com/vitaminac/code/blob/2c9df93333/judge/src/main/java/foobar/l5/DodgeTheLasers.java)\n\n## Reference\n\n* [How to find $\\sum_{i=1}^{n} \\left\\lfloor i\\sqrt{2} \\right\\rfloor$](https://math.stackexchange.com/a/2053713/698177)\n* [A beatty sequence A001951](https://oeis.org/A001951)\n* [Beatty sequence](https://en.wikipedia.org/wiki/Beatty_sequence)\n* [sun-mylove's solution](https://github.com/sun-mylove/google-foobar/blob/master/lev05_ch01.py)\n* [arinkverma's solution](https://github.com/arinkverma/google-foobar/blob/master/5.1_dodge_the_lasers.py)\n* [oneshan's solution](https://github.com/oneshan/foobar/blob/master/dodge_the_lasers/solution.py)\n",
        "slug": "Google-Foobar-Dodge-The-Lasers",
        "date": "2019-12-17 22:50:54",
        "lang": "en",
        "tags": ["algorithms"],
        "path": "/Google-Foobar-Dodge-The-Lasers/index.html"
      },
      {
        "title": "Google Foobar Escape Pods",
        "markdownContentSource": "\nThis problem involves solving the equivalent maximum flow problem. We can use Ford–Fulkerson algorithm with BFS to calculate the total flow.\n\n# Maximum flow problem\n\nWe can use a directed graph as a flow network where the source produces the elements at some rate and the sink consumes the material at the same rate.\n\n## Definition\n\nWe define a flow network $G = (V, E)$ as a directed graph in which each edge $(u, y) \\in E$ has a nonnegative capacity $c(u, v) \\ge 0$ and there is not an edge $(v, u)$ in the reverse direction. We distinguish two vertices in a flow network: a **source s** and a **sink t**, thus for each vertex $v \\in V$, the flow network contains a path $ s \\rightsquigarrow v \\rightsquigarrow t$. A **flow** in $G$ is a real-valued function $f: V \\times V \\to R$ that satisfies $0 \\le f(u, v) \\le  c(u, v)$ and **flow conservation** implies for all $u \\in V - \\\\{s, t\\\\}$, $\\sum_{v \\in V} f(v, u) = \\sum_{v \\in V} f(u, v)$.\n\nWe call the nonegative quantity $f(u, v)$ the flow from vertex $u$ to vertex $v$. When $(u, v) \\notin E$, there can be no flow from $u$ to $v$, and $f(u, v) = 0$. The **value** $|f|$ of a flow $f$ is defined as $|f| = \\sum_{v \\in V} f(s, v) - \\sum_{v \\in V} f(v, s)$. In the **maximum-flow problem**, we are given a flow network $G$ with source $s$ and sink $t$, and we wish to find a flow of maximum value.\n\n## Transformation\n\nWe call the two edges $(v_1, v_2)$ and $(v_2, v_1)$ antiparallel. And if we wish model a flow problem with **antiparallel edges**, we must transform the network into an equivalent one containing without antiparallel edges by adding a new vertex $v^\\prime$ and replacing edege $(v_1, v_2)$ with the pair of edges $(v_1, v^\\prime)$ and $(v^\\prime, v_2)$.\n\n![Equivalent Antiparallel Flow](equivalent-antiparallel-flow.png)\n\nWhen there are several sources and sinks, we can reduce the problem to an ordinary maximum flow problem by adding a **supersource** $s$ and a directed edge $(s, s_i)$ with capacity $c(s, s_i) = \\infty$ for each $i=1, 2, ..., n$ and a new **supersink** $t$ and a directed edge $(t_i, t)$ with capacity $c(t_i, t) = \\infty$ for each $i = 1, 2, .., n$.\n\n![Equivalent Multiple Sources Sinks Flow](equivalent-multiple-sources-sinks-flow.png)\n\n## Residual Network\n\nGiven a flow network $G = (V, E)$  with source $s$ and sink $t$ and a flow $f$, the **residual network** $G_{f}$ consists of **residual edges** with capacities $c_{f}(u, v)$\n\n$$\nc_{f}(u, v) =\n\\begin{cases} \n    c(u, v) - f(u, v) & \\text{ if } (u, v) \\in E\\\\\n    f(v, u) & \\text{ if } (v, u) \\in E\\\\\n    0 & \\text{otherwise}\\\\\n\\end{cases}\n$$\n\n![Residual Network](residual-network.png)\n\n### Augmenting Paths\n\nGiven a flow network $G = (V, E)$ and a flow $f$, an **augmenting path** $p$ is a simple path from $s$ to $t$ in  the residual network $G_{f}$. We call the maximum amount by which we can increase the flow on each edge in an augmenting path $p$ the **residual capacity** of $p$, given by $c_f(p) = \\min{ \\\\{ c_f(u, v) : (u, v) \\text{ is on } p \\\\} }$.\n\n## The Ford Fulkerson Method\n\nThe Ford-Fulkerson method is a **greedy algorithm** that iteratively increases the value of the flow. At each iteration.\n\nGiven a network $G = (V, E)$ with flow capacity $c$, a source node $s$ and a sink node $t$\n1. $f \\leftarrow 0$\n2. $f(u, v) \\leftarrow 0$ for all edges $(u, v) \\in E$\n3. **while** there exists an **augmenting path** $p$\n    1. Find $c_f(p)$\n    2. $f \\leftarrow f + c_f(p)$\n    3. for each edge $(u, v) \\in p$\n       1. if $(u, v) \\in E$\n          1. $f(u, v) \\leftarrow f(u, v) + c_f(p)$\n       2. else\n          1. $f(v, u) \\leftarrow f(v, u) - c_f(p)$\n4. **return** $f$\n\n[Escape Pods](https://github.com/vitaminac/code/blob/2c9df93333/judge/src/main/java/foobar/l4/EscapePods.java)\n\n## Reference\n\n* Introduction to Algorithms, pages 708~730\n* [Ford–Fulkerson algorithm](https://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm#Algorithm)\n* [GeeksforGeeks: Ford-Fulkerson Algorithm for Maximum Flow Problem](https://www.geeksforgeeks.org/ford-fulkerson-algorithm-for-maximum-flow-problem/)\n* [Edmonds Karp Algorithm for Max-Flow](https://www.youtube.com/watch?v=SqGeM3FYkfo)\n* [Google Foobar Round 4](https://surajshetiya.github.io/Google-foobar/#round-4)\n* [4.2_escape_pods.py](https://github.com/nkapliev/google-foo.bar/blob/master/problems/4.2_escape_pods.py)\n* [Google foo.bar 面试记](http://xiaohanyu.me/posts/2017-04-30-google-foobar-interview/#cb8)",
        "slug": "Google-Foobar-Escape-Pods",
        "date": "2019-11-22 22:35:28",
        "lang": "en",
        "tags": ["algorithms"],
        "path": "/Google-Foobar-Escape-Pods/index.html"
      },
      {
        "title": "Install Windows Sandbox in Windoes 10 Home Edition",
        "markdownContentSource": "\nsave the following command as bat file and then execute\n\n    @echo off\n    \n    echo Checking for permissions\n    >nul 2>&1 \"%SYSTEMROOT%\\system32\\cacls.exe\" \"%SYSTEMROOT%\\system32\\config\\system\"\n    \n    echo Permission check result: %errorlevel%\n    \n    REM --> If error flag set, we do not have admin.\n    if '%errorlevel%' NEQ '0' (\n    echo Requesting administrative privileges...\n    goto UACPrompt\n    ) else ( goto gotAdmin )\n    \n    :UACPrompt\n    echo Set UAC = CreateObject^(\"Shell.Application\"^) > \"%temp%\\getadmin.vbs\"\n    echo UAC.ShellExecute \"%~s0\", \"\", \"\", \"runas\", 1 >> \"%temp%\\getadmin.vbs\"\n    \n    echo Running created temporary \"%temp%\\getadmin.vbs\"\n    timeout /T 2\n    \"%temp%\\getadmin.vbs\"\n    exit /B\n    \n    :gotAdmin\n    if exist \"%temp%\\getadmin.vbs\" ( del \"%temp%\\getadmin.vbs\" )\n    pushd \"%CD%\"\n    CD /D \"%~dp0\" \n    \n    echo Batch was successfully started with admin privileges\n    echo .\n    cls\n    Title Sandbox Installer\n    \n    pushd \"%~dp0\"\n    \n    dir /b %SystemRoot%\\servicing\\Packages\\*Containers*.mum >sandbox.txt\n    \n    for /f %%i in ('findstr /i . sandbox.txt 2^>nul') do dism /online /norestart /add-package:\"%SystemRoot%\\servicing\\Packages\\%%i\"\n    \n    del sandbox.txt\n    \n    Dism /online /enable-feature /featurename:Containers-DisposableClientVM /LimitAccess /ALL\n\n    pause",
        "slug": "Install-Windows-Sandbox-Win10Home",
        "date": "2019-11-16 15:07:28",
        "lang": "en",
        "tags": ["Hacker"],
        "path": "/Install-Windows-Sandbox-Win10Home/index.html"
      },
      {
        "title": "Google Foobar Free the Bunny Prisoners",
        "markdownContentSource": "\n# Free the Bunny Prisoners\n\n## Analysys\n\nThe problem can be rearrange as follow:\n\nIf you have N bunnies, and M locks, distribute M distinct keys among the bunnies so that it will always require num_required bunnies to open the locks, and no bunny should have the same key twice.\n\nLet us now consider this simple situation, let us say we have chosen $\\text{num_required} - 1$ bunnies at random, and we were to choose 1 more bunny to get the complete set of keys to open the prison door. We know that these $\\text{num_required} - 1$ bunnies cannot open the door by themselves and hence the remaining $\\text{num_bunnies} - \\text{num_required} + 1$ bunnies must have a key that these $\\text{num_required} - 1$ bunnies don't. We have total [$\\binom{\\text{num_buns}}{\\text{num_required} - 1}$](https://en.wikipedia.org/wiki/Binomial_coefficient) combination of each $\\text{num_required}- 1$ bunnies pair, so we have also [$\\binom{\\text{num_buns}}{\\text{num_required} - 1}$](https://en.wikipedia.org/wiki/Binomial_coefficient) distinct keys. And there should be $\\text{num_buns} - \\text{num_required} + 1$ copies of each distinct key among the bunnies.\n\nNote that $\\binom{\\text{num_buns}}{\\text{num_required} - 1} = \\binom{\\text{num_buns}}{\\text{num_buns} - \\text{num_required} + 1}$\n\nThus, for the example of N = 5 and M = 3, there are $\\binom{5}{3 - 1}$ distinct keys (10 keys).\n\nWe must distribute $5 - 3 + 1$ copies of all $10$ keys amongst the bunnies in such a way that any 3 bunnies we pair together have, amongst them, at least one copy of every key.\n\n1. we count number of distinct keys we have with formula $\\binom{\\text{num_buns}}{\\text{num_required} - 1}$.\n2. Find the number of copies per key $\\text{num_buns} - \\text{num_required} + 1$.\n3. Yield combinations of keyholders one at a time, and we give key to each keyholder.\n\n[Free The Bunny Prisoners](https://github.com/vitaminac/code/blob/master/competition/foobar/Free_The_Bunny_Prisoners.py)\n\n## Reference\n\n* [Combination](https://en.wikipedia.org/wiki/Combination)\n* [Pigeonhole principle](https://en.wikipedia.org/wiki/Pigeonhole_principle)\n* [Google Foobar Round 4](https://surajshetiya.github.io/Google-foobar/#round-4)\n* [foobar_4-1_free_the_bunny_prisoners.py](https://github.com/FoxHub/Google-FooBar/blob/master/Level-4/foobar_4-1_free_the_bunny_prisoners.py)\n* [鸽笼原理](https://zhuanlan.zhihu.com/p/75478415)\n* [4.1_bunny.py](https://github.com/arinkverma/google-foobar/blob/master/4.1_bunny.py)\n",
        "slug": "Google-Foobar-Free-the-Bunny-Prisoners",
        "date": "2019-11-09 09:28:42",
        "lang": "en",
        "tags": ["algorithms"],
        "path": "/Google-Foobar-Free-the-Bunny-Prisoners/index.html"
      },
      {
        "title": "Google Kickstart 2019 Round F Flattening",
        "markdownContentSource": "\n# [Google Kickstart 2019 Round F Flattening](https://codingcompetitions.withgoogle.com/kickstart/round/0000000000050edc/000000000018666c)\n\nFirst, we find the major height $H_k$ from each position $i$ to each position $j$. The change is not more than K, it means it will have at most $K+1$ group. We start counting the minimum number of steps needed to position j if we allow $g$ group exists. Follow this sequence of operation, we start from $g=0$ until we get $g=k+1$.\n\n[Solution](https://github.com/vitaminac/code/blob/master/competition/kickstart/kickstart2019RoundF_Flattening.cpp)",
        "slug": "Google-Kickstart-2019-Round-F-Flattening",
        "date": "2019-11-03 17:51:53",
        "lang": "en",
        "tags": ["algorithms"],
        "path": "/Google-Kickstart-2019-Round-F-Flattening/index.html"
      },
      {
        "title": "Google Kickstart 2019 Round G Shifts",
        "markdownContentSource": "\n# [Google Kickstart 2019 Round G Shifts](https://codingcompetitions.withgoogle.com/kickstart/round/0000000000050e02/000000000018fd5e)\n\nWe need to calculate the number of valid combinations that makes two guards happy. I first tried backtracking but it will have TL problem. In each node, we have three options, so in fact, we will have $3^{20}$ in the hidden set. We need to discard as many options as early as possible. As the max n is 20 less the 32 we can use bit representation for each combination. We first calculate valid **a** and **b** guard's permutations separately.  For each valid b guard's permutation, we find also the number of permutation that can convert to that permutation flipping some zero bit to one. And then for each valid, a guard permutation finds the corresponding b guard's permutation.\n\nPay attention that the sum of happiness may exceed 32 bit.\n\n[Solution](https://github.com/vitaminac/code/blob/master/competition/kickstart/kickstart2019RoundG_Shifts.cpp)\n",
        "slug": "Google-Kickstart-2019-Round-G-Shifts",
        "date": "2019-11-03 01:47:31",
        "lang": "en",
        "tags": ["algorithms"],
        "path": "/Google-Kickstart-2019-Round-G-Shifts/index.html"
      },
      {
        "title": "Google Kickstart 2019 Round G The Equation",
        "markdownContentSource": "\n# [Google Kickstart 2019 Round G The Equation](https://codingcompetitions.withgoogle.com/kickstart/round/0000000000050e02/000000000018fe36)\n\nThe essential of the problem is to find the number $k$ that xor with each $A_i$ gives the largest number. As described in the statement the upper limit is $10^{15}$ which result the highest bit is $log_2{10^{15}} = 50$. We start from the highest bit and try to set 1 at every possible position if it is feasible. But first, we need to process the $A$ to find out the minimum sum of n lowest bits, so that having the bit set to one or zero we know in advance if we will result in an infeasible solution or no. \n\nPay Attention to **1LL << n** and read carefully of each limit, for example: **M < $10^{15}$**.\n\n[Solution](https://github.com/vitaminac/code/blob/master/competition/kickstart/kickstart2019RoundG_The_Equation.cpp)\n",
        "slug": "Google-Kickstart-2019-Round-G-The-Equation",
        "date": "2019-11-02 22:11:00",
        "lang": "en",
        "tags": ["algorithms"],
        "path": "/Google-Kickstart-2019-Round-G-The-Equation/index.html"
      },
      {
        "title": "Install Docker Desktop in Windows10 Home Edition",
        "markdownContentSource": "\n# Install Docker Desktop in Windows10 Home Edition\n\nMost laptop are now pre-installed with Windows 10 Home Edition, while the Home Edition does not support Hyper-V. Docker Desktop cannot be installed directly. But in fact, the home version can activate Hyper-V by script to install Docker Desktop. Here's how to do it.\n\n## Activate Hyper-V\n\nThe method is very simple, save the following as a .cmd file, then open the file as an administrator. Prompt to restart, and then you can use the full-featured Hyper-V after the restart.\n\n    pushd \"%~dp0\"\n\n    dir /b %SystemRoot%\\servicing\\Packages\\*Hyper-V*.mum >hyper-v.txt\n\n    for /f %%i in ('findstr /i . hyper-v.txt 2^>nul') do dism /online /norestart /add-package:\"%SystemRoot%\\servicing\\Packages\\%%i\"\n\n    del hyper-v.txt\n\n    Dism /online /enable-feature /featurename:Microsoft-Hyper-V-All /LimitAccess /ALL\n\n## Fake as a professional version to bypass installation detection\n\nSince Docker Desktop will detect the system version at the time of installation, direct installation will show that the installation failed. So you need to change the registry to bypass the installation detection. Execute the following command.\n\n    REG ADD \"HKEY_LOCAL_MACHINE\\software\\Microsoft\\Windows NT\\CurrentVersion\" /v EditionId /T REG_EXPAND_SZ /d Professional /F\n\n## Other issues\n\nInstall the [docker-ce-desktop-windows](https://docs.docker.com/docker-for-windows/install/) directly on the official website, and uncheck the window container during installation. After testing, the linux container is running normally. Switching to the windows container will detect the windows version and will not start. However, the windows container is generally not used.\n",
        "slug": "Install-DockerDesktop-Win10Home",
        "date": "2019-11-02 21:11:31",
        "lang": "en",
        "tags": ["Docker", "Hacker"],
        "path": "/Install-DockerDesktop-Win10Home/index.html"
      },
      {
        "title": "Google Kickstart 2019 Round G Book Reading",
        "markdownContentSource": "\n# [Google Kickstart 2019 Round G Book Reading](https://codingcompetitions.withgoogle.com/kickstart/round/0000000000050e02/000000000018fd0d)\n\nThe most challenge part of this problem is that Q and N is a very larger number. So making a nested for loop will cause $N^2$ complexity. Instead, we use an array of size of MAX N to cache the answer. Thanks to multiple readers could probably have the same $R_i$. Another trick is that $R_i$ is not necessarily smaller than N, we should take into account and skip unnecessary operation. It is faster if we count the reading pages by stepping $R_i$ towards the N. A mistake I had was that I didn´t realize the answer size, the sum of Q readers' reading pages will probably be larger the 32 bits, we must define the answer with at least 64 bits integer.\n\n[Solution](https://github.com/vitaminac/code/blob/master/competition/kickstart/kickstart2019RoundG_BookReading.cpp)\n",
        "slug": "Google-Kickstart-2019-Round-G-Book-Reading",
        "date": "2019-11-02 19:24:59",
        "lang": "en",
        "tags": ["algorithms"],
        "path": "/Google-Kickstart-2019-Round-G-Book-Reading/index.html"
      },
      {
        "title": "Neo4j Graph Database",
        "markdownContentSource": "\n# Neo4j\n\nNeo4j is an open source, NoSQL, native **graph databases**. In Neo4j, the data are persisted for long term durability. Neo4j can be used with both the open-source license (Community Edition) or a commercial license (Enterprise Edition) which includes technical support.\n\n## Graph Database\n\nGraph database is a database used to model the data in the form of graph. The model represents data in **Nodes**, **Relationships** and **Properties**. Unlike relational database, graph databases store relationships and connections as first-class entities.\n\n### Concepts\n\n![Simple Graph Diagram](Simple-Graph-Diagram.png)\n\n#### Node\n\n**Nodes** are often used to represent entities\n\n![Node](Node.png)\n\n#### Label\n\n**Labels** are used to shape the **domain** by grouping nodes into sets. With that in place, you can ask to perform operations only on your given label nodes. Since labels can be added and removed during runtime, they can also be used to mark **temporary states** for nodes. A node can have zero to many labels. Labels are used associate indexes and constraints with groups of nodes.\n\n#### Relationships\n\nA structure with a name and direction that describes the relationship between two nodes and provides structure and context to the graph. A **relationship** connects **two nodes** or **one nodes(self-reference)**, it must have exactly one relationship type. **Relationships** always havs a **directions**, for each **Relationship** contains **From Node** and **To Node**.\n\n![Relationship](Relationship.png)\n\n#### Properties\n\nNodes and relationships define the graph while properties add context by storing relevant information in **key-value pairs** in the nodes and relationships that are used to add qualities to nodes and relationships.\n\nProperty values can be\n\n* Number, an abstract type, which has the subtypes Integer and Float\n* String\n* Boolean\n* The spatial type Point\n* Temporal types: Date, Time, LocalTime, DateTime, LocalDateTime and Duration\n\n#### Traversals and paths\n\n**Traversing** a graph means visiting nodes by **following relationships according to some rules**. The traversal result could be returned as a **path**.\n\n![Traversal](Traversal.png)\n\n#### Schema\n\nA schema in Neo4j refers to **indexes** and **constraints**.\n\n##### Indexes\n\n**Indexes** are used to increase performance.\n\n##### Constraints\n\n**Constraints** are used to make sure that the data adheres to the rules of the domain. This is optional, you can create data without defining a schema.\n\n### Comparison with SQL\n\n| RDBMS          | Graph Database   |\n| ------------- -|:----------------:|\n| **Tables**     | **Graphs**       |\n| **Rows**       | **Nodes**        |\n| **Columns**    | **Properties**   |\n| **Cells**      | **Values**       |\n| **Constraints**| **Relationships**|\n| **Joins**      | **Traversal**    |\n\n### Features\n\n* **Constant time traversals**\n* **Scalability**: Users can scale the database by increasing the number of reads/writes and volume without effecting the query speed and data integrity.\n* **Cypher Query Language (CQL)**: a declarative query language similar to SQL, but designed for graph pattern matching and traversals\n* **Flexible Schema**: easily change according to the requirement, property graph schema that can adapt over time\n* **ACID Properties**: Atomicity, Consistency, Isolation, and Durability\n* **Reliability**: It also support for **Replication** for data safety and reliability.\n* **Built-in Neo4j browser web applications** to create and retrieve graph data\n* **Driver Support**: Java, Go, Python\n\n### Technical View\n\n#### Object Cache\n\nThe object cache caches individual relationships and nodes and respectively their properties in a form which is optimized for traversal of the graph.\n\n#### Protocol\n\nThe Neo4j Browser and the official Neo4j Drivers use the Bolt database protocol to communicate with Neo4j. From an application or from the Neo4j Browser, you can execute query statements.\n\n| Connector name       | Protocol      | Default port number |\n| -------------------- |:-------------:| -------------------:|\n| dbms.connector.bolt  | Bolt          | 7687                |\n| dbms.connector.http  | HTTP          | 7474                |\n| dbms.connector.https | HTTPS         | 7473                |\n\nBolt is an efficient binary protocol for access to the database layer that compresses data sent over the wire as well as encrypting the data.\n\n#### Index-free adjacency\n\nIndex-free adjacency means that the query engine uses pointers to traverse paths (nodes connected by relationships) in the graph which is very fast.\n\n## CQL\n\nNeo4j has CQL as query language, CQL stands for Cypher Query Language. You can practise your CQL skill in [Neo4j Sandbox](https://neo4j.com/sandbox-v2).\n\n### [NULL](https://neo4j.com/docs/cypher-manual/current/syntax/working-with-null/)\n\nNULL is used to represent missing or undefined values.\n\n### [Parameters](https://neo4j.com/docs/cypher-manual/current/syntax/parameters/)\n\nCypher supports querying with parameters for.\n\n* literals and expressions\n* node and relationship ids\n* for explicit indexes only: index values and queries\n\n#### String literal\n\n    MATCH (n:LabelOfNode) WHERE n.property = $value RETURN n\n\n#### Regular expression\n\n    MATCH (n:LabelOfNode) WHERE n.property ~= $regex RETURN n\n\n#### Create node with properties\n\n    CREATE ($props)\n\n#### Setting all properties on a node\n\n    MATCH (n:LabelOfNode) WHERE n.property=value SET n = $props\n\n#### SKIP and LIMIT\n\n    MATCH (n:LabelOfNode) RETURN n.property SKIP $s LIMIT $l\n\n#### Multiple node ids\n\n    MATCH (n) WHERE id(n) IN $ids RETURN n.property\n\n#### Calling procedures\n\n    CALL db.resampleIndex($indexname)\n\n### MATCH\n\n#### Returns all the nodes in database\n\n    MATCH (node) RETURN node\n\n#### Get all the nodes under a specific label\n\n    MATCH (node:LabelOfNode) RETURN node\n\n#### Match by Relationship\n\n    MATCH (node1)-[:Relationship_Of]->(node2) RETURN node1\n    MATCH (node1)-[:Relationship_Of]->(node2) RETURN node2\n    MATCH (node1)<-[:Relationship_Of]-(node2) RETURN node1\n    MATCH (node1)<-[:Relationship_Of]-(node2) RETURN node2\n    MATCH (node1)-[:Relationship_Of]-(node2) RETURN node1\n    MATCH (node1)-[:Relationship_Of]-(node2) RETURN node2\n\n#### Variable length relationships\n\nReturns all node2 related to node1 by 1 to 3 hops\n\n    MATCH (node1:LabelOfNode1)-[:Relationship_Of*1..3]->(node2:LabelOfNode2) RETURN node2\n\n#### Match a path\n\n    MATCH p =(a)-->(b)-->(c)\n    RETURN p\n\n#### OPTIONAL MATCH\n\nThe OPTIONAL MATCH clause is used to search for the pattern described in it, while using nulls for missing parts of the pattern, it could be considered the equivalent of the outer join in SQL\n\n    MATCH (node1:LabelOfNode {properties...}) \n    OPTIONAL MATCH (node1)-[:Relationship_Of]->(node2)\n    RETURN node2\n\n    MATCH (node1:LabelOfNode {properties...}) \n    OPTIONAL MATCH (node1)-->(node2)\n    RETURN node2\n\n### WHERE\n\n#### Filter by property\n\n    MATCH (node)\n    WHERE node.property = value\n    RETURN node\n\n#### WHERE Clause with Multiple Conditions\n\n    MATCH (node)  \n    WHERE node.property1 = value1 AND node.property2 = value2 \n    RETURN node\n\n#### Using Relationship with Where Clause\n\n    MATCH (node)\n    WHERE (node)-[:Relationship_Of]->({properties}) \n    RETURN node\n\n#### String matching\n\n    MATCH (n) WHERE n.property STARTS WITH value RETURN n\n    MATCH (n) WHERE n.property ENDS WITH value RETURN n\n    MATCH (n) WHERE n.property CONTAINS value RETURN n\n\n#### In\n\n    MATCH (n) WHERE n.property IN [value1, value2[,...]] RETURN n\n\n### CREATE\n\n#### Creating Nodes\n\n##### Create a simple node\n\nThe **node name** in CQL are actually variables. You can assign a node (or a relationship) to a variable to handle this node in the rest of the Cypher query.\n\n    CREATE (node);\n\n##### Create multiple nodes\n\n    CREATE (node1),(node2);\n\n##### Create node with a label\n\n    CREATE (node:LabelOfNode) \n\n##### Create a node with multiple labels\n\n    CREATE (node[:LabelOfNode1[:LabelOfNode2[...]]]) \n\n##### Create multiple nodes with multiple labels\n\n    CREATE (node1[:LabelOfNode1[:LabelOfNode2[...]]]), (node2[:LabelOfNode1[:LabelOfNode2[...]]])[,...];\n\n##### Create node with Properties\n\n    CREATE (node:LabelOfNode {key1:value[, key2:value,[...]]})\n\n##### RETURN newly created node\n\n    CREATE (Node:LabelOfNode{properties}) RETURN Node\n\n#### Creating Relationships\n\n##### Creating a new relationship\n\n    CREATE (node1)\n    CREATE (node2)\n    CREATE (node1)-[:Relationship_Of]->(node2) \n\n##### Creating a Relationship Between the Existing Nodes\n\n    MATCH (node1:LabeofNode1), (node2:LabeofNode2)\n    WHERE node1.property1 = value1 AND node2.property2 = value2 \n    CREATE (node1)-[:Relationship_Of]->(node2)\n\n##### Creating a Relationship with Label and Properties\n\n    CREATE (node1)-[variable:Rel_Type {key1:value1, key2:value2, . . . n}]->(node2)\n\n##### Creating a Complete Path\n\n    CREATE p = (node1 {properties})-[:Relationship_Of1]->(node2 {properties})[:Relationship_Of2]->(node3 {properties}) \n    RETURN p\n\n### RETURN\n\n#### Returning Created Node\n\n    Create (node:LabelOfNode {properties}) \n    RETURN node\n\n### Unique results\n\n    Create (node:LabelOfNode {properties}) \n    RETURN DISTINCT node\n\n#### Returning Multiple Nodes\n\n    CREATE (node1:LabelOfNode1 {properties}), (node2:LabelOfNode2 {properties})\n    RETURN node1, node2\n\n#### Returning Relationships\n\n    CREATE (node1)-[r:Relationship_Of]->(node2) RETURN r\n\n#### Returning Properties\n\n    MATCH (node:LabelOfNode {properties}) RETURN node.property\n\n#### Returning All Elements\n\n    MATCH p = (node1)-[:Relationship_Of]-(node2) RETURN COUNT(*)\n\n    MATCH p = (node1)-[:Relationship_Of]->(node2) RETURN COUNT(*)\n\n#### Returning a Variable With a Column Alias\n\n    MATCH (node:LabelOfNode {properties}) RETURN node.property AS alias\n\n### ORDER BY\n\n#### Ordering By a Property\n\n    MATCH (n)  \n    RETURN n.property1, n.property2[,...]\n    ORDER BY n.property1\n\n#### Ordering Nodes by Multiple Properties\n\n    MATCH (n)\n    RETURN n \n    ORDER BY n.property1, n.property2\n\n#### Ordering Nodes by Descending Order\n\n    MATCH (n)\n    RETURN n\n    ORDER BY n.property DESC\n\n### LIMIT\n\nThe **LIMIT** clause is used to limit the number rows returned from the query or passed to other parts of a query.\n\n    MATCH (n) \n    RETURN n \n    ORDER BY n.property\n    LIMIT 1\n\n#### Limit with expression\n\n    MATCH (n) \n    RETURN n \n    ORDER BY n.property\n    LIMIT toInt(3 * rand()) + 1\n\n### SKIP\n\nThe **SKIP** clause is used to define from which row to start including the rows in the output.\n\n#### Skipping the first 3 nodes\n\n    MATCH (n)  \n    RETURN n.property1, n.property2 \n    ORDER BY n.property1 DESC \n    SKIP 3\n\n#### Skip Using Expression\n\n    MATCH (n)  \n    RETURN n.property1, n.property2 \n    ORDER BY n.property1 DESC \n    SKIP toInt(3 * rand()) + 1\n\n### WITH\n\n**WITH** clause is used to perform some intermediate processing during a query where you may want to save some results or test some values during the query to control whether a query will end.\n\n    MATCH (n)\n    WITH n\n    ORDER BY n.property\n    RETURN collect(n.property)\n\n### UNWIND\n\nThe **UNWIND** clause is used to unwind a list into a sequence of rows.\n\n    UNWIND [a, b, c, d] AS x \n    RETURN x\n\n### MERGE\n\nMERGE command is a combination of CREATE command and MATCH command. It ensures that a patterns exists in the graph, either the pattern already exists or it is created.\n\n#### Merging a Node with a Label\n\n    MERGE (node:LabelOfNode) RETURN node\n\n#### Merging a Node with Properties\n\n    MERGE (node:LabelOfNode {key1:value[, key2:value, key3:value[...]]})\n\n#### OnCreate and OnMatch\n\nWhenever, we execute a merge query, a node is either matched or created. Using on create and on match, you can set properties for indicating whether the node is created or matched.\n\n    MERGE (node:LabelOfNode {properties}) \n    ON CREATE SET node.isCreated =\"true\" \n    ON MATCH SET node.isFound =\"true\"\n\n#### Merge a Relationship\n\n    MATCH (node1:LabelOfNode1), (node2:LabelOfNode2) \n        WHERE node1.property1 = value1 AND node2.property2 = value2\n        MERGE (node1)-[:Relationship_Of]->(node2) \n    RETURN node1, node2 \n\n### DELETE\n\nNodes cannot be deleted if they still have relationships attached to them.\n\n#### Deleting All Nodes and Relationships\n\n    MATCH (node) DETACH DELETE node\n\n#### Deleting a Particular Node\n\n    MATCH (node:LabelOfNode)\n    WHERE node.property = value \n    DETACH DELETE node\n\n### REMOVE\n\n#### Removing a Property\n\n    MATCH (node:LabelOfNode)\n    WHERE node.property1 = value1 AND node.property2 = value2 \n    REMOVE node.property3\n    RETURN node\n\n#### Removing Label From a Node\n\n    MATCH (node:LabelOfNode)\n    WHERE node.property1 = value1 AND node.property2 = value2 \n    REMOVE node:LabelOfNode1[:LabelOfNode2[:LabelOfNode3...]]\n    RETURN node\n\n### SET\n\nUsing Set clause, you can add new properties to an existing Node or Relationship, and also add or update existing Properties values\n\n#### Setting a Property\n\n    MATCH (node:LabelOfNode)\n    WHERE node.property1 = value1 [AND node.property2 = value2[...]]\n    SET node.property = value\n    RETURN node\n\n#### Setting Multiple Properties\n\n    MATCH (node:LabelOfNode)\n    WHERE node.property1 = value1 [AND node.property2 = value2[...]]\n    SET node.property3 = value3, node.property4 = value4\n    RETURN node\n\n#### Setting a Label on a Node\n\n    MATCH (node:LabelOfNode)\n    WHERE node.property1 = value1 [AND node.property2 = value2[...]]\n    SET node:LabelOfNode\n    RETURN node\n\n#### Setting Multiple Labels on a Node\n\n    MATCH (node:LabelOfNode)\n    WHERE node.property1 = value1 [AND node.property2 = value2[...]]\n    SET node:LabelOfNode1, LabelOfNode2\n    RETURN node\n\n### FOREACH\n\nThe FOREACH clause is used to update data within a list whether components of a path, or result of aggregation.\n\n    MATCH p = (node1)-[*]->(node2) \n    WHERE node1.property1 = value1 AND node2.property2 = value2 \n    FOREACH (n IN nodes(p)| SET n.marked = TRUE)\n\n### Index\n\nA database index is a redundant copy of some of the data in the database for the purpose of making searches of related data more efficient. This comes at the cost of additional storage space and slower writes.\n\n#### Creating an Index\n\n    CREATE INDEX ON:LabelOfNode(property[,properties..])\n\n#### Deleting an Index\n\n    DROP INDEX ON:LabelOfNode(property[,properties..])\n\n### Constraint\n\nConstraint enforce data integrity, it can be applied to either nodes or relationships.\n\n#### Unique node property constraints\n\nUnique property constraints is a rule that ensures that property values are unique for all nodes with a specific label\n\n    CREATE CONSTRAINT ON (n:LabelOfNode) ASSERT n.property IS UNIQUE\n    DROP CONSTRAINT ON (n:LabelOfNode) ASSERT n.property IS UNIQUE\n\n#### Node property existence constraints\n\n    CREATE CONSTRAINT ON (n:LabelOfNode) ASSERT exists(n.property)\n    DROP CONSTRAINT ON (n:LabelOfNode) ASSERT exists(n.property)\n\n#### Relationship property existence constraints\n\n    CREATE CONSTRAINT ON ()-[r:Relationship_Of]-() ASSERT exists(r.property)\n    DROP CONSTRAINT ON ()-[r:Relationship_Of]-() ASSERT exists(r.property)\n\n### String Functions\n\n#### UPPER\n\n#### LOWER\n\n#### SUBSTRING\n\n#### Replace\n\n### Aggregation Function\n\n#### COUNT\n\nThe COUNT function is used to count the number of rows.\n\n    MATCH (node) RETURN COUNT(*)\n\nThe following state doesn't counts null values.\n\n    MATCH (node) RETURN COUNT(node)\n\n##### Group Count\n\nThe COUNT clause is also used to count the groups of relationship types. We can count by one direction or by both direction.\n\n    Match(node1)-[r]-(node2)  \n    RETURN type(r), count(*)\n\n    Match(node1)-[r]->(node2)  \n    RETURN type(r), count(*)\n\n#### MAX\n\n#### MIN\n\n#### SUM\n\n#### AVG\n\n### List Functions\n\n#### EXTRACT\n\nExtract returns a list containing the values resulting from an expression which has been applied to each element in a list list\n\n    EXTRACT(variable IN list | expression)\n\n#### FILTER\n\nFILTER returns a list lresult containing all the elements from a list list that comply with the given predicate.\n\n    FILTER(variable IN list WHERE predicate)\n\n#### KEYS\n\nKEYS returns a list containing the string representations for all the property names of a node, relationship, or map.\n\n    KEYS(expression)\n\n#### LABELS\n\nLABELS returns a list containing the string representations for all the labels of a node.\n\n    LABELS(node)\n\n#### NODES\n\nNODES returns a list containing all the nodes in a path.\n\n    NODES(path)\n\n#### RANGE\n\nRANGE returns a list comprising all integer values within a range bounded by a start value start and end value end, where the difference step between any two consecutive values is constant\n\n    RANGE(start, end [, step])\n\n#### REDUCE\n\nREDUCE returns the value resulting from the application of an expression on each successive element in a list in conjunction with the result of the computation thus far.\n\n    REDUCE(accumulator = initial, variable IN list | expression)\n\n#### RELATIONSHIPS\n\nRELATIONSHIPS returns a list containing all the relationships in a path.\n\n    RELATIONSHIPS(path)\n\n#### REVERSE\n\nREVERSE returns a list in which the order of all elements in the original list have been reversed.\n\n    REVERSE(original)\n\n#### TAIL\n\nTAIL returns a list lresult containing all the elements, excluding the first one, from a list list.\n\n    TAIL(list)\n\n### Scalar functions\n\n#### COALESCE\n\nCOALESCE returns the first non-null value in the given list of expressions.\n\n    COALESCE(expression [, expression]*)\n\n#### STARTNODE\n\nSTARTNODE returns the start node of a relationship.\n\n    STARTNODE(relationship)\n\n#### ENDNODE\n\nENDNODE returns the end node of a relationship.\n\n    ENDNODE(relationship)\n\n#### HEAD\n\nHEAD returns the first element in a list.\n\n    HEAD(list)\n\n#### LAST\n\nLAST returns the last element in a list.\n\n    LAST(expression)\n\n#### SIZE\n\nSIZE returns the number of elements in a list or set of results or string.\n\n    SIZE(list or pattern expression or string)\n\n#### LENGTH\n\nLENGTH returns the length of a path.\n\n    LENGTH(path)\n\n#### ID\n\nID returns the id of a relationship or node.\n\n    MATCH (n) WHERE ID(n)=id RETURN n\n\n#### TYPE\n\nTYPE returns the string representation of the relationship type.\n\n    TYPE(relationship)\n\n#### PROPERTIES\n\nPROPERTIES returns a map containing all the properties of a node or relationship.\n\n    PROPERTIES(expression)\n\n#### RANDOMUUID\n\n#### TIMESTAMP\n\n#### TOBOOLEAN\n\n#### TOFLOAT\n\n#### TOINTEGER\n\n### [Procedures](https://neo4j.com/docs/operations-manual/current/reference/procedures/#ref-procedure-reference-community-edition)\n\n#### Check which procedures are available\n\n    CALL dbms.procedures()\n\n* db.indexes() List all indexes in the database.\n* db.labels() List all labels in the database\n* db.propertyKeys() List all property keys in the database.\n* db.relationshipTypes() List all relationship types in the database.\n* db.schema() Show the schema of the data.\n* db.constraints() List all constraints in the database.\n\n## Data Import\n\nWe can use [LOAD CSV](https://neo4j.com/docs/cypher-manual/current/clauses/load-csv/) to batch imports of large amouts of data from CSV files. We can use [Neo4j ETL Tool](https://neo4j.com/developer/neo4j-etl/) or Neo4j Import tool.\n\n    LOAD CSV FROM \"url\" AS row FIELDTERMINATOR \";\"\n\n## Query management\n\n### [Profiling a query](https://neo4j.com/docs/cypher-manual/current/query-tuning/how-do-i-profile-a-query/)\n\n#### Explain\n\nIf you want to see the execution plan but not run the statement.\n\n#### PROFILE\n\nIf you want to run the statement and see which operators are doing most of the work, use PROFILE. This will run your statement and keep track of how many rows pass through each operator, and how much each operator needs to interact with the storage layer to retrieve the necessary data.\n\n### Kill Queries\n\nWhen you have a query that is taking too long to execute on your system, You can execute\n\n    CALL dbms.listQueries()\n\nWhich return the list of queries currently running. And then\n\n    CALL dbms.killQuery('query-id')\n\n## UIs\n\n### [Neo4j Desktop](https://neo4j.com/developer/neo4j-desktop/)\n\nIn Neo4j Desktop, you can only start a single database.\n\n#### Plugin\n\nIf you need additional functionality from a specialized library, you can add the library as a plugin for your project.\n\n### Neo4j Browser\n\nYou can only use Neo4j Browser to connected to a running Neo4j instance and you can only connect to a single database at a time.\n\n#### Browser Sync\n\nWhen you log in to Browser Sync from your Neo4j Browser, you can add folders and Cypher scripts in the cloud for use in a different Neo4j Browser session.\n\n### Neo4j Bloom\n\n## [Neo4j Sandbox](https://neo4j.com/sandbox-v2/)\n\nIt is a temporary Neo4j instance in the cloud where you can access a database for 3 to 10 days. By default it will also install APOC, Graph Algorithms and GraphQL.\n\n## Neo4j [Cluster](https://neo4j.com/docs/operations-manual/current/clustering/)\n\nNeo4j clusters allow your application to be highly available so that if a server goes down, another server will take over and also highly scalable so that application that read the data can be distributed in many places.",
        "slug": "Neo4j-Graph-Database",
        "date": "2019-10-11 23:37:29",
        "lang": "en",
        "tags": ["Database", "NoSQL", "Neo4j", "Graph Database", "CQL"],
        "path": "/Neo4j-Graph-Database/index.html"
      },
      {
        "title": "Lehman Laws",
        "markdownContentSource": "\nThe Lehman laws describe a balance between forces driving new developments on one hand, and forces that slow down progress on the other hand.\n\nLehman distinguishs between three categories of software:\n\n* A S-program (**specified**) is written according to an exact specification\n* A P-program (**procedural**) is written to implement certain procedures that completely determine what the program can do\n* An E-program (**evolving**) is written to perform some real-world activity, such a program needs to adapt to varying requirements and circumstances in that environment\n\nThe laws are said to apply only to the last category of systems\n\n## Continuing Change\n\nAn E-type system must be continually adapted or it becomes progressively less satisfactory.\n\n## Increasing Complexity\n\nAs an E-type system evolves, its complexity increases unless work is done to maintain or reduce it.\n\n## Self Regulation\n\nE-type system evolution processes are self-regulating with the distribution of product and process measures close to normal.\n\n## Conservation of Organisational Stability (invariant work rate)\n\nThe average effective global activity rate in an evolving E-type system is invariant over the product's lifetime.\n\n## Conservation of Familiarity\n\nAs an E-type system evolves, the average incremental growth remains invariant as the system evolves.\n\n## Continuing Growth \n\nThe functional content of an E-type system must be continually increased to maintain user satisfaction over its lifetime.\n\n## Declining Quality \n\nThe quality of an E-type system will appear to be declining unless it is rigorously maintained and adapted to operational environment changes.\n\n## Feedback System \n\nE-type evolution processes constitute multi-level, multi-loop, multi-agent feedback systems and must be treated as such to achieve significant improvement over any reasonable base.",
        "slug": "Lehman-laws",
        "date": "2019-05-12 21:53:33",
        "lang": "en",
        "tags": [
          "Lehman laws",
          "Software Evolution",
          "Agile Software Development"
        ],
        "path": "/Lehman-laws/index.html"
      },
      {
        "title": "Survivorship bias",
        "markdownContentSource": "\n# Introduction\n\nSurvivorship bias or survival bias is the logical error of concentrating on the people or things that made it past some selection process and overlooking those that did not, typically because of their lack of visibility. This can lead to false conclusions in several ways. It is a form of selection bias. It is a form of selection bias.\n\n# History\n\nDuring World War II, the statistician Abraham Wald took survivorship bias into his calculations when considering how to minimize bomber losses to enemy fire. Researchers from the Center for Naval Analyses had conducted a study of the damage done to aircraft that had returned from missions, and had recommended that armor be added to the areas that showed the most damage. Wald noted that the study only considered the aircraft that had survived their missions—the bombers that had been shot down were not present for the damage assessment. The holes in the returning aircraft, then, represented areas where a bomber could take damage and still return home safely. Wald proposed that the Navy reinforce areas where the returning aircraft were unscathed[, since those were the areas that, if hit, would cause the plane to be lost.\n\n# Conclusion\n\nSurvivorship bias (or survivor bias) is studies on the remaining population are fallaciously compared with the historic average despite the survivors having unusual properties, mostly, the unusual property in question is a track record of success.",
        "slug": "Survivorship-bias",
        "date": "2019-03-25 22:57:23",
        "lang": "en",
        "tags": ["Operation Research", "Survivorship bias", "Selection bias"],
        "path": "/Survivorship-bias/index.html"
      },
      {
        "title": "Microeconomics",
        "markdownContentSource": "\n# Supply, demand, and market equilibrium\n\n## Introduction to economics\n\nHe being an economic actor, neither intends to promote the public interest, nor knows how much he is promoting it. By directing that industry, so that the industry in control of that individual actor in such a manner, as its produce may be of the greatest value. **He intends only his own gain**. And he led by an invisible hand to promote an end which was no part of his intention. By pursuing his own interest he frequently promotes that of the society more effectually than when he really intends to promote it. - Adam Smith 1776\n\nThis is the foundational ideas of capitalism. In competition, individual ambition serves the common good. The self-interested action could often lead to promote public interest.\n\nThe microeconomic is that people, individual actors are acting out of their own self-interest and the macroeconomic is that it might be good for the economy or for the nation as a whole. Microeconomis is essentially how actors make decisions of scarce resources and how does that affect prices and markets. Macroeconomis is the study of what happens at the aggregate to an economy from the millions of individual actors, and often focuses on policy-related questions. How does that affect the overall productivity when you do this policy.\n\n## Law of demand\n\nThe **law of demand** states that a higger price to a lower quantit demanded and that lower price leads to a higher quantity demanded. **Demand curves** and **demand schedules** are tools used to summarize the relationship between quantity demandded and price.\n\n### Demand for goods and services\n\nEconomists use the term **demand** to refer to the amount of some good or service consumers are willing and able to purchase at each price. Demand is based on needs and wants a consumer may be able to diferentiate between a need and a want, but from an economist's perspective they are the same thing. What a buyer pays for a unit of the specific good or service is called **price**. The total number of units purchased at that price is called the **quantity demanded**. A rise in price of a good or service almost always **decreases the quantity demanded**. Conversely, a fall in price will **increase the quantity demanded**. Econimists call this inverse relationship between price and quantity demanded the **law of demand**. The **law of demand** assumes that all other variables that affect demand are held constant.\n\n### Demand schedule  and demand curve\n\n* A demand schedule is a table that shows the quantity demanded at each price.\n* A demand curve is a graph that shows the quantity demanded at each price.\n\n### The difference between demand and quantity demanded\n\nThe demand mean the relationship between a range of prices and the quantities demanded at those prices. When economists talk about quantity demanded, they mean only a certain point on the demand.\n\n# Reference\n\n1. [Supply Demand Equilibrium](https://www.khanacademy.org/economics-finance-domain/microeconomics/supply-demand-equilibrium)",
        "slug": "Microeconomics",
        "date": "2019-01-31 22:53:35",
        "lang": "en",
        "tags": ["Microeconomics"],
        "path": "/Microeconomics/index.html"
      }
    ],
    "_nextI18Next": {
      "initialI18nStore": {
        "en": {
          "common": { "Next": "Next" },
          "navbar": {
            "Home": "Home",
            "Language": "Language",
            "Tags": "Tags",
            "en": "English",
            "es": "Spanish",
            "zh": "Chinese"
          }
        }
      },
      "initialLocale": "en",
      "ns": ["common", "navbar"],
      "userConfig": {
        "i18n": { "defaultLocale": "en", "locales": ["en", "es", "zh"] },
        "default": {
          "i18n": { "defaultLocale": "en", "locales": ["en", "es", "zh"] }
        }
      }
    }
  },
  "__N_SSG": true
}
