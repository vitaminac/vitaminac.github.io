{
  "pageProps": {
    "postData": {
      "title": "Java Concurrent Programming",
      "markdownContentSource": "\n## Introduction\n\nAt the beginning, a program ran from **start** to **finish**, it had access to **all resources** on the system. Nowadays with **operating systems** we are allowed to run various programs simultaneously in processes **independent of the hardware architecture** we have.\n\n### Reason\n\n1. Take advantage of the **IO operations waiting time** to do other tasks.\n2. A better strategy, sharing resources instead of waiting them.\n3. Divide into various **independent tasks**.\n\n### Process\n\nThe execution of a computer program. Each **process** had its own memory space for instructions and data. The same computer program can run multiple times, with some **overlap** and **simultaneously**.\n\n### Sequential Model\n\nThey executed the instructions **sequentially**, interacting with the outside to through I/O where it is **perfectly defined** what instruction will run next.\n\n![Sequential Model](sequential-processing.png)\n\n### Concurrent Process\n\nP1 and P2 are said to be two processes **concurrent** if the first instruction of one of them run between the first and the last instruction of the other. They can be independent, competing the resources or cooperative. The interations between concurrent processes are carried out through the **synchronization** and **communication**. The order of instructions is not guaranteed. The results of executions may vary.\n\n![Concurrent Process](concurrent-process.png)\n\n### Multiprocess\n\nThey run in different processors at the same time.\n\n![Multiprocess](multiprocess.png)\n\n### Multiprogramming\n\nThey are assigned to the same processor and execute in different time slots. You can take advantage of the processor while a process waits for input/output.\n\n![Multiprogramming](multiprogramming.png)\n\n### Race Condition\n\nIn the concurrent context, multiple control flows can access, modify the same resource at the same time and produces **unexpected result**.\n\n### Atomic Operation\n\nAn atomic statement is an instruction that execute as a single indivisible unit.\n\n#### Fine-Grained\n\nMachine Level Intruction\n\n#### Coarse-Grained\n\nA set of instruction execute in conjunction as a single unit.\n\n### Critical Section\n\nIt is **code segment** that **accesses shared variables** and has to be executed as an atomic action.\n\n#### Preprotocol/Postprotocol\n\nThe pre-protocol and post-protocol are the sequences of instructions that the processes must execute to ensure that the instructions in the critical section are executed in compliance with the requirements.\n\n### Mutual Exclusion\n\n**Mutual exclusion** is used to prevent **race conditions**. hence prevents simultaneous access to a shared resource. **Only one of the processes can access** the resource at same time and the others have to wait. When a process **releases** the exclusive access resource and another process was waiting, the waiting process will access the resource. Mutual exclusion allows creating coarse-grained atomic statements.\n\n## Abstractions\n\nWe use abstractions that allow us to abstract from details about system architecture.\n\n1. Each process is considered to **run on its own processor**.\n2. The relative speeds of each process are ignored, making it possible to consider only the sequences of instructions being executed.\n3. The sequences of execution of the **atomic actions** of all processes are considered to be **interleaved in a single sequence** completely without interference.\n\nAll the abstractions can be summarized as the study of the sequences of interleaved execution of the atomic instructions of the sequential processes.\n\n## Thread\n\nThey allow **different control flows** of a program **coexist** within the same process. They share resources like memory, but each has its **own program counter, stack and local variables**. They are sometimes called light processes, and many operating systems consider them the basic units planning. Operating Systems take care of scheduling threads and assigning to different processors as they're available to run simultaneously and asynchronous with respect to each other. They share **process stask**, so everyone has access to them, allowing share data more efficiently.\n\n* Create: When an instance of Thread is created (via new Thread(Runnable target)), it does not start executing right.\n* Start: The threads start running when invokes the **start()** method of the Thread class.\n\nA program ends its execution when\n\n* All its threads have finished their execution\n* The **System.exit()** method is executed\n\nA thread ends its execution\n\n* When all your statements have been executed\n* When an unchecked exception is raised (RuntimeException) in the **run()** method\n* User cancellation\n* Timeout\n* Events that trigger in other threads\n  * Interruption with **Thread.interrupt()**\n    * We should periodically check if another thread has interrupted this thread.\n    * We will receive InterruptedException if the theread is blocked.\n  * etc...\n\nWhen a thread finishs it should release resources appropriately, close connections, and leave objects in a stable state.\n\nIf A thread wants to wait for another thread to end then invokes the **join()** method on the Thread class object that represents that thread.\n\n### Daemon\n\nDaemon threads ends automatically when all non-daemon threads in a program have ended. A thread is a demon if the thread that creates it is also a demon. The **isDaemon()** and **setDaemon()** methods allow you to change this property of threads.\n\n### Priority\n\nThe priority of a new thread is the same as the parent thread. The SO will respect the Java thread priority as far as possible.\n\n* **setPriority(int p)**: Set the priority. Its value must be between Thread.MIN_PRIORITY and Thread.MAX_PRIORITY\n* **int getPriority()**: Returns the priority of the thread.\n\n### Group\n\nThreads can be grouped into groups represented by **ThreadGroup**. A ThreadGroup can have other groups of threads inside, creating a tree structure.\n\n* Limit the priority of the threads that contain\n* Manage certain properties of threads together\n\n### Properties\n\n* **Thread.currentThread()**\n* **.getName()**\n* **isAlive()**\n* **getState()**: Returns the state of the thread (new, executing, waiting ...)\n\n## Conditional Synchronization\n\nOccurs when one or more process must wait for a certain condition to be met to continue its execution. That condition must be set by another process.\n\n### Barrier Synchronization\n\n**Barrier** is a conditional synchronization in which the processes have to wait for the rest of the processes to reach the same point in order to continue their execution.\n\n## Properties of Corrections\n\nWe use them to judge if a concurrent algorithm is correct. They must be complied with in any possible intercalation of atomic instructions.\n\n### Safety\n\n1. **Mutual Exclusion**\n2. **Absense of Deadlock** (when multiples threads wait to each other and forms a cycle)\n\n### Liveness\n\nWe must avoid a parallel program enter a state in which it stops makeing forward progress.\n\n1. **Starvation**\n2. **Freedom from deadlock**\n3. **Absense of Livelock**\n\n### Absence of Unnecessary Delays\n\n### Fairness\n\n1. Linear Waiting\n2. FIFO\n3. Priority\n4. Random\n\n## Parallelism\n\nIn the sequential programming you take a sequential algorithm and specify it as a sequence of steps. The parallelism is the study of which of these steps can run in parallel with each and how they should be coordinated. We use some notation here. **fork** when followed by a statement that causes the parent task to create a new child task to execute the body of the asynchronously with the remainder of the parent task statement, and **join** that specifies at the end of finish scope you're guaranteed all asynchronous sub-tasks will have completed before can proceed. **fork** and **join** constructs may be arbitrarily nested.\n\n### Computation Graph (CGs)\n\nWe can use **computation graph** to model the task relationship between each task. The **computation graph** which model the **execution of a parallel program** as a **partially ordered set**. CGs consists of: \n\n* A set of vertices or nodes, in which each node represents a **step** consisting of an arbitrary sequential computation.\n* A set of directed edges that represent **ordering constraints among steps**.\n\nFor fork join programs, it is useful to partition the edges into three cases:\n\n1.  **Continue edges** that capture sequencing of steps within a task.\n2.  **Fork edges** that connect a fork operation to the first step of child tasks.\n3.  **Join edges** that connect the last step of a task to all join operations on that task.\n\nIt helps us reason about which statements can execute in parallel. We ask \"Is there a path of directed edges from one statement to another?\". So for example, there's path from S2 and S4. So that tells us that S2 and S4 cannot run in parallel with each other. But between S2 and S3, we can see there's a parallel execution that's possible, because these's no path of directed edges between S2 and S3. \n\n**CGs** can be used to define **data races**, an important class of bugs in parallel programs. We say that a data race occurs on location $L$ in a computation graph, $G$, if there exist steps $S_1$ and $S_2$ in $G$ such that there is no path of directed edges from $S_1$ to $S_2$ or from $S_2$ to $S_1$ in $G$, and both $S_1$ and $S_2$ read or write _L_ (with at least one of the accesses being a write, since two parallel reads do not pose a problem).\n\n#### Work and Span\n\nCGs can also be used to reason about the **performance** of a parallel program as follows:\n\n* Define  **WORK(G)** to be the sum of the execution times of all nodes in CG  **G**,\n* Define  **SPAN(G)** to be the length of a longest path in  $G$, when adding up the execution times of all nodes in the path. The longest paths are known as  **critical paths**.\n* Given the above definitions of **WORK** and **SPAN**, we define the **ideal parallelism** of Computation Graph $G$ as the ratio, $\\frac{WORK(G)}{SPAN(G)}$.\n* The ideal parallelism is an upper limit on the speedup factor that can be obtained from parallel execution of nodes in computation graph _G_. Note that ideal parallelism is only a function of the parallel program, and does not depend on the actual parallelism available in a physical computer.\n\nSo in this case, the **work** would be 1 plus 10 plus 10 plus 1. That's 22. And the **span** which is 12 in this case. So the **ideal parallelism** is 2\n\n![Computation Graph](computation-graph.png)\n\n#### Time of execution and Speedup\n\nWe defined $T_{P}$ as the execution time of a CG on $P$ processors. The definition of the execution time on $P$ processors actually depends on the schedule. Suppose we have greedy schedule in which a processor is not permitted to be idle if a CG node is available to be scheduled on it. Given any $P$ processors,  $Span = T_{\\infty} \\le T_P \\le T_{1} = Work$. We then defined the parallel speedup for a given schedule of a CG on $P$ processors as $\\text{Speedup}(P) = \\frac{T_1}{T_P}$​ and observed that $\\text{Speedup}(P)$ must be less than the number of processors $P$, and also less than the ideal parallelism, $\\frac{Work}{Span}$. Our goal in parallel algorithms is to generate computation graphs with ideal parallelism that's much larger than the number of processors that you have, so that you have the flexibility of running that parallel program on numerous processors.\n\n#### Amdahl's Law\n\nLet's assume that $q$ is the fraction of $WORK$ in a parallel program that must be executed **sequentially**, the **span** must be at least $q \\cdot \\text{Work}$, then the maximum speedup that can be obtained for that program for any number of processors $P$ is going to be bounded over by $\\text{Speedup}(P) \\le \\frac{1}{q}$.\n\nThis observation follows directly from a lower bound on parallel execution time $\\text{SPAN}(G) \\le T_p$. If fraction $q$ of $WORK(G)$ is sequential, it must be the case that $q \\cdot \\text{WORK}(G) \\le \\text{Span}(G) \\le T_p$. Therefore, $\\text{Speedup}(P) = \\frac{T_1}{T_P} \\le \\frac{\\text{WORK}(G)}{q \\cdot \\text{WORK}(G)} = \\frac{1}{q}$ since $T_1​ = WORK(G)$ for greedy schedulers.\n\nWhat they mean is that there's some part of the computation that's being done inherently sequentially that is going to limit the speedup. Amdahl’s Law reminds us to watch out for sequential bottlenecks both when designing parallel algorithms and when implementing programs on real machines. Even if $q=10\\%$ then best possible speedup must be $\\le 10 = \\frac{1}{q}$, regardless of the number of processors available.\n\n### Forkjoin\n\nParallel versions of classic algorithms divide and conquer. Divide a task into smaller sub-tasks that can be executed concurrently.\n\n#### Fork\n\nA task split itself into smaller subtasks a task into smaller sub-tasks which can be executed concurrently.\n\n![Fork](fork.png)\n\n### Join\n\nWhen a task has split itself up into subtasks, the task waits until the subtasks have finished executing. Once the subtasks have finished executing, the task may join all the results into one result.\n\n![Join](join.png)\n\n### ForkJoinPool\n\nForkJoinPool is similar to the ExecutorService but designed to work efficiently with fork/join task division.\n\n* **new ForkJoinPool()**: The default constructor will create a pool with as many threads as there are processors available.\n* **new ForkJoinPool(int parallelism)**: We can also specify the number of threads\n* **ForkJoinPool.commonPool()**: return a common pool\n\nThe order matter, we should create first with **fork()** and then **join()**.\n\n[Fork Join Sum](https://github.com/vitaminac/code/blob/9e9c769b30e3eb9c21e449b9b641a020250e76fb/rubbish/src/main/java/concurrente/Tema5.java#L193-L221)\n\nThe idea is that you perform all the computations in L and R in parallel, wait for them to complete and then proceed next.\n\n### MapReduce\n\nThe main idea of MapReduce is similar to Fork/Join.\n\n* Reduce task size and assign them to multiple computers concurrently.\n* The results are retrieved and integrated to create the final result.\n\n#### Map\n\nYou select one data set and transform it into another, where each element is divided into key value pairs.\n\n#### Reduce\n\nSelect Map output as input and combine those pairs into smaller sets of tuples\n\n![Map Reduce](map-reduce.png)\n\n## Future\n\nA Java Future represents the result of an asynchronous computation. When the asynchronous task is created, a Java Future object is returned. This Future object functions as a handle to the result of the asynchronous task. Once the asynchronous task completes, the result can be accessed via the Future object returned when the task was started.\n\n### Get Result\n\nAs mentioned earlier, a Java **Future** represents the result of an asynchronous task. To obtain the result, you call **get()** methods on the Future. If you call the get() method before the asynchronous task has completed, the get() method will block until the result is ready.\n\n### Functional Parallelism\n\nThe future is carefully defined to avoid the possibility of a race condition and it is suited for functional parallelism. So if we write our tasks as pure functions calls like $F(X)$ so that if you call $F(X)$ multiple times with the same input, you will always get the same output $Y$. From that observation, we see that if pure function $G$ and pure function $H$ only depend on the output of $F$ then these two computations could actually execute in either order. Future of $F$ act as a wrapper of the output value $F$ and it is final. If we pass the future of output value from $F$, then even if $G$ and $F$ run in parallel when they actually need the output value from $F$ they will get blocked and wait until the value is available. From the computation graph perspective, the join edges arise from the get operations of future object.\n\n![Future Get](Future-Get.png)\n\n## Memorization\n\nThe memoization pattern lends itself easily to parallelization using futures by modifying the memoized data structure X to store Future\\<X\\>\n\n## Determinism\n\n### Functional Determinism\n\nA parallel program is said to be **functionally deterministic** if it always computes the same answer when given the same input.\n\n### Structural Determinism\n\nThe idea behind **structural determinism** is it always computes the same computation graph, when given the same input.\n\n### Data Race\n\nA data race is an unsafe access to the same piece of data from two independently parallel executions without some mechanism in place to avoid the conflict. The presence of data races often leads to functional and/or structural nondeterminism because a parallel program with data races may exhibit different behaviors for the same input, depending on the relative scheduling and timing of memory accesses involved in a data race. In general, the **absence of data races is not sufficient to guarantee determinism**. However, all the parallel constructs **fork**, **join**, **future** were carefully selected to ensure if a parallel program is written using these constructs and is guaranteed to be **data-race freedom**  that's the **absence of data races**, then it implies both **functional determinism** and **structural determinism**.\n\n### Nondeterministic\n\nFurthermore, there may be cases of **nondeterministic** programs in which different executions with the same input may generate different outputs, but all the outputs may be **acceptable** in the context of the application.\n\n## Parallelism Loop\n\nThe most general way express parallelism loop is to think of each iteration of a parallel loop as an **fork** task, with a **join** construct encompassing all iterations.\n\n### Barriers in Parallel Loops\n\nThe barriers extend a parallel loop by dividing its execution into a sequence of **phases**. While it may be possible to write a separate **parallelism loop** for each phase, it is both more convenient and more efficient to instead insert barriers in a single **parallelism  loop**.\n\n### Iteration Grouping\n\nWe observed that this approach creates  **n** tasks, one per  **parallelism loop** iteration, which is wasteful when **n** is much larger than the number of available processor cores. To address this problem, we learned a common tactic used in practice that is referred to as  **iteration grouping**, and focuses on reducing the number of tasks created to be closer to the number of processor cores, to reduce the overhead of parallel execution. There are two well known approaches for iteration grouping: **block**  and **cyclic**. The **block** form maps consecutive iterations to the same group, whereas **cyclic** maps iterations in the same congruence class $i \\mod ng$ to the same group.\n\n## Phaser\n\n**Barrier** may take some computation time and it is ideal if we can overlap the computation with other computation and the **span** can be lower. So **phaser** come in, **arriveAndAwaitAdvance** can be used to implement a barrier through **phaser** object. To facilitate the **split-phase barrier** (also known as a **fuzzy barrier**) we use two separate APIs from Java Phaser class — **.arrive()** and **awaitAdvance()**. Together these two APIs form a barrier, but we now have the freedom to insert a computation to be performed in parallel with the barrier between the two calls.\n\n### Point-to-Point Synchronization with Phasers and Dataflow Synchronization\n\nWe can use multiple phasers to create the dependencies graph between them.\n\n### Pipeline Parallelism\n\nLet $n$ be the number of input items and $p$ the number of stages in the pipeline, $WORK = n × p$_ is the total work that must be done for all data items, and $SPAN = n + p − 1$ for the pipeline. Thus, the ideal parallelism is $WORK/SPAN = np / (n + p − 1)$. When $n$ is much larger than $p$ then the ideal parallelism approaches $PAR = p$ in the limit. The synchronization required for pipeline parallelism can be implemented as follows\n\n\t// Code for pipeline stage i\n\twhile ( there is an input to be processed ) {\n\t  // wait for previous stage, if any \n\t  if (i > 0) ph[i - 1].awaitAdvance(); \n\t  \n\t  process input;\n\t  \n\t  // signal next stage\n\t  ph[i].arrive();\n\t}\n\n## Producer/Consumer\n\n* **Producer**: produces a message.\n* **Consumer**: consumes and remove the message.\n* Each producer generates a single data each time.\n* A consumer can only consume one data.\n* All the product will be process.\n* You cannot consume the same product twice.\n\n## Reader/Writer\n\nSome threads may read and some may write, with the constraint that no thread may access the shared resource for either reading or writing while another thread is in the act of writing to it.\n\n* **Readers**: Processes which are not required to exclude one another. Any number of readers may simultaneously read the resource.\n* **Writers**: Processes which exclude all the other readers when writing a resource. As long as a reader is reading, no writer can access the DB.\n* Requirement: There may be several writers working, although these will have to be synchronized so that the writing is carried out one by one\n* Requirement: Writers have priority. No reader can access the DB when there are writers who wish to do so.\n\n## Dining Philosopher Problem\n\n* Preprotocol: The philosopher sits down in front of his plate and takes the forks on either side of his plate one by one.\n* Critical Section: Eat\n* Postprotocol: When finished, leave the two forks in their original position\n* Requirement: Every philosopher who eats, at some point is satisfied and ends\n* Requirement: A philosopher can only eat when he has both forks\n* Requirement: The forks are picked up and put down one by one\n* Requirement: Two philosophers cannot have the same fork simultaneously\n* Requirement: If several philosophers try to eat at the same time, one of them must succeed\n* Requirement: In the absence of competition, a philosopher who wants to eat must do so without unnecessary delay.\n\n### Deadlock\n\n[Deadlock](https://github.com/vitaminac/code/blob/194f39a2c674b803dad8fb64ff6897bd271fe955/judge/src/main/java/geeksforgeeks/concurrent/DiningPhilosopher.java#L18-L44)\n\n### Livelock\n\n[Livelock](https://github.com/vitaminac/code/blob/194f39a2c674b803dad8fb64ff6897bd271fe955/judge/src/main/java/geeksforgeeks/concurrent/DiningPhilosopher.java#L46-L81)\n\n### Solution\n\n[DiningPhilosopher solution](https://github.com/vitaminac/code/blob/194f39a2c674b803dad8fb64ff6897bd271fe955/judge/src/main/java/geeksforgeeks/concurrent/DiningPhilosopher.java#L83-L112)\n\n## Volatile\n\nReading and writing **simple type** variable are atomic instructions, unless the variable is of type **long** or **double**. For modern computer, usually there is a cache for each processor. Although the change to primitive variable is atomic, the write is not neccesary **visible** to other processor. To do so, you have to declare the variable as **volatile** which guarantee the visibility for all the primitive types for all processors. \n\nIt is not necessary to use volatile if the threads use some synchronization method. If the shared resources are mutually exclusive with semaphore, the correct values are guaranteed to be read. If one process writes an resource and unlocks another process, the other process will read the written value.\n\n## Busy Waiting\n\nWhen we use volatile variable for synchronization. Busy waiting is very inefficient and generally should be avoided.\n\n### Dekker's algorithm\n\n#### Mandatory Alternation - Unnecessary Delays\n\nA turn variable is used that indicates the process that can enter the critical section.\n\n    private static volatile int turn;\n\n    private static void p1() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            while (turno != 1);\n            \n            /* Critical Section */\n\n            /* Postprotocol */\n            turn = 2;\n            \n            /* No critical Section */\n        }\n    }\n\n    private static void p2() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            while (turn != 2);\n\n            /* Critical Section */\n\n            /* Postprotocol */\n            turn = 1;\n\n            /* No critical Section */\n        }\n    }\n\n#### No Mutual Exclusion\n\nWe can use a boolean variable for each process that indicates if said process is in the critical section.\n\n    private static volatile boolean p1cs;\n    private static volatile boolean p2cs;\n\n    private static void p1() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            while (p2cs);\n            p1cs = true;\n\n            /* Critical Section */\n\n            /* Postprotocol */\n            p1cs = false;\n\n            /* No critical Section */\n        }\n    }\n\n    private static void p2() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            while (p1cs);\n            p2cs = true;\n\n            /* Critical Section */\n\n            /* Postprotocol */\n            p2cs = false;\n            \n            /* No critical Section */\n        }\n    }\n\nBoth processes can execute the critical section instructions at the same time.\n\n#### Deadlock\n\nLet we request the access before we enter the critical section.\n\n    private static volatile boolean intent_p1;\n    private static volatile boolean intent_p2;\n    \n    private static void p1() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            intent_p1 = true;\n            while (intent_p2);\n\n            /* Critical Section */\n            \n            /* Postprotocol */\n            intent_p1 = false;\n\n            /* No critical Section */\n        }\n    }\n\n    private static void p2() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            intent_p2 = true;\n            while (intent_p1);\n\n            /* Critical Section */\n            \n            /* Postprotocol */\n            intent_p2 = false;\n\n            /* No critical Section */\n        }\n    }\n\n#### Starvation - Livelock - No Mutual Exclusion\n\nYields your right to enter the critical section if you discover that there is competition with another process.\n\n    private static volatile boolean intent_p1;\n    private static volatile boolean intent_p2;\n\n    private static void p1() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            intent_p1 = true;\n            while (intent_p2) {\n                intent_p1 = false;\n                intent_p2 = true;\n            }\n\n            /* Critical Section */\n            \n            /* Postprotocol */\n            intent_p1 = false;\n\n            /* No critical Section */\n        }\n    }\n\n    private static void p2() {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            intent_p2 = true;\n            while (intent_p1) {\n                intent_p2 = false;\n                intent_p1 = true;\n            }\n\n            /* Critical Section */\n            \n            /* Postprotocol */\n            intent_p2 = false;\n\n            /* No critical Section */\n        }\n    }\n\nAlternatively giving the right and never enter the critical section.\n\n#### Final Solution\n\nIt is a combination of the 1st and 4th approach.\n\n[Dekker's algorithm](https://github.com/vitaminac/code/blob/31d59e0182225178c7eec5631b3661adf3cf650a/judge/src/main/java/geeksforgeeks/concurrent/DekkerAlgorithm.java)\n\n## Semaphore\n\nA semaphore is a Abstract Data Type that support two operations.\n\n1. The **acquire()** method decreases the number of semaphore permissions or hangs until someone release a permission when is zero.\n2. The **release()** method increases the number of semaphore permissions. If there are blocked processes at the semaphore, unblock one of them FIFO or Randomly and continues its execution\n\n* **acquire()**: Acquires the given number of permits from this semaphore, blocking until all are available, or the thread is interrupted.\n* **acquireUninterruptibly()**: Acquires a permit from this semaphore, blocking until one is available.\n* **tryAcquire()**: Acquires a permit from this semaphore, only if one is available at the time of invocation. return false if it is unavailable.\n* **tryAcquire(long timeout, TimeUnit unit)**: Acquires a permit from this semaphore, if one becomes available within the given waiting time and the current thread has not been interrupted, return false if no one is available within the timeout.\n* **drainPermits()**: Acquires and returns all permits that are immediately available.\n* **getQueueLength()**: Returns an estimate of the number of threads waiting to acquire.\n* **hasQueuedThreads()**: Queries whether any threads are waiting to acquire.\n\n### Development Process\n\n1. Define process architecture (number of processes and type)\n2. Sequential implementation\n3. Determine sync points in code\n   1. Conditional Synchronization or Mutual Exclusion\n   2. Number of Semaphores Needed\n   3. Can all processes be blocked together?\n   4. Can any of them be unlocked?\n4. Define necessary semaphores to control synchronization and write acquire() and release()\n5. Variable management\n   1. Initialization of boolean and counters\n   2. Under Mutual Exclusion if they are shared\n\n### Conditional Synchronization With Semaphore\n\n    private static volatile boolean continue = false;\n    private static Semaphore semaphore = new Semaphore(0);\n\n    private static void p1() {\n        /* task p1.1 */\n        continue = true;\n        /* task p1.2 */\n    }\n\n    private static void p2() {\n        /* task p2.1 */\n        while (!continue);\n        /* task p2.2 */\n    }\n\n    private static void p1_semaphore() {\n        /* task p1.1 */\n        semaphore.release();\n        /* task p1.2 */\n    }\n\n    private static void p2_semaphore() {\n        /* task p2.1 */\n        semaphore.acquire();\n        /* task p2.2 */\n    }\n\n### Mutual Exclusion with Semaphore\n\n    private static Semaphore sem = new Semaphore(1);\n\n    private static void p() throws InterruptedException {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            sem.acquire();\n\n            /* Critical Section */\n\n            /* Postprotocol */\n            sem.release();\n\n            /* No Critical Section */\n        }\n    }\n\n### Barrier with Semaphore First Example\n\nThe processes have to wait for everyone to have written the letter 1 before writing the 2.\n\n#### First Incorrect Approach\n\n    private static volatile int nProcess = 0;\n    private static Semaphore sem = Semaphore(0);\n    private static void p() throws InterruptedException{\n        System.out.println(\"1\");\n        nProcess += 1;\n        if (nProcess < N_PROCESSES) sem.acquire();\n        else for (int i = 0; i < N_PROCESSES - 1; i++) sem.release();\n        System.out.println(\"2\");\n    }\n\n**nProcess** is not under mutual exclusion\n\n#### Second Incorrect Approach\n\n    private static volatile int nProcess = 0;\n    private static Semaphore sem = Semaphore(0);\n    private static Semaphore lock = Semaphore(1);\n    private static void p() throws InterruptedException{\n        System.out.println(\"1\");\n        lock.acquire();\n        nProcess += 1;\n        lock.release();\n        if (nProcess < N_PROCESSES) sem.acquire();\n        else for (int i = 0; i < N_PROCESSES - 1; i++) sem.release();\n        System.out.println(\"2\");\n    }\n\nQuery nProcess outside the mutual exclusion may result multiple processes release the sem which will leave the sem in the unpredictable state.\n\n#### First Correct Approach\n\n    private static volatile int nProcess = 0;\n    private static Semaphore sem = Semaphore(0);\n    private static Semaphore lock = Semaphore(1);\n    private static void p() throws InterruptedException{\n        System.out.println(\"1\");\n        lock.acquire();\n        nProcess += 1;\n        if (nProcess < N_PROCESSES) {\n            lock.release();\n            sem.acquire();\n        }\n        else {\n            lock.release();\n            for (int i = 0; i < N_PROCESSES - 1; i++) {\n                sem.release();\n            }\n        }\n        System.out.println(\"2\");\n    }\n\nWe release after the query\n\n#### Second Correct Approach\n\n    private static volatile int nProcess = 0;\n    private static Semaphore sem = Semaphore(0);\n    private static Semaphore lock = Semaphore(1);\n    private static void p() throws InterruptedException{\n        System.out.println(\"1\");\n        lock.acquire();\n        nProcess += 1;\n        if (nProcess == N_PROCESSES) {\n            for (int i = 0; i < N_PROCESSES; i++) {\n                sem.release();\n            }\n        }\n        lock.release(); \n        sem.acquire();\n        System.out.println(\"2\");\n    }\n\n### Barrier with Semaphore\n\n[SemaphoreBarrier](https://github.com/vitaminac/code/blob/a286df7d1569887f7adbafd4d2aec83ff0ee4c3a/core/src/main/java/core/concurrent/SemaphoreBarrier.java)\n\n### K Mutual Exclusion\n\nWhen the number of processes that can run the critical section at once is N > 1, it is implemented with semaphore assigning initially a K value at the semaphore.\n\n    private static final int N;\n    private static final Semaphore sem = new Semaphore(N);\n\n    private static void p() throws InterruptedException {\n        for (int i = 0; i < N; i++) {\n            /* Preprotocol */\n            sem.acquire();\n            \n            /* Critical Section */\n\n            /* Postprotocol */\n            sem.release();\n            \n            /* No Critical Section */\n        }\n    }\n\n### Buffer\n\nIn producer-consumer problem\n\n* A buffer will be used to store the data produced before being consumed\n* Producer should block if the buffer is full\n* Consumers should block when they have no data to consume.\n* Control variables must be under mutual exclusion\n\n[SemaphoreArrayBlockingQueue](https://github.com/vitaminac/code/blob/e34b48b84814921fe2947e9123a1d138227c4762/core/src/main/java/core/concurrent/SemaphoreArrayBlockingQueue.java)\n\n### Reader/Writer With Semaphore\n\n[Semaphore_DB_Lock](https://github.com/vitaminac/code/blob/c47e3f3e58e612ed98dbd1f1cb5d2a53c26f8350/core/src/main/java/core/concurrent/Semaphore_DB_Lock.java)\n\n## Lock\n\n### Structured locks\n\nStructured locks can be used to enforce mutual exclusion and avoid data races. A major benefit of structured locks is that their acquire and release operations are implicit, since these operations are automatically performed by the Java runtime environment when entering and exiting the scope of a **synchronized** statement or method, even if an exception is thrown in the middle.\n\nThe synchronized keyword is put into the method and current object acts as a lock. Synchronized statements and methods are reentrant. If a thread that has acquired the lock in a synchronized method, calls another synchronized method, picks up the lock again, it does not stay locked. This allows the reuse of synchronized methods. But they have some limitations.\n\n* A synchronized block makes no guarantees about the sequence in which threads waiting to entering it are granted access.\n* Mutual exclusion cannot be acquired in one method and released in another.\n* You cannot specify a maximum waiting time to acquire the lock.\n* You cannot create an extended mutual exclusion.\n\n#### Synchronized Block\n\n\n    private static int x = 0;\n    private static Object lock = new Object();\n    \n    private static void inc() {\n        for (int i = 0; i < N; i++) {\n            synchronized (lock) {\n                x = x + 1;\n            }\n        }\n    }\n\n    private static void dec() {\n        for (int i = 0; i < N; i++) {\n            synchronized (lock) {\n                x = x - 1;\n            }\n        }\n    }\n\n#### Synchronized Method\n\n    public class Counter {\n        private int x = 0;\n        \n        public synchronized void inc() {\n            x = x + 1;\n        }\n        \n        public int getValue() {\n            return x;\n        }\n    }\n\n### Lock Interface\n\n**java.util.concurrent.locks.Lock** provide explicit **lock()** and **unlock()** operations on unstructured locks can be used to support a hand-over-hand locking pattern that implements a non-nested pairing of lock/unlock operations which cannot be achieved with **synchronized** statements/methods.\n\n    lock.lock();\n    try {\n        /* Critical Section */\n    } finally {\n        lock.unlock()\n    }\n\n* **lock()**: locks the Lock instance if possible. If the Lock instance is already locked, the thread calling lock() is blocked until the Lock is unlocked.\n* **lockInterruptibly()**: Acquires the lock unless the current thread is interrupted.\n* **tryLock()**: Acquires the lock only if it is not held by another thread at the time of invocation.\n* **tryLock(long time, TimeUnit unit)**: Acquires the lock if it is not held by another thread within the given waiting time and the current thread has not been interrupted.\n* **unlock()**: Unlocks the Lock instance. Typically, a Lock implementation will only allow the thread that has locked the Lock to call this method.\n\n### ReentrantLock\n\n* **ReentrantLock(boolean fair)**: Creates an instance of ReentrantLock with the given fairness policy.\n* **getQueueLength()**: Returns an estimate of the number of threads waiting to acquire this lock.\n* **isHeldByCurrentThread()**: Queries if this lock is held by the current thread.\n* **isLocked()**: Queries if this lock is held by the current thread.\n* **isFair()**: Returns true if this lock has fairness set true.\n\n### ReadWriteLock\n\nImplementation of extended mutual exclusion of readers and writers.  It allows multiple threads to read a certain resource, but only one to write it, at a time.\n\n* **Read Lock**: If no threads have locked the ReadWriteLock for writing, and no thread have requested a write lock (but not yet obtained it). Thus, multiple threads can lock the lock for reading.\n* **Write Lock**: If no threads are reading or writing. Thus, only one thread at a time can lock the lock for writing.\n\n\n    private ReadWriteLock readWriteLock = new ReentrantReadWriteLock();\n    private void reader() {\n        readWriteLock.readLock().lock();\n        \n        // multiple readers can enter this section\n        // if not locked for writing, and not writers waiting\n        // to lock for writing.\n\n        readWriteLock.readLock().unlock();\n    }\n    private void writer() {\n        readWriteLock.writeLock().lock();\n\n        // only one writer can enter this section,\n        // and only if no threads are currently reading.\n\n        readWriteLock.writeLock().unlock();\n    }\n\n### StampedLock\n\n## Monitor\n\n* Provides mutual exclusion\n* Provides conditional synchronization allowing one process to be locked and another process to unlock it.\n* Provide a mechanism for threads to temporarily give up for a exclusive access in order to wait for some condition to be met, before regaining exclusive access and resuming their task.\n\nA monitor consists of a **lock** and **condition variables**. In Java every object can act as a **monitor**, the mutual exclusion is define by mutual exclusion by **synchronized** on the object and the condition variables are provided with following method in the synchronized object.\n\n* **wait()**: Causes the current thread to wait until another thread invokes the **notify()** method or the **notifyAll()** method for this object\n* **notify()**: Wakes up a single thread that is waiting on this object's monitor.\n* **notifyAll()**: Wakes up all threads that are waiting on this object's monitor.\n\nThose methods can only be invoked within a synchronized statement or synchronized method. When a thread is locked in the condition variables the mutual exclusion is released automatically.\n\n[MonitorBarrier](https://github.com/vitaminac/code/blob/b55abe7deed23e639ca6b55e671b3728b43f307f/core/src/main/java/core/concurrent/MonitorBarrier.java)\n\nIt is not possible to notify a particular thread. Because unexpected activations can occur, we have to implement a protection mechanism in **wait()**. It is recommended to block the threads with a guard condition when notify all threads and all will be blocked again except the one that meets the condition.\n\nWith locks we can have several conditions rather the single one, we can create a condition with **newCondition()** that return a object with following methods.\n\n* **await()**\n* **signal()**\n* **signalAll()**\n\nIt is necesarry to have **await** inside a loop due to the expected activation.\n\n[LockBarrier](https://github.com/vitaminac/code/blob/62a4cd718375cfb4725a671a7212a2583be2df5c/core/src/main/java/core/concurrent/LockBarrier.java)\n\n## Exchanger\n\nExchanger allows two threads to exchange objects with each other\n\n    public V exchange (V e) throws InterruptedException\n\n* The first thread that executes exchange() is blocked until the other thread also executes that method.\n* When the second thread executes exchange() both threads exchange the values passed as a parameter and continue their execution.\n\n------------------------------------------------------------------------------\n\n    private Exchanger<Integer> exchanger = new Exchanger<Integer>();\n\n------------------------------------------------------------------------------\n\n    public static void producer() throws InterruptedException {\n        for(int i = 0; i < N; i++) {\n            exchanger.exchange(i);\n        }\n    }\n\n------------------------------------------------------------------------------\n\n    public static void consumer() throws InterruptedException {\n        for(int i = 0; i < N; i++) {\n            exchanger.exchange(i);\n        }\n    }\n\n## CountDownLatch\n\nOne or more threads invoke **await()** and that blocks them waiting for **countDown()** is invoked as many times as specified in the object's constructor.\n\n## CyclicBarrier\n\n**CyclicBarrier(int parties, Runnable barrierAction)**: Number of barrier threads and code executed when the barrier is tripped.\n\n**await()**: blocks the thread until the other threads arrive.\n\n## Concurrent Collections\n\nAll actions performed on the collection must be synchronized.\n\n    public String deleteLast(List<String> list) {\n        synchronized (list) {\n            int lastIndex = list.size() - 1;\n            return list.remove(lastIndex);\n        }\n    }\n\n### Synchronization Wrappers\n\n* Collections.synchronizedList\n* Collections.synchronizedMap\n* Collections.synchronizedSet\n\n### BlockingQueue\n\nIt is a **thread-safe** queue which is thread safe to put elements into, and take elements out of from. If the queue is full, the **put(E)** methods are locked until there is space. If the queue is empty then **take()** will block the thread until one element is available.\n\n![BlockingQueue](blocking-queue.png)\n\n* **put(e)**: Blocks until operation can be performed\n* **offer(e, time, unit)**: Blocks and returns false if the operation is not performed in the indicated time\n* **take()**: Blocks until operation can be performed\n* **poll(time, unit)**: Bloquea y devuelve null si no se realiza la operación en el tiempo indicado.\n\nImplementations:\n\n* ArrayBlockingQueue\n* LinkedBlockingQueue\n* PriorityBlockingQueue\n* SynchronousQueue\n  * just allow a single element\n* DelayQueue\n  * keeps items in the queue for a specified time\n* LinkedBlockingDeque\n* TransferQueue\n  * calling **transfer(E e)** will guarantee that all existing queue items will be processed before the transferred item\nan element\n  * LinkedTransferQueue\n\n\n    public class Producer implements Runnable {\n\n        protected BlockingQueue queue = null;\n\n        public Producer(BlockingQueue queue) {\n            this.queue = queue;\n        }\n\n        public void run() {\n            try {\n                queue.put(\"1\");\n                Thread.sleep(1000);\n                queue.put(\"2\");\n                Thread.sleep(1000);\n                queue.put(\"3\");\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n\n    public class Consumer implements Runnable {\n\n        protected BlockingQueue queue = null;\n\n        public Consumer(BlockingQueue queue) {\n            this.queue = queue;\n        }\n\n        public void run() {\n            try {\n                System.out.println(queue.take());\n                System.out.println(queue.take());\n                System.out.println(queue.take());\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            }\n        }\n    }\n\n### ConcurrentMap\n\nIt represents a extended Map's interface which is capable of handing concurrent access to it.\n\n* **compute(K key, remappingFunction)**: map for the specified key and its current mapped value (or null if there is no current mapping\n* **putIfAbsent(K key, V value)**: If the specified key is not already associated with a value, associate it with the given value.\n* **replace(K key, V value)**: Replaces the entry for a key only if currently mapped to some value.\n* **replace(K key, V oldValue, V newValue)**: Replaces the entry for a key only if currently mapped to a given value.\n\nImplementations:\n\n* **ConcurrentHashMap**\n* **ConcurrentSkipListMap**\n\n### CopyOnWrite\n\nCopy-on write collections allow secure concurrent access because they are unchanging objects. When they are modified, a copy is created for subsequent readings. As it is expensive to make the copy when it is modified, this structure is designed for cases where reads are much more common than writes\n\n#### CopyOnWriteArrayList\n\n#### CopyOnWriteArraySet\n\n### ConcurrentSkipListSet\n\n### ConcurrentLinkedQueue\n\n### ConcurrentLinkedDeque\n\n## Thread Pool\n\nA thread pool is responsible for managing the execution of a group of threads. They contain a queue that is responsible for managing and waiting for tasks to run. Threads are running continuously, checking for a new task in the queue to perform.\n\n![Thread Pool](ThreadPool.png)\n\n### Executor\n\nInterface that allows launching new tasks.\n\n### ExecutorService\n\nImplements the Executor interface, adding the functionality of thread life cycle management\n\n* **newSingleThreadExecutor()**: Create a single thread.\n* **newFixedThreadPool**: Create a thread pool that contains the number of threads we need.\n* **newCachedThreadPool()**\n  * Create a thread pool that will launch new threads when necessary, but will try to reuse old ones when they become available.\n  * They are recommended for applications that perform very short tasks\n  * Threads that have not been used for more than 60 seconds end and are removed from the pool.\n* Each task is executed using the **execute()** method of the ExecutorService that we have created.\n* It is necessary to finish each ExecutorService that we use with**shutdown()**.\n\n    ExecutorService executor = Executors.newSingleThreadExecutor();\n    for (int i = 0; i < 10; i++) {\n        executor.execute(() -> Thread.sleep(100));\n    }\n    executor.shutdown();\n\n\nIf we need a task to return a result, we have to use **Callable** interface. We use **submit()**, which execute the task and it a Future\\<T\\>.\n\n    public class RunnableExample implements Runnable {\n        @Override\n        public void run() {\n            // Task code\n        }\n    }\n    public class CallableExample implements Callable<T> {\n        @Override\n        public T call() throws Exception {\n            // Task code\n            return null;\n        }\n    }\n\n#### Shutdown\n\n The ExecutorService needs to be shut down when you are finished using it. If not, it will keep the JVM running, even when all other threads have been shut down.\n\n### ScheduledExecutorService\n\nThe java.util.concurrent.ScheduledExecutorService is an ExecutorService which can schedule tasks to run after a delay, or to execute repeatedly with a fixed interval of time in between each execution.\n\n    ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(4);\n\n#### schedule(Callable task, long delay, TimeUnit timeunit)\n\nThis method schedules the given Callable for execution after the given delay.\n\nThe method returns a ScheduledFuture which you can use to either cancel the task before it has started executing, or obtain the result once it is executed.\n\n#### schedule(Runnable task, long delay, TimeUnit timeunit)\n\nThis method works like the method version taking a Callable as parameter, except a Runnable cannot return a value, so the ScheduledFuture.get() method returns null when the task is finished.\n\n#### scheduleAtFixedRate(Runnable task, long initialDelay, long period, TimeUnit timeunit)\n\nThis method schedules a task to be executed **periodically**. The task is executed the first time after the **initialDelay**, and then recurringly every time the period expires.\n\nIf any execution of the given task throws an exception, the task is no longer executed. If no exceptions are thrown, the task will continue to be executed until the **ScheduledExecutorService is shut down**.\n\nIf a task takes longer to execute than the period between its scheduled executions, the next execution will start after the current execution finishes. The scheduled task will not be executed by more than one thread at a time.\n\n#### scheduleWithFixedDelay(Runnable task, long initialDelay, long period, TimeUnit timeunit)\n\nThis method works very much like scheduleAtFixedRate() except that the period is interpreted differently.\n\nIn the scheduleAtFixedRate() method the period is interpreted as a delay between the start of the previous execution, until the start of the next execution.\n\nIn this method, however, the period is interpreted as the delay between the end of the previous execution, until the start of the next. The delay is thus between finished executions, not between the beginning of executions.\n\n## Reference\n\n![Java Concurrent](java-concurrent.png)\n* [Java Concurrency and Multithreading Tutorial](http://tutorials.jenkov.com/java-concurrency/index.html)\n* [Java Concurrency Utilities](http://tutorials.jenkov.com/java-util-concurrent)\n* [Overview of the java.util.concurrent](https://www.baeldung.com/java-util-concurrent)\n* [Parallel Programming in Java](https://www.coursera.org/learn/parallel-programming-in-java)",
      "slug": "Java-Concurrent-Programming",
      "date": "2021-10-06 23:09:55",
      "lang": "en",
      "tags": ["Java", "Concurrent Programming", "Parallelism", "Java Stream"],
      "path": "/Java-Concurrent-Programming/index.html"
    },
    "lang": "en",
    "_nextI18Next": {
      "initialI18nStore": {
        "en": {
          "common": { "Next": "Next" },
          "feature-tags": { "Tags": "Tags" },
          "navbar": {
            "Home": "Home",
            "Language": "Language",
            "Tags": "Tags",
            "en": "English",
            "es": "Spanish",
            "zh": "Chinese"
          },
          "post-layout": { "Author": "Author: {{author}}" }
        }
      },
      "initialLocale": "en",
      "ns": ["common", "feature-tags", "navbar", "post-layout"],
      "userConfig": {
        "i18n": { "defaultLocale": "en", "locales": ["en", "es", "zh"] },
        "default": {
          "i18n": { "defaultLocale": "en", "locales": ["en", "es", "zh"] }
        }
      }
    }
  },
  "__N_SSG": true
}
